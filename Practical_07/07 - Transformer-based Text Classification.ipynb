{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka6H0qnGSpW9"
      },
      "source": [
        "# Transformer-based Text Classfication\n",
        "\n",
        "This notebook shows hot to make use of pre-trained Transformer models for building various types of text classifiers, some of which are multi-lingual. In particular, the notebook demonstrates the training of BERT-based text classifiers for: \n",
        "- categorising news group messages \n",
        "- detecting sentiment in Twitter messages\n",
        "- determining if a sentence paraphrases another.\n",
        "\n",
        "The notebook makes use of: \n",
        "- Transformer models from HuggingFace for training: https://github.com/huggingface/transformers\n",
        "- Lime for explanations: https://lime-ml.readthedocs.io/en/latest/index.html\n",
        "\n",
        "Much of the notebook is based on: \n",
        "- this blog post: https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "- this Google Colab notebook: https://colab.research.google.com/drive/1YxcceZxsNlvK35pRURgbwvkgejXwFxUt\n",
        "- this notebook: https://github.com/amaiya/ktrain/blob/master/examples/text/MRPC-BERT.ipynb\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RLWptUEpvTfh"
      },
      "source": [
        "## Prepare environment\n",
        "\n",
        "**NOTE**: To run this notebook, you will **need a GPU** (Graphics Processing Unit) card \n",
        "- the notebook has been tested on Google Colab and you are advised to run it there\n",
        "- to use a GPU, you need to click on \"Runtime\" above, then \"Change runtime type\" and for \"Hardware accelerator\" choose the \"GPU\" option\n",
        "\n",
        "Run the code below to check whether there is a gpu available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-l_oq6HmglH"
      },
      "outputs": [],
      "source": [
        "#!pip3 install -q transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fW5YE12VMOt4"
      },
      "outputs": [],
      "source": [
        "#!pip install ktrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_EfL3Ys_MSNA"
      },
      "outputs": [],
      "source": [
        "#!pip3 uninstall flask\n",
        "#!pip3 install -q eli5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "60uo8Ma8lruI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7w7V8Na_gPv2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/teo/.pyenv/versions/3.10.6/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import Dataset, DatasetDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "knkhmHlqgIjx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "buLDJl-hZFhr"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#if device.type != 'cuda':\n",
        "#    raise SystemError('GPU device not found')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdk5lPu3bxze"
      },
      "source": [
        "## Classification on the 20 Newsgroups dataset\n",
        "\n",
        "In the first example we'll build a multi-class classifier for newsgroup messages, as examples of longer text messages.\n",
        "\n",
        "Training on all of the documents in the dataset will take too long (a few hours), so we'll use just 4 today. \n",
        "- If you want to try on the full dataset, just remove the 'categories' attribute from the commands below. \n",
        "- Be warned though, that training time will be MUCH longer if you include all of the data, so first try subset of the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-rSbSqApYPBe"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "categories = ['comp.graphics', 'rec.sport.baseball', 'sci.med', 'alt.atheism']\n",
        "train = fetch_20newsgroups(subset='train', shuffle=True, categories=categories)\n",
        "test = fetch_20newsgroups(subset='test', shuffle=True, categories=categories)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xibNw613s4or"
      },
      "source": [
        "Print out some basic information about the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kCyuWfrDtHn7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# train instances:  2255\n",
            "# test instances:   1501\n",
            "classes:  ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n"
          ]
        }
      ],
      "source": [
        "print('# train instances: ', len(train.data))\n",
        "print('# test instances:  ', len(test.data))\n",
        "print('classes: ', train.target_names)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ugMhwArFA64S"
      },
      "source": [
        "Let's compute the occurrences of the classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KwZL6On6A64S"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({2: 597, 3: 594, 1: 584, 0: 480})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(train.target)\n",
        "counts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3XOMhW8X1j-O"
      },
      "source": [
        "A class count shows that there is a relatively even distribution of messages across the categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9VJnzlUStdLl"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlGElEQVR4nO3df1CU94HH8Q+ILP5aCCbsyojUNKlKojHRiJvkEqtUYphMnDBt9DhDcza2DuSitCYyYzXBXkidXEztEE1bK+k1jq1tTRtrVMSK1wioJE4JelxMnWJiFu5qYZWeoPC9Pzo81/X3IrjfXd+vmWem+zzfZ/f7nac7vvOwCzHGGCMAAACLxIZ7AgAAAOcjUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYJy7cE+iN7u5unThxQsOGDVNMTEy4pwMAAK6CMUanTp1SamqqYmMvf48kIgPlxIkTSktLC/c0AABALxw/flwjR4687JiIDJRhw4ZJ+tsC3W53mGcDAACuRiAQUFpamvPv+OVEZKD0/FjH7XYTKAAARJir+XgGH5IFAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWCfkQPn000/1T//0Txo+fLgGDRqk8ePH6+DBg85xY4yWL1+uESNGaNCgQcrKytJHH30U9BwnT55UXl6e3G63kpKSNH/+fJ0+ffraVwMAAKJCSIHyl7/8Rffff78GDhyod999V4cPH9a//du/6aabbnLGrFq1SmvWrNG6detUW1urIUOGKDs7W2fOnHHG5OXlqaGhQRUVFdq6dav27t2rBQsW9N2qAABARIsxxpirHbx06VK99957+o//+I+LHjfGKDU1Vd/85jf1rW99S5LU1tYmj8ej8vJyzZkzR0eOHFFGRoYOHDigyZMnS5K2b9+uRx55RJ988olSU1OvOI9AIKDExES1tbXxq+4BAIgQofz7HdIdlN/85jeaPHmyvvzlLyslJUV33323fvjDHzrHjx07Jr/fr6ysLGdfYmKiMjMzVV1dLUmqrq5WUlKSEyeSlJWVpdjYWNXW1oYyHQAAEKVCCpQ//vGPWrt2rW6//Xbt2LFDCxcu1L/8y7/ozTfflCT5/X5JksfjCTrP4/E4x/x+v1JSUoKOx8XFKTk52Rlzvo6ODgUCgaANAABEr5D+mnF3d7cmT56sl156SZJ0991368MPP9S6deuUn5/fLxOUpNLSUr344ov99vwAAMAuIQXKiBEjlJGREbRv3Lhx+uUvfylJ8nq9kqTm5maNGDHCGdPc3KyJEyc6Y1paWoKe49y5czp58qRz/vmKi4tVVFTkPA4EAkpLSwtl6gCi2LQnl4d7CjesPT8pCfcUEKVC+hHP/fffr8bGxqB9//Vf/6X09HRJ0ujRo+X1elVZWekcDwQCqq2tlc/nkyT5fD61traqrq7OGbN79251d3crMzPzoq/rcrnkdruDNgAAEL1CuoOyePFi3XfffXrppZf0la98Rfv379cPfvAD/eAHP5AkxcTEaNGiRfrOd76j22+/XaNHj9a3v/1tpaamavbs2ZL+dsfl4Ycf1tNPP61169bp7NmzKiws1Jw5c67qGzwAgBsHd8fCJ9x3x0IKlHvvvVdbtmxRcXGxSkpKNHr0aL322mvKy8tzxjz33HNqb2/XggUL1NraqgceeEDbt29XQkKCM+att95SYWGhZsyYodjYWOXm5mrNmjV9tyoAABDRQvo9KLbg96AA+Hv8V3b49Pd/ZXNtw6c/rm2//R4UAACA64FAAQAA1iFQAACAdUL6kCwQyfhZdviE+9sAACIPd1AAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1gkpUF544QXFxMQEbWPHjnWOnzlzRgUFBRo+fLiGDh2q3NxcNTc3Bz1HU1OTcnJyNHjwYKWkpGjJkiU6d+5c36wGAABEhbhQT7jjjju0a9eu/3+CuP9/isWLF+u3v/2tNm/erMTERBUWFurxxx/Xe++9J0nq6upSTk6OvF6v9u3bp88++0xPPvmkBg4cqJdeeqkPlgMAAKJByIESFxcnr9d7wf62tjatX79eGzdu1PTp0yVJGzZs0Lhx41RTU6OpU6dq586dOnz4sHbt2iWPx6OJEydq5cqVev755/XCCy8oPj7+2lcEAAAiXsifQfnoo4+UmpqqW2+9VXl5eWpqapIk1dXV6ezZs8rKynLGjh07VqNGjVJ1dbUkqbq6WuPHj5fH43HGZGdnKxAIqKGh4ZKv2dHRoUAgELQBAIDoFVKgZGZmqry8XNu3b9fatWt17Ngx/cM//INOnTolv9+v+Ph4JSUlBZ3j8Xjk9/slSX6/PyhOeo73HLuU0tJSJSYmOltaWloo0wYAABEmpB/xzJo1y/nfEyZMUGZmptLT0/Xzn/9cgwYN6vPJ9SguLlZRUZHzOBAIECkAAESxa/qacVJSkr7whS/o6NGj8nq96uzsVGtra9CY5uZm5zMrXq/3gm/19Dy+2OdaerhcLrnd7qANAABEr2sKlNOnT+vjjz/WiBEjNGnSJA0cOFCVlZXO8cbGRjU1Ncnn80mSfD6f6uvr1dLS4oypqKiQ2+1WRkbGtUwFAABEkZB+xPOtb31Ljz76qNLT03XixAmtWLFCAwYM0Ny5c5WYmKj58+erqKhIycnJcrvdeuaZZ+Tz+TR16lRJ0syZM5WRkaF58+Zp1apV8vv9WrZsmQoKCuRyufplgQAAIPKEFCiffPKJ5s6dqz//+c+65ZZb9MADD6impka33HKLJGn16tWKjY1Vbm6uOjo6lJ2drddff905f8CAAdq6dasWLlwon8+nIUOGKD8/XyUlJX27KgAAENFCCpRNmzZd9nhCQoLKyspUVlZ2yTHp6enatm1bKC8LAABuMPwtHgAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgnZD/mvGNYNqTy8M9hRvWnp/wlXMAAHdQAACAhQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGCdawqUl19+WTExMVq0aJGz78yZMyooKNDw4cM1dOhQ5ebmqrm5Oei8pqYm5eTkaPDgwUpJSdGSJUt07ty5a5kKAACIIr0OlAMHDuiNN97QhAkTgvYvXrxY77zzjjZv3qyqqiqdOHFCjz/+uHO8q6tLOTk56uzs1L59+/Tmm2+qvLxcy5cv7/0qAABAVOlVoJw+fVp5eXn64Q9/qJtuusnZ39bWpvXr1+vVV1/V9OnTNWnSJG3YsEH79u1TTU2NJGnnzp06fPiwfvrTn2rixImaNWuWVq5cqbKyMnV2dvbNqgAAQETrVaAUFBQoJydHWVlZQfvr6up09uzZoP1jx47VqFGjVF1dLUmqrq7W+PHj5fF4nDHZ2dkKBAJqaGi46Ot1dHQoEAgEbQAAIHrFhXrCpk2b9P777+vAgQMXHPP7/YqPj1dSUlLQfo/HI7/f74z5+zjpOd5z7GJKS0v14osvhjpVAAAQoUK6g3L8+HE9++yzeuutt5SQkNBfc7pAcXGx2tranO348ePX7bUBAMD1F1Kg1NXVqaWlRffcc4/i4uIUFxenqqoqrVmzRnFxcfJ4POrs7FRra2vQec3NzfJ6vZIkr9d7wbd6eh73jDmfy+WS2+0O2gAAQPQKKVBmzJih+vp6HTp0yNkmT56svLw8538PHDhQlZWVzjmNjY1qamqSz+eTJPl8PtXX16ulpcUZU1FRIbfbrYyMjD5aFgAAiGQhfQZl2LBhuvPOO4P2DRkyRMOHD3f2z58/X0VFRUpOTpbb7dYzzzwjn8+nqVOnSpJmzpypjIwMzZs3T6tWrZLf79eyZctUUFAgl8vVR8sCAACRLOQPyV7J6tWrFRsbq9zcXHV0dCg7O1uvv/66c3zAgAHaunWrFi5cKJ/PpyFDhig/P18lJSV9PRUAABChrjlQ9uzZE/Q4ISFBZWVlKisru+Q56enp2rZt27W+NAAAiFL8LR4AAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1QgqUtWvXasKECXK73XK73fL5fHr33Xed42fOnFFBQYGGDx+uoUOHKjc3V83NzUHP0dTUpJycHA0ePFgpKSlasmSJzp071zerAQAAUSGkQBk5cqRefvll1dXV6eDBg5o+fboee+wxNTQ0SJIWL16sd955R5s3b1ZVVZVOnDihxx9/3Dm/q6tLOTk56uzs1L59+/Tmm2+qvLxcy5cv79tVAQCAiBYXyuBHH3006PG//uu/au3ataqpqdHIkSO1fv16bdy4UdOnT5ckbdiwQePGjVNNTY2mTp2qnTt36vDhw9q1a5c8Ho8mTpyolStX6vnnn9cLL7yg+Pj4vlsZAACIWL3+DEpXV5c2bdqk9vZ2+Xw+1dXV6ezZs8rKynLGjB07VqNGjVJ1dbUkqbq6WuPHj5fH43HGZGdnKxAIOHdhLqajo0OBQCBoAwAA0SvkQKmvr9fQoUPlcrn0jW98Q1u2bFFGRob8fr/i4+OVlJQUNN7j8cjv90uS/H5/UJz0HO85dimlpaVKTEx0trS0tFCnDQAAIkjIgTJmzBgdOnRItbW1WrhwofLz83X48OH+mJujuLhYbW1tznb8+PF+fT0AABBeIX0GRZLi4+N12223SZImTZqkAwcO6Hvf+56eeOIJdXZ2qrW1NeguSnNzs7xeryTJ6/Vq//79Qc/X8y2fnjEX43K55HK5Qp0qAACIUNf8e1C6u7vV0dGhSZMmaeDAgaqsrHSONTY2qqmpST6fT5Lk8/lUX1+vlpYWZ0xFRYXcbrcyMjKudSoAACBKhHQHpbi4WLNmzdKoUaN06tQpbdy4UXv27NGOHTuUmJio+fPnq6ioSMnJyXK73XrmmWfk8/k0depUSdLMmTOVkZGhefPmadWqVfL7/Vq2bJkKCgq4QwIAABwhBUpLS4uefPJJffbZZ0pMTNSECRO0Y8cOfelLX5IkrV69WrGxscrNzVVHR4eys7P1+uuvO+cPGDBAW7du1cKFC+Xz+TRkyBDl5+erpKSkb1cFAAAiWkiBsn79+sseT0hIUFlZmcrKyi45Jj09Xdu2bQvlZQEAwA2Gv8UDAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwTkiBUlpaqnvvvVfDhg1TSkqKZs+ercbGxqAxZ86cUUFBgYYPH66hQ4cqNzdXzc3NQWOampqUk5OjwYMHKyUlRUuWLNG5c+eufTUAACAqhBQoVVVVKigoUE1NjSoqKnT27FnNnDlT7e3tzpjFixfrnXfe0ebNm1VVVaUTJ07o8ccfd453dXUpJydHnZ2d2rdvn958802Vl5dr+fLlfbcqAAAQ0eJCGbx9+/agx+Xl5UpJSVFdXZ0efPBBtbW1af369dq4caOmT58uSdqwYYPGjRunmpoaTZ06VTt37tThw4e1a9cueTweTZw4UStXrtTzzz+vF154QfHx8X23OgAAEJGu6TMobW1tkqTk5GRJUl1dnc6ePausrCxnzNixYzVq1ChVV1dLkqqrqzV+/Hh5PB5nTHZ2tgKBgBoaGi76Oh0dHQoEAkEbAACIXr0OlO7ubi1atEj333+/7rzzTkmS3+9XfHy8kpKSgsZ6PB75/X5nzN/HSc/xnmMXU1paqsTERGdLS0vr7bQBAEAE6HWgFBQU6MMPP9SmTZv6cj4XVVxcrLa2Nmc7fvx4v78mAAAIn5A+g9KjsLBQW7du1d69ezVy5Ehnv9frVWdnp1pbW4PuojQ3N8vr9Tpj9u/fH/R8Pd/y6RlzPpfLJZfL1ZupAgCACBTSHRRjjAoLC7Vlyxbt3r1bo0ePDjo+adIkDRw4UJWVlc6+xsZGNTU1yefzSZJ8Pp/q6+vV0tLijKmoqJDb7VZGRsa1rAUAAESJkO6gFBQUaOPGjfr1r3+tYcOGOZ8ZSUxM1KBBg5SYmKj58+erqKhIycnJcrvdeuaZZ+Tz+TR16lRJ0syZM5WRkaF58+Zp1apV8vv9WrZsmQoKCrhLAgAAJIUYKGvXrpUkTZs2LWj/hg0b9NWvflWStHr1asXGxio3N1cdHR3Kzs7W66+/7owdMGCAtm7dqoULF8rn82nIkCHKz89XSUnJta0EAABEjZACxRhzxTEJCQkqKytTWVnZJcekp6dr27Ztobw0AAC4gfC3eAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYJOVD27t2rRx99VKmpqYqJidHbb78ddNwYo+XLl2vEiBEaNGiQsrKy9NFHHwWNOXnypPLy8uR2u5WUlKT58+fr9OnT17QQAAAQPUIOlPb2dt11110qKyu76PFVq1ZpzZo1WrdunWprazVkyBBlZ2frzJkzzpi8vDw1NDSooqJCW7du1d69e7VgwYLerwIAAESVuFBPmDVrlmbNmnXRY8YYvfbaa1q2bJkee+wxSdJPfvITeTwevf3225ozZ46OHDmi7du368CBA5o8ebIk6fvf/74eeeQRvfLKK0pNTb2G5QAAgGjQp59BOXbsmPx+v7Kyspx9iYmJyszMVHV1tSSpurpaSUlJTpxIUlZWlmJjY1VbW3vR5+3o6FAgEAjaAABA9OrTQPH7/ZIkj8cTtN/j8TjH/H6/UlJSgo7HxcUpOTnZGXO+0tJSJSYmOltaWlpfThsAAFgmIr7FU1xcrLa2Nmc7fvx4uKcEAAD6UZ8GitfrlSQ1NzcH7W9ubnaOeb1etbS0BB0/d+6cTp486Yw5n8vlktvtDtoAAED06tNAGT16tLxeryorK519gUBAtbW18vl8kiSfz6fW1lbV1dU5Y3bv3q3u7m5lZmb25XQAAECECvlbPKdPn9bRo0edx8eOHdOhQ4eUnJysUaNGadGiRfrOd76j22+/XaNHj9a3v/1tpaamavbs2ZKkcePG6eGHH9bTTz+tdevW6ezZsyosLNScOXP4Bg8AAJDUi0A5ePCgvvjFLzqPi4qKJEn5+fkqLy/Xc889p/b2di1YsECtra164IEHtH37diUkJDjnvPXWWyosLNSMGTMUGxur3NxcrVmzpg+WAwAAokHIgTJt2jQZYy55PCYmRiUlJSopKbnkmOTkZG3cuDHUlwYAADeIiPgWDwAAuLEQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKwT1kApKyvT5z73OSUkJCgzM1P79+8P53QAAIAlwhYoP/vZz1RUVKQVK1bo/fff11133aXs7Gy1tLSEa0oAAMASYQuUV199VU8//bSeeuopZWRkaN26dRo8eLB+/OMfh2tKAADAEnHheNHOzk7V1dWpuLjY2RcbG6usrCxVV1dfML6jo0MdHR3O47a2NklSIBDol/md6+y48iD0i/66phLXNZz687pKXNtw4tpGr/64tj3PaYy58mATBp9++qmRZPbt2xe0f8mSJWbKlCkXjF+xYoWRxMbGxsbGxhYF2/Hjx6/YCmG5gxKq4uJiFRUVOY+7u7t18uRJDR8+XDExMZc9NxAIKC0tTcePH5fb7e7vqYYVa41eN9J6WWv0upHWy1ovzhijU6dOKTU19YrPG5ZAufnmmzVgwAA1NzcH7W9ubpbX671gvMvlksvlCtqXlJQU0mu63e6o/z9JD9YavW6k9bLW6HUjrZe1XigxMfGqni8sH5KNj4/XpEmTVFlZ6ezr7u5WZWWlfD5fOKYEAAAsErYf8RQVFSk/P1+TJ0/WlClT9Nprr6m9vV1PPfVUuKYEAAAsEbZAeeKJJ/Tf//3fWr58ufx+vyZOnKjt27fL4/H06eu4XC6tWLHigh8RRSPWGr1upPWy1uh1I62XtV67GGOu5rs+AAAA1w9/iwcAAFiHQAEAANYhUAAAgHUIFAAAYJ2oC5STJ08qLy9PbrdbSUlJmj9/vk6fPn3Zc6ZNm6aYmJig7Rvf+MZ1mnFoysrK9LnPfU4JCQnKzMzU/v37Lzt+8+bNGjt2rBISEjR+/Hht27btOs302oWy1vLy8guuYUJCwnWcbe/t3btXjz76qFJTUxUTE6O33377iufs2bNH99xzj1wul2677TaVl5f3+zz7Sqjr3bNnzwXXNiYmRn6///pMuJdKS0t17733atiwYUpJSdHs2bPV2Nh4xfMi9T3bm/VG6vt27dq1mjBhgvOLyXw+n959993LnhOp11UKfb19dV2jLlDy8vLU0NCgiooKbd26VXv37tWCBQuueN7TTz+tzz77zNlWrVp1HWYbmp/97GcqKirSihUr9P777+uuu+5Sdna2WlpaLjp+3759mjt3rubPn68PPvhAs2fP1uzZs/Xhhx9e55mHLtS1Sn/7LYZ/fw3/9Kc/XccZ9157e7vuuusulZWVXdX4Y8eOKScnR1/84hd16NAhLVq0SF/72te0Y8eOfp5p3wh1vT0aGxuDrm9KSko/zbBvVFVVqaCgQDU1NaqoqNDZs2c1c+ZMtbe3X/KcSH7P9ma9UmS+b0eOHKmXX35ZdXV1OnjwoKZPn67HHntMDQ0NFx0fyddVCn29Uh9d17758392OHz4sJFkDhw44Ox79913TUxMjPn0008ved5DDz1knn322esww2szZcoUU1BQ4Dzu6uoyqampprS09KLjv/KVr5icnJygfZmZmebrX/96v86zL4S61g0bNpjExMTrNLv+I8ls2bLlsmOee+45c8cddwTte+KJJ0x2dnY/zqx/XM16f/e73xlJ5i9/+ct1mVN/aWlpMZJMVVXVJcdE8nv2fFez3mh53xpjzE033WR+9KMfXfRYNF3XHpdbb19d16i6g1JdXa2kpCRNnjzZ2ZeVlaXY2FjV1tZe9ty33npLN998s+68804VFxfrr3/9a39PNySdnZ2qq6tTVlaWsy82NlZZWVmqrq6+6DnV1dVB4yUpOzv7kuNt0Zu1StLp06eVnp6utLS0K9Z9JIvU63qtJk6cqBEjRuhLX/qS3nvvvXBPJ2RtbW2SpOTk5EuOiaZrezXrlSL/fdvV1aVNmzapvb39kn+qJZqu69WsV+qb6xoRf834avn9/gtu+8bFxSk5OfmyP6/+x3/8R6Wnpys1NVV/+MMf9Pzzz6uxsVG/+tWv+nvKV+1//ud/1NXVdcFv2vV4PPrP//zPi57j9/svOt72n933Zq1jxozRj3/8Y02YMEFtbW165ZVXdN9996mhoUEjR468HtO+bi51XQOBgP73f/9XgwYNCtPM+seIESO0bt06TZ48WR0dHfrRj36kadOmqba2Vvfcc0+4p3dVuru7tWjRIt1///268847LzkuUt+z57va9Uby+7a+vl4+n09nzpzR0KFDtWXLFmVkZFx0bDRc11DW21fXNSICZenSpfrud7972TFHjhzp9fP//WdUxo8frxEjRmjGjBn6+OOP9fnPf77Xz4vrx+fzBdX8fffdp3HjxumNN97QypUrwzgzXKsxY8ZozJgxzuP77rtPH3/8sVavXq1///d/D+PMrl5BQYE+/PBD/f73vw/3VK6Lq11vJL9vx4wZo0OHDqmtrU2/+MUvlJ+fr6qqqkv+ox3pQllvX13XiAiUb37zm/rqV7962TG33nqrvF7vBR+iPHfunE6ePCmv13vVr5eZmSlJOnr0qDWBcvPNN2vAgAFqbm4O2t/c3HzJtXm93pDG26I3az3fwIEDdffdd+vo0aP9McWwutR1dbvdUXf35FKmTJkSMf/YFxYWOh/Yv9J/PUbqe/bvhbLe80XS+zY+Pl633XabJGnSpEk6cOCAvve97+mNN964YGw0XNdQ1nu+3l7XiPgMyi233KKxY8dedouPj5fP51Nra6vq6uqcc3fv3q3u7m4nOq7GoUOHJP3t1rIt4uPjNWnSJFVWVjr7uru7VVlZecmfA/p8vqDxklRRUXHZnxvaoDdrPV9XV5fq6+utuoZ9JVKva186dOiQ9dfWGKPCwkJt2bJFu3fv1ujRo694TiRf296s93yR/L7t7u5WR0fHRY9F8nW9lMut93y9vq7X/DFbyzz88MPm7rvvNrW1teb3v/+9uf32283cuXOd45988okZM2aMqa2tNcYYc/ToUVNSUmIOHjxojh07Zn7961+bW2+91Tz44IPhWsIlbdq0ybhcLlNeXm4OHz5sFixYYJKSkozf7zfGGDNv3jyzdOlSZ/x7771n4uLizCuvvGKOHDliVqxYYQYOHGjq6+vDtYSrFupaX3zxRbNjxw7z8ccfm7q6OjNnzhyTkJBgGhoawrWEq3bq1CnzwQcfmA8++MBIMq+++qr54IMPzJ/+9CdjjDFLly418+bNc8b/8Y9/NIMHDzZLliwxR44cMWVlZWbAgAFm+/bt4VpCSEJd7+rVq83bb79tPvroI1NfX2+effZZExsba3bt2hWuJVyVhQsXmsTERLNnzx7z2WefOdtf//pXZ0w0vWd7s95Ifd8uXbrUVFVVmWPHjpk//OEPZunSpSYmJsbs3LnTGBNd19WY0NfbV9c16gLlz3/+s5k7d64ZOnSocbvd5qmnnjKnTp1yjh87dsxIMr/73e+MMcY0NTWZBx980CQnJxuXy2Vuu+02s2TJEtPW1hamFVze97//fTNq1CgTHx9vpkyZYmpqapxjDz30kMnPzw8a//Of/9x84QtfMPHx8eaOO+4wv/3tb6/zjHsvlLUuWrTIGevxeMwjjzxi3n///TDMOnQ9X6M9f+tZX35+vnnooYcuOGfixIkmPj7e3HrrrWbDhg3Xfd69Fep6v/vd75rPf/7zJiEhwSQnJ5tp06aZ3bt3h2fyIbjYGiUFXatoes/2Zr2R+r7953/+Z5Oenm7i4+PNLbfcYmbMmOH8Y21MdF1XY0Jfb19d1xhjjAntngsAAED/iojPoAAAgBsLgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6/wf6j4y0wzMQrAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(counts.keys(), counts.values(), color=\"#3F5D7D\", width=0.8);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_wrvSxqqA64U"
      },
      "source": [
        "### Model training and evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pe5xxVPrb4IO"
      },
      "source": [
        "#### Creating a model and preprocessing data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rctt-zMfkhME"
      },
      "source": [
        "For the moment we will use the standard pretrained Multilingual BERT model. Note that this **model is big**! \n",
        "- it contains 110 million parameters\n",
        "- it represents words (subword tokens to be precise) with embeddings of size 768 <-- compare this with the 50 dimensions we used with GloVe\n",
        "- it contains 12 self-attention layers (each with 12 heads) stacked on top each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K-MljyocSpXG"
      },
      "outputs": [],
      "source": [
        "model_name = 'bert-base-multilingual-uncased'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CaS6XnAqSpXG"
      },
      "source": [
        "There are many other pretrained models availalable here: https://huggingface.co/transformers/pretrained_models.html\n",
        "- Give them a try and see which ones are faster and/or more stable to train!\n",
        "\n",
        "Load the transformer setting the maximum text length to 500 and the classes to be the four newsgroup categories mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "myYmdpaVi6XB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(categories))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Kn0RMEg1ZN"
      },
      "source": [
        "Move model to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M1rV7KHBwZA9"
      },
      "outputs": [],
      "source": [
        "bert = bert.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iuk_C-3zaDeX"
      },
      "source": [
        "Now that we've downloaeded the base Multiligual BERT model, let's have a look at the architecture. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "WnHq1_iZYdSG",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(bert)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rwIeFrFNjgf9"
      },
      "source": [
        "Check which of the parameters are trainable: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "vGGOvNFchCNB",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([105879, 768]) True\n",
            "torch.Size([512, 768]) True\n",
            "torch.Size([2, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([3072, 768]) True\n",
            "torch.Size([3072]) True\n",
            "torch.Size([768, 3072]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([768, 768]) True\n",
            "torch.Size([768]) True\n",
            "torch.Size([4, 768]) True\n",
            "torch.Size([4]) True\n"
          ]
        }
      ],
      "source": [
        "for param in bert.parameters():\n",
        "    print(param.size(), param.requires_grad)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0QGePPmboBHT"
      },
      "source": [
        "Load the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LUbB1gjAoDnE"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp4ju07NsN24"
      },
      "source": [
        "Now we can preprocess the training (and test) data using the transformer's data set API"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BBfcR-j-hjWI"
      },
      "source": [
        "First create a list of dictionaries with all the samples in the train and test data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4Pw8W1OIrK6R"
      },
      "outputs": [],
      "source": [
        "train_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(train.data, train.target)]\n",
        "test_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(test.data, test.target)]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "32C0MMkLhr2g"
      },
      "source": [
        "Convert the data sets into the Huggingface data set API format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wvpGAb7us2ij"
      },
      "outputs": [],
      "source": [
        "train_data = Dataset.from_list(train_data)\n",
        "test_data = Dataset.from_list(test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lTYfG4twhyBL"
      },
      "source": [
        "Create a container with all the splits of the data set\n",
        "\n",
        "**Note**: We are using the same data for validation and testing, this is not a good practice, it's just for the tutoria, in general you should have separate data for validation and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MSxUkZy6tcew"
      },
      "outputs": [],
      "source": [
        "data = DatasetDict()\n",
        "data['train'] = train_data\n",
        "data['validation'] = test_data\n",
        "data['test'] = test_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iM1xuM2th2SL"
      },
      "source": [
        "Finally use the tokenizer to convert the input strings into sequences of tokens (more on this later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xPOAbmdwsMaG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                 \r"
          ]
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "tokenized_data = data.map(tokenize_function, batched=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6jDlmkQAfFNI"
      },
      "source": [
        "Let's have a quick look at what the tokenised data looks like: \n",
        "- First print out the text of the first document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NZVwnMckfgba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From: cab@col.hp.com (Chris Best)\n",
            "Subject: Re: Is MSG sensitivity superstition?\n",
            "Organization: your service\n",
            "Lines: 20\n",
            "NNTP-Posting-Host: hpctdkz.col.hp.com\n",
            "\n",
            "\n",
            "Jason Chen writes:\n",
            "> Now here is a new one: vomiting. My guess is that MSG becomes the number one\n",
            "> suspect of any problem. In this case. it might be just food poisoning. But\n",
            "> if you heard things about MSG, you may think it must be it.\n",
            "\n",
            "----------\n",
            "\n",
            "Yeah, it might, if you only read the part you quoted.  You somehow left \n",
            "out the part about \"we all ate the same thing.\"  Changes things a bit, eh?\n",
            "\n",
            "You complain that people blame MSG automatically, since it's an unknown and\n",
            "therefore must be the cause.  It is equally (if not more) unreasonable to\n",
            "defend it, automatically assuming that it CAN'T be the culprit.\n",
            "\n",
            "Pepper makes me sneeze.  If it doesn't affect you the same way, fine.\n",
            "Just don't tell me I'm wrong for saying so.\n",
            "\n",
            "These people aren't condemning Chinese food, Mr. Chen - just one of its \n",
            "(optional) ingredients.  Try not to take it so personally.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(train.data[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BJc8ABEBSpXH"
      },
      "source": [
        "Now print out the result of preprocessing the first document: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xO_FOkTdSpXH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[101, 10195, 131, 70438, 137, 12308, 119, 20931, 119, 10241, 113, 13446, 11146, 114, 19513, 131, 11449, 131, 10127, 16777, 10251, 20242, 14119, 52902, 12278, 12054, 11101, 136, 14935, 131, 12787, 11416, 17294, 131, 10200, 37580, 84723, 118, 42191, 10422, 118, 17624, 131, 20931, 15106, 10163, 10167, 10311, 119, 12308, 119, 20931, 119, 10241, 17110, 20728, 47332, 131, 135, 11628, 14048, 10127, 143, 10246, 10399, 131, 11106, 69431, 119, 11153, 41567, 22523, 10127, 10203, 16777, 10251, 23259, 10103, 11395, 10399, 135, 75695, 10108, 11318, 15640, 119, 10104, 10372, 12700, 119, 10197, 20025, 10346, 12125, 15225, 55340, 10285, 119, 10502, 135, 11526, 10855, 27134, 17994, 10935, 16777, 10251, 117, 10855, 10431, 21506, 10197, 14650, 10346, 10197, 119, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 82600, 117, 10197, 20025, 117, 11526, 10855, 10902, 18593, 10103, 10649, 10855, 62259, 119, 10855, 10970, 38644, 12174, 10871, 10103, 10649, 10935, 107, 11312, 10367, 12811, 10103, 11714, 21973, 119, 107, 17992, 17994, 143, 16464, 117, 58375, 136, 10855, 85065, 44409, 10203, 11227, 88633, 16777, 10251, 62428, 117, 11500, 10197, 112, 161, 10144, 22669, 10110, 20598, 14650, 10346, 10103, 15126, 119, 10197, 10127, 77697, 113, 11526, 10497, 10772, 114, 10119, 13480, 11796, 13356, 10114, 43461, 10197, 117, 62428, 13967, 19955, 10422, 10203, 10197, 10743, 112, 162, 10346, 10103, 10707, 57485, 16100, 119, 50559, 19552, 10525, 47410, 12168, 10732, 119, 11526, 10197, 39707, 112, 162, 56365, 10855, 10103, 11714, 12140, 117, 12922, 119, 12125, 11530, 112, 162, 21166, 10525, 151, 112, 155, 33413, 10139, 22811, 10297, 119, 11269, 11227, 29938, 112, 162, 18276, 53243, 10422, 13387, 15225, 117, 12854, 119, 20728, 118, 12125, 10399, 10108, 10491, 113, 77906, 114, 88703, 119, 26333, 10497, 10114, 11622, 10197, 10297, 73602, 119, 102]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer(train.data[0]).input_ids)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VolZFjl9f61n"
      },
      "source": [
        "Each integer above is the index of a token from the vocabulary of the BERT model. \n",
        "- How big is the vocabulary?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7zYUTkJUgatH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary size:  105879\n"
          ]
        }
      ],
      "source": [
        "print(\"vocabulary size: \", len(tokenizer.vocab))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NXZgNOrmgoTJ"
      },
      "source": [
        "This vocabulary is relatively small compared to many word embeddings discussed in the lecture, which often reach over 1 million distinct words. \n",
        "- The vocab size is kept relatively small through the use of sub-word tokens that are found using a byte-pair encoding scheme.\n",
        "- We can take a single string and run it through the tokenizer to see the tokens produced as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LP3gdwRe1AVL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input text: I always wondered what those Transformer models were doing to my text. Here is a OutOfVocabWord.\n",
            "tokenized:   ['[CLS]', 'i', 'always', 'wonder', '##ed', 'what', 'those', 'transforme', '##r', 'models', 'were', 'doing', 'to', 'my', 'text', '.', 'here', 'is', 'a', 'out', '##of', '##vo', '##ca', '##b', '##word', '.', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "example_text = \"I always wondered what those Transformer models were doing to my text. Here is a OutOfVocabWord.\"\n",
        "encoded_text = tokenizer(example_text).input_ids\n",
        "vocab_terms = list(tokenizer.vocab.keys())\n",
        "vocab_index = list(tokenizer.vocab.values())\n",
        "print(\"input text: \"+example_text)\n",
        "print(\"tokenized:  \", [vocab_terms[vocab_index.index(i)] for i in encoded_text])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cQRSfSWWhFnF"
      },
      "source": [
        "Note that:\n",
        "- a '[CLS]' token has been added to the start of the tokenized text and a '[SEP]' token to the end.\n",
        "- BERT works as a masked autoencoder, meaning that it is trained to produce the same terms as output that are presented in input, including those that were masked out.\n",
        "- the '[CLS]' token is simply a special mask for the unknown class label that needs to be produced at the output. \n",
        "- the Byte-Pair Encoding has removed the suffixes from certain words (e.g. 'wondered' lost the 'ed'), but not all of them ('models' kept the 's'). "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G4sGPJgOcBTd"
      },
      "source": [
        "#### Training the model\n",
        "\n",
        "Now we're ready to start training the model. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gO6NOLH2qIOP"
      },
      "source": [
        "Prepare training arguments (including a name for the trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "HyLEhf2kvlxc"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    \"cool_trainer_name\", \n",
        "    per_device_train_batch_size=16\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MUPaP_OPjmFd"
      },
      "source": [
        "Disable weights in model encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Um37Z8bLwwHg"
      },
      "outputs": [],
      "source": [
        "for param in bert.bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewvcli7wkAWb"
      },
      "source": [
        "Build the traininer using the Huggingface trainer API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "doJDqHl6opqa"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=bert, \n",
        "    args=training_args, \n",
        "    train_dataset=tokenized_data['train'], \n",
        "    eval_dataset=tokenized_data['validation']\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pji9wH6rkLfl"
      },
      "source": [
        "Finally, run the training process (we train for three epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VVm_LxBEopvX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/teo/.pyenv/versions/3.10.6/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 423/423 [57:56<00:00,  8.22s/it]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 3476.1102, 'train_samples_per_second': 1.946, 'train_steps_per_second': 0.122, 'train_loss': 1.3749216499058068, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=423, training_loss=1.3749216499058068, metrics={'train_runtime': 3476.1102, 'train_samples_per_second': 1.946, 'train_steps_per_second': 0.122, 'train_loss': 1.3749216499058068, 'epoch': 3.0})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ljEgcP9Pkkb1"
      },
      "source": [
        "Sorry Mark, no learning rate plot :("
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ho6eSo9IcI3_"
      },
      "source": [
        "#### Evaluating the model\n",
        "\n",
        "Now that the model has finished training, let's evaluate in on the test data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oX8NZVwVktii"
      },
      "source": [
        "First we can run the evaluation process included with the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zB1pCPU10Jwd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 188/188 [04:49<00:00,  1.54s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.3650124073028564,\n",
              " 'eval_runtime': 290.9298,\n",
              " 'eval_samples_per_second': 5.159,\n",
              " 'eval_steps_per_second': 0.646,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A8AV-Qi9k7Te"
      },
      "source": [
        "Then we can get the predictions on the test set..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TPoK1yoq1IRf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 188/188 [04:49<00:00,  1.54s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-0.29245675,  0.10277522,  0.31252596,  0.04978357],\n",
              "       [-0.2525715 ,  0.11141657,  0.16185516,  0.0646465 ],\n",
              "       [-0.44719857,  0.12924637,  0.46883777,  0.02780216],\n",
              "       ...,\n",
              "       [-0.26905823,  0.11218481,  0.23013744,  0.03226996],\n",
              "       [-0.24218898,  0.11854254,  0.21023665,  0.05792751],\n",
              "       [-0.3066854 ,  0.07030223,  0.23812479,  0.06731809]],\n",
              "      dtype=float32), label_ids=array([0, 1, 1, ..., 3, 1, 3]), metrics={'test_loss': 1.3650124073028564, 'test_runtime': 291.0774, 'test_samples_per_second': 5.157, 'test_steps_per_second': 0.646})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = trainer.predict(tokenized_data['test'])\n",
        "preds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ojpVT2tEmh0S"
      },
      "source": [
        "Convert predicted logits to classes using the $\\mathrm{argmax}$ operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8tjOw1YYmSHI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 2, 2, ..., 2, 2, 2])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n",
        "y_pred"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HBNOiC6iuQYg"
      },
      "source": [
        "Get target label names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2CmMFBNouPA-"
      },
      "outputs": [],
      "source": [
        "label_names = test.target_names"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gTyjAzeak_uV"
      },
      "source": [
        "... and use them to preprare a classification report using Scikit-Learn API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "UvcxCvLOcOje"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "       alt.atheism       0.17      0.00      0.01       319\n",
            "     comp.graphics       0.75      0.03      0.06       389\n",
            "rec.sport.baseball       0.28      0.99      0.44       397\n",
            "           sci.med       0.53      0.11      0.18       396\n",
            "\n",
            "          accuracy                           0.30      1501\n",
            "         macro avg       0.43      0.28      0.17      1501\n",
            "      weighted avg       0.44      0.30      0.18      1501\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test.target, y_pred, target_names=label_names))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jHRb1I-JlIMF"
      },
      "source": [
        "... or compute a confusion matrix using the Scikit-Learn API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "73YiamzdlI6t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  1   1 298  19]\n",
            " [  1  12 359  17]\n",
            " [  1   0 393   3]\n",
            " [  3   3 346  44]]\n"
          ]
        }
      ],
      "source": [
        "print(confusion_matrix(test.target, y_pred))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yhG3fPtPcVKe"
      },
      "source": [
        "We can have a look at the test examples on which the model made the worst predictions:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IzX6OeNZqYIu"
      },
      "source": [
        "Compute loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mCABLebacTWM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.7447, 1.3086, 1.3532,  ..., 1.3967, 1.3176, 1.3551])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = torch.nn.functional.cross_entropy(torch.tensor(preds.predictions), torch.tensor(preds.label_ids), reduction='none')\n",
        "loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9j_ACNkMqcKL"
      },
      "source": [
        "Identify the index of the element with the highest loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "r4-Ts24aqWQW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1393"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_of_instance = torch.argmax(loss).item()\n",
        "index_of_instance"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ymQd8xY3qugI"
      },
      "source": [
        "Loss of the corresponding sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WJoHPh7VqWbN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.877933144569397"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss[index_of_instance].item()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjuo7nRcq7Bm"
      },
      "source": [
        "Predicted class and target class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "AXWwP5IDq-JP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target class:    alt.atheism\n",
            "Predicted class: rec.sport.baseball\n"
          ]
        }
      ],
      "source": [
        "print(f'Target class:    {label_names[test.target[index_of_instance]]}')\n",
        "print(f'Predicted class: {label_names[y_pred[index_of_instance]]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nJG_H42_b3ra"
      },
      "source": [
        "Print out the post with the top loss (by changing INDEX_OF_INSTANCE below) to try to find out why it is confusing the classfier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "pHYRBdBycfne"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From: Nanci Ann Miller <nm0w+@andrew.cmu.edu>\n",
            "Subject: Books\n",
            "Organization: Sponsored account, School of Computer Science, Carnegie Mellon, Pittsburgh, PA\n",
            "Lines: 23\n",
            "\t<EDM.93Apr20145436@gocart.twisto.compaq.com>\n",
            "NNTP-Posting-Host: andrew.cmu.edu\n",
            "In-Reply-To: <EDM.93Apr20145436@gocart.twisto.compaq.com>\n",
            "\n",
            "edm@twisto.compaq.com (Ed McCreary) writes:\n",
            "> While we're on the topic of books, has anyone else noticed that Paine's\n",
            "> \"The Age of Reason\" is hard to find.  I've been wanting to pick up\n",
            "> a copy for a while, but not bad enough to mail order it.  I've noticed\n",
            "> though that none of the bookstores I go to seem to carry it.  I thought\n",
            "> this was supposed to be classic.  What's the deal?\n",
            "\n",
            "Actually, I've got an entire list of books written by various atheist\n",
            "authors and I went to the largest bookstore in my area (Pittsburgh) and\n",
            "couldn't find _any_ of them.  What section of the bookstore do you find\n",
            "these kinds of books in?  Do you have to look in an \"alternative\" bookstore\n",
            "for most of them?  Any help would be appreciated (I can send you the list\n",
            "if you want).\n",
            "\n",
            "Thanks,\n",
            "Nanci\n",
            ".........................................................................\n",
            "If you know (and are SURE of) the author of this quote, please send me\n",
            "email (nm0w+@andrew.cmu.edu):\n",
            "The fate of the country does not depend on what kind of paper you drop into\n",
            "the ballot box once a year, but on what kind of man you drop from your\n",
            "chamber into the street every morning.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(test.data[index_of_instance])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CcZQ6HbqdMcF"
      },
      "source": [
        "### Making predictions on new data\n",
        "\n",
        "We can run the trained classification model on new examples. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rTkP9KKyA64n"
      },
      "source": [
        "#### Utility functions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cpnQiMKyrbvd"
      },
      "source": [
        "Define a simple function to make a prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2Sqrv83frb5Z"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict(text):\n",
        "    input_encodings = tokenizer(text, return_tensors='pt').to(device)\n",
        "    outputs = bert(**input_encodings)\n",
        "\n",
        "    lbl = label_names[torch.argmax(outputs.logits).item()]\n",
        "\n",
        "    return lbl"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mxQIB1-0rcCv"
      },
      "source": [
        "Define  simple function to predict the classes probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NUYdUWthrcMz"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_proba(text):\n",
        "    input_encodings = tokenizer(text, return_tensors='pt').to(device)\n",
        "    outputs = bert(**input_encodings)\n",
        "\n",
        "    proba = torch.softmax(outputs.logits, dim=1).cpu().squeeze().numpy()\n",
        "\n",
        "    \n",
        "    return dict(zip(label_names, proba))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OyjLvuMus_fw"
      },
      "source": [
        "We can now try the trained model on new samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "hp8tw3Y0cnJa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sci.med'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_text = 'Both of the home runs hit by Fernando Tatís in the third inning for the St. Louis Cardinals on April 23, 1999, were grand slams.'\n",
        "predict(sample_text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YYqzU7nJhROw"
      },
      "source": [
        "We can see the probabilities assigned by the model to each of the classes as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "YHT1jVTGi0yJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_values([0.2483198, 0.24909294, 0.23996086, 0.2626264])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_proba(sample_text).values()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H2p7Xojzi8uB"
      },
      "source": [
        "Where the classes are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Zg93RQvMjAFd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med'])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_proba(sample_text).keys()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1H3TOwIgSpXL"
      },
      "source": [
        "OK, so the model appears to work well on English text."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmWpMvoiPha"
      },
      "source": [
        "#### Multi-lingual examples\n",
        "\n",
        "Let's try on a phrase in another language (Italian). \n",
        "- Note that the news group data we used to train the model is in English only, so the model has never seen any examples of news group messages in Italian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "LZOeu9cDdguM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sci.med'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "italian_text = 'I tasti su e giù sul mio portatile non funzionano più, quindi non posso più usarlo per giocare ;-('\n",
        "predict(italian_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xiCLME7Dnoye"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'alt.atheism': 0.21772848,\n",
              " 'comp.graphics': 0.25820065,\n",
              " 'rec.sport.baseball': 0.26080987,\n",
              " 'sci.med': 0.26326105}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_proba(italian_text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gWcw5VGIinK5"
      },
      "source": [
        "Did it get the prediction right? \n",
        "- Note that while the characer set for Italian and English is almost the same, there weren't any words (except perhaps for \"non\") in the above message that would have occurred in the training data. \n",
        "\n",
        "Let's try a message in a different character set: \n",
        "- Translate the text into Chinese using Google Translate: https://translate.google.com/\n",
        "- Then copy the text below and rerun the prediction. \n",
        "- Does it still work? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JuMmx8f5dr45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'comp.graphics'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chinese_text = '笔记本电脑上的向上和向下键不再起作用，因此我无法再使用它来播放;-(' #'INSERT CHINESE TEXT HERE'\n",
        "predict(chinese_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "2OiNy8look-l"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'rec.sport.baseball'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "russian_text = \"Клавиши «вверх» и «вниз» на моем ноутбуке больше не работают, поэтому я больше не могу использовать их для игры ;-(\"\n",
        "predict(russian_text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g1r31KjnjU2m"
      },
      "source": [
        "### Explaining the predictions\n",
        "\n",
        "Let's try another piece of text, this time in English, but talking about a politician who wasn't a politician (but rather a celebrity) at the time in which the 20 News Groups dataset was created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "oXJ9qVdIjmlI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'rec.sport.baseball'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_text = 'Donald Trump is the greatest living ex-president in the history of living ex-presidents. He\\'s also a bad golfer.'\n",
        "predict(sample_text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9tHos7V6d8RQ"
      },
      "source": [
        "Let's invoke the `explain` method to see which words contribute most to the classification.\n",
        "\n",
        "We will need to install the **eli5** library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "XjruXVE47G9d"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import numpy as np\n",
        "def monkeypath_itemfreq(sampler_indices):\n",
        "   return zip(*np.unique(sampler_indices, return_counts=True))\n",
        "\n",
        "scipy.stats.itemfreq=monkeypath_itemfreq\n",
        "import eli5\n",
        "from eli5.lime import TextExplainer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NV3GfJCLl2sq"
      },
      "source": [
        "Define funxtion to output the probability distribution as a matrix `(n_samples, n_classes)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "wfYGGX-mlYnS"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_proba_values(text):\n",
        "    input_encodings = tokenizer(text, return_tensors='pt', padding=True).to(device)\n",
        "    outputs = bert(**input_encodings)\n",
        "\n",
        "    proba = torch.softmax(outputs.logits, dim=1).cpu().squeeze().numpy()\n",
        "\n",
        "    return proba"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xWR5fodljOV5"
      },
      "source": [
        "Run the explainer (based on LIME) to see which words in the sentence are most important for the prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "kQxI9hUK59oc"
      },
      "outputs": [],
      "source": [
        "# See https://amaiya.github.io/ktrain/text/predictor.html\n",
        "# See https://eli5.readthedocs.io/en/latest/autodocs/lime.html\n",
        "def explain(text, truncate_len=512, all_targets=False, n_samples=2500):\n",
        "    prediction = [predict(text)] if not all_targets else None\n",
        "    text = \" \".join(text.split()[:truncate_len])\n",
        "    te = TextExplainer(random_state=42, n_samples=n_samples)\n",
        "    _ = te.fit(text, predict_proba_values)\n",
        "    return te.show_prediction(\n",
        "        target_names=label_names, targets=prediction\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3HgZDLYUeVaM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/teo/.pyenv/versions/3.10.6/envs/nlp/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=rec.sport.baseball\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.337</b>, score <b>-0.450</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 85.93%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.262\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x23\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.66%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.243\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x31\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.81%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.189\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x5\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.09%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.159\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x7\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.90%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.140\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x29\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.69%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.123\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x9\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 92.70%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.103\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x19\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 93.56%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.086\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x36\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 94.24%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.073\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x27\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 95.31%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.055\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x33\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 95.40%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.053\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x20\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 96.07%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.042\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x25\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 96.33%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.038\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x21\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 97.48%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.022\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x24\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 96.77%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.032\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x3\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 95.88%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.045\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x32\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 95.76%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.047\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x11\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 94.17%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.074\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x28\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 94.10%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.076\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x22\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 94.03%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.077\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x18\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 93.23%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.092\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x35\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 93.09%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.095\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x30\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 92.73%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.102\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x12\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 92.12%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.114\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x4\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 92.03%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.116\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x16\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 90.94%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.140\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x2\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 89.87%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.164\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x26\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 89.03%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.184\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x8\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 88.27%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.202\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x6\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.433\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"opacity: 0.80\">donald trump is the greatest living ex-president in the history of living ex-presidents. he&#x27;s also a bad golfer.</span>\n",
              "    </p>\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "explain(sample_text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mcph2bSLe5cW"
      },
      "source": [
        "The words in the darkest shade of green contribute most to the classification.\n",
        "- Do they agree with what you would have expected for this example?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wceyWpdC-C1H"
      },
      "source": [
        "### Inspecting the Model\n",
        "\n",
        "Let's have a look at the architecture of the model that we have used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ISHuI-fpSpXT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(bert)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "03oHrJjZvVxT"
      },
      "source": [
        "Compute the total number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "CgL3V56lvWGD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "167359492"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_params = sum(param.numel() for param in bert.parameters())\n",
        "n_params"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "erGkWyP_WJWg"
      },
      "source": [
        "The model contains over 100 million parameters consists of:\n",
        "- BERT (which contains alost all the parameters), \n",
        "- a drop-out layer (that is inserted to prevent overfitting), and \n",
        "- a dense (feedforward) layer to map the BERT embedding of size 768 to the 2 output neurons (representing the positive and negative classes).\n",
        "\n",
        "Let's print out information on the configuration of BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "hiz0p_Zo7eB8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
              "  \"architectures\": [\n",
              "    \"BertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"directionality\": \"bidi\",\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\",\n",
              "    \"3\": \"LABEL_3\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2,\n",
              "    \"LABEL_3\": 3\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"pooler_fc_size\": 768,\n",
              "  \"pooler_num_attention_heads\": 12,\n",
              "  \"pooler_num_fc_layers\": 3,\n",
              "  \"pooler_size_per_head\": 128,\n",
              "  \"pooler_type\": \"first_token_transform\",\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"problem_type\": \"single_label_classification\",\n",
              "  \"transformers_version\": \"4.29.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 105879\n",
              "}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert.bert.config"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-s-YjRXLmXo6"
      },
      "source": [
        "### Saving and loading a fine-tuned model\n",
        "\n",
        "We can save predictor for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Z1nzxI_Jec-5"
      },
      "outputs": [],
      "source": [
        "bert.save_pretrained('/tmp/my_predictor')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_ImfZ2Bf1M"
      },
      "source": [
        "Reload the predictor and use it to predict the class of a new piece of text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "DDEU2s03fHsw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'rec.sport.baseball'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert = AutoModelForSequenceClassification.from_pretrained('/tmp/my_predictor').to(device)\n",
        "predict('My computer monitor is really blurry.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9bjyrTvao3CD"
      },
      "source": [
        "## Classifying the sentiment of Twitter messages\n",
        "\n",
        "We'll now train a different model to detect sentiment on Twitter data as we did for the previous tutorials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "FCJgsiUzg1wg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to\n",
            "[nltk_data]     /Users/teo/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "from nltk.corpus import twitter_samples\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "import re \n",
        "emoticon_regex = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
        "positive_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in positive_tweets]\n",
        "negative_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in negative_tweets]\n",
        "\n",
        "tweets_x = positive_tweets_noemoticons + negative_tweets_noemoticons\n",
        "tweets_y = ['positive']*len(positive_tweets) + ['negative']*len(negative_tweets)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7LuKhzRHvmYA"
      },
      "source": [
        "Use scikit-learn to split the data into training, validation and test sets:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "wWghucxwufOf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "temp_x, test_x, temp_y, test_y = train_test_split(tweets_x, tweets_y, test_size=0.2)\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(temp_x, temp_y, test_size=0.2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bS8svgXSwbYc"
      },
      "source": [
        "This time we are going to use a different tool called [ktrain](https://github.com/amaiya/ktrain): "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "0uwolFPLIB-i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "import ktrain\n",
        "from ktrain import text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QPBbalh8IEOl"
      },
      "source": [
        "1.   Load the model (this time we'll try DistilBERT, which is a smaller transformer model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "bM226FFCuhYk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/teo/.pyenv/versions/3.10.6/envs/nlp/lib/python3.10/site-packages/ktrain/text/preprocessor.py:382: UserWarning: The class_names argument is replacing the classes argument. Please update your code.\n",
            "  warnings.warn(\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 3.38MB/s]\n",
            "Downloading tf_model.h5: 100%|██████████| 363M/363M [00:06<00:00, 54.3MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1 Pro\n",
            "\n",
            "systemMemory: 16.00 GB\n",
            "maxCacheSize: 5.33 GB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "transformer = text.Transformer(MODEL_NAME, maxlen=500, classes=['positive','negative'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wFe12_blSpXP"
      },
      "source": [
        "2. Process the training/test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "QwgWazABSpXP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preprocessing train...\n",
            "language: en\n",
            "train sequence lengths:\n",
            "\tmean : 11\n",
            "\t95percentile : 23\n",
            "\t99percentile : 27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/teo/.pyenv/versions/3.10.6/envs/nlp/lib/python3.10/site-packages/ktrain/utils.py:744: UserWarning: class_names argument was ignored, as they were extracted from string labels in dataset\n",
            "  warnings.warn(\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 112kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.14MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 4.64MB/s]\n"
          ]
        }
      ],
      "source": [
        "processed_train = transformer.preprocess_train(train_x, train_y)\n",
        "processed_test = transformer.preprocess_test(valid_x, valid_y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne5PhBw9SpXP"
      },
      "source": [
        "3. Create a model and learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "KV_5oz6PSpXQ"
      },
      "outputs": [],
      "source": [
        "model = transformer.get_classifier()\n",
        "learner = ktrain.get_learner(model, train_data=processed_train, val_data=processed_test, batch_size=6)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9d5OI2QBSpXQ"
      },
      "source": [
        "4.   Train the model (We'll run for one epoch to make it faster, but it would be better to run for 4 or more)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "F886bGFlxoQL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 5e-05...\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "learner.fit_onecycle(5e-5, 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VtIofrVNSpXQ"
      },
      "source": [
        "5.   Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smj0NtNWx6sK"
      },
      "outputs": [],
      "source": [
        "learner.validate(class_names=transformer.get_classes())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AU0kI3s3SpXQ"
      },
      "source": [
        "6.   Make some predictions with it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE6X0pAYx8Mi"
      },
      "outputs": [],
      "source": [
        "predictor = ktrain.get_predictor(learner.model, preproc=transformer)\n",
        "predictor.predict('Had a lot of fun today!')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MkORO7iQcCIl"
      },
      "source": [
        "Did it work?\n",
        "\n",
        "Let's print out some information about the architecture of this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brb3vCVpcCia"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ag6GSzlohyph"
      },
      "source": [
        "We see that the distilbert model has fewer parameters than the BERT model we used previously. It's interesting to see the pre-classifier layer in this model that maps the output embeddings from the transformer into a second embedding space (of the same size) that is then used as features for the final classification layer.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mVGz2LalY93G"
      },
      "source": [
        "## Pairwise classification task \n",
        "\n",
        "We now use the Microsoft Research Paraphrase Corpus (MRPC) to build a classifier for detecting paraphrased sentences. \n",
        "- This is an example of a paired sentence classfication task, where the prediction model takes in a **pair of texts as input** and produces a **binary label as output**.\n",
        "\n",
        "We'll first use pandas (Python's data analysis library) to download and parse the tab-separated data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNyAj2W4yElI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "train_file = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt'\n",
        "test_file = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt'\n",
        "train_df = pd.read_csv(train_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "test_df = pd.read_csv(test_file, delimiter='\\t', quoting=csv.QUOTE_NONE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h0G1mLT1SpXR"
      },
      "source": [
        "We need to format the training data in the format needed for ktrain\n",
        "- so select the two columns containing strings \n",
        "- and get their values (as an 2d array) \n",
        "- then convert the array to a list of tuples (str,str) as required by ktrain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onWjDOIXSpXR"
      },
      "outputs": [],
      "source": [
        "x_train = train_df[['#1 String', '#2 String']].values\n",
        "# for sentence pair classification, ktrain expects a list of tuples of form (str, str)\n",
        "x_train = list(map(tuple, x_train))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CXbuIu7pSpXR"
      },
      "source": [
        "Repeat for the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u7EnB7WSpXR"
      },
      "outputs": [],
      "source": [
        "x_test = test_df[['#1 String', '#2 String']].values\n",
        "x_test = list(map(tuple, x_test))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2xjeE5LySpXR"
      },
      "source": [
        "And of course, we need also the labels for the training / test examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PInAix7dSpXR"
      },
      "outputs": [],
      "source": [
        "y_train = train_df['Quality'].values\n",
        "y_test = test_df['Quality'].values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h-47XE1nf5ia"
      },
      "source": [
        "Print out sizes of dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PXCUQtKe0Hk"
      },
      "outputs": [],
      "source": [
        "print(\"# train instances: \",len(x_train))\n",
        "print(\"# test instances:  \",len(x_train))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0jKOFDNqkkTR"
      },
      "source": [
        "Have a look at a random sentence pair:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYhG2fI0f-Y8"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "i = random.randint(0,len(x_train)-1)\n",
        "print(\"Sentance 1: \"+x_train[i][0])\n",
        "print(\"Sentence 2: \"+x_train[i][1])\n",
        "print(\"Label:      \"+str(y_train[i]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xuK5uMvlk25Z"
      },
      "source": [
        "Build and train a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FTGq3kgkq5P"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'bert-base-uncased'\n",
        "transformer = text.Transformer(MODEL_NAME, maxlen=128, class_names=['not paraphrase', 'paraphrase'])\n",
        "processed_train = transformer.preprocess_train(x_train, y_train)\n",
        "processed_test = transformer.preprocess_test(x_test, y_test)\n",
        "model = transformer.get_classifier()\n",
        "learner = ktrain.get_learner(model, train_data=processed_train, val_data=processed_test, batch_size=32)\n",
        "learner.fit_onecycle(5e-5, 3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PKlnZJim8L2L"
      },
      "source": [
        "Get the predictor object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBj4rbzdk-TR"
      },
      "outputs": [],
      "source": [
        "predictor = ktrain.get_predictor(learner.model, transformer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mKPEWtkUSpXS"
      },
      "source": [
        "And try the model on a pair of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vefxX5PllN8K"
      },
      "outputs": [],
      "source": [
        "#predictor.predict(('Their chestnut cat stretched out its legs on the antique wooden desk in the centre of his study.','The cat was on the table.'))\n",
        "predictor.predict(('The cat stretched out on the antique wooden desk in the study.','The cat was on the table.'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9hwiVBJD9Oci"
      },
      "source": [
        "Did it work? \n",
        "- Try it out on some of your own sentences!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5wdznm-9MEE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
