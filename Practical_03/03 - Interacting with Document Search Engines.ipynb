{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a4be7314-a04d-4a6b-8a11-64da4499be5e",
      "metadata": {
        "id": "a4be7314-a04d-4a6b-8a11-64da4499be5e"
      },
      "source": [
        "# Search engines\n",
        "\n",
        "This notebook shows how to use [PyTerrier](https://github.com/terrier-org/pyterrier) on the [CORD19 corpus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7251955/) and the [TREC Covid test collection](https://ir.nist.gov/covidSubmit/).\n",
        "Hereafter we are going to see how to:\n",
        "- index a collection\n",
        "- access an index\n",
        "- search an index\n",
        "- compare the performances of indexing approaches\n",
        "- learn ranking\n",
        "- evaluate ranking\n",
        "\n",
        "The notebook in mainly based on the tutorials at this [link](https://github.com/terrier-org/cikm2021tutorial/tree/main/notebooks), which are part of the tutorial series \"[IR From Bag-of-words to BERT and Beyond through Practical Experiments](https://github.com/terrier-org/cikm2021tutorial/)\" created for the [CIKM 2021](https://www.cikm2021.org/)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "40359018-8d88-448a-a57c-bb5ab0e2ffbf",
      "metadata": {
        "id": "40359018-8d88-448a-a57c-bb5ab0e2ffbf"
      },
      "source": [
        "## Tools installation and configuration\n",
        "\n",
        "PyTerrier is a Python framework, but uses the underlying [Terrier information retrieval toolkit](http://terrier.org) for many indexing and retrieval operations. While PyTerrier was new in 2020, Terrier is written in Java and has a long history dating back to 2001. PyTerrier makes it easy to perform IR experiments in Python, but using the mature Terrier platform for the expensive indexing and retrieval operations. \n",
        "\n",
        "PyTerrier is installed as follows. This might take a few minutes, in the meanwhile you can take a look at [PyTerrier documentation](https://pyterrier.readthedocs.io/en/latest/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ee08f366-961c-4403-b4db-d5ffe0ab791e",
      "metadata": {
        "id": "ee08f366-961c-4403-b4db-d5ffe0ab791e"
      },
      "outputs": [],
      "source": [
        "#!pip install -q python-terrier"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dbc6cec0-3ea7-4be9-8ca8-ec4054be5c6b",
      "metadata": {
        "id": "dbc6cec0-3ea7-4be9-8ca8-ec4054be5c6b"
      },
      "source": [
        "The next step is to initialise PyTerrier. This is performed using PyTerrier's `init()` method. The `init()` method is needed as PyTerrier must download Terrier's jar file and start the Java virtual machine. We prevent `init()` from being called more than once by checking `started()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "04f6a4f9-7ebb-4d2d-9ab0-0b4c37074a51",
      "metadata": {
        "id": "04f6a4f9-7ebb-4d2d-9ab0-0b4c37074a51"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "\n        Unable to find libjvm.so, (tried ['/usr/bin/java/jre/lib/jli/libjli.dylib', '/usr/bin/java/lib/jli/libjli.dylib', '/usr/bin/java/lib/libjli.dylib'])\n        you can use the JVM_PATH env variable with the absolute path\n        to libjvm.so to override this lookup, if you know\n        where pyjnius should look for it.\n\n        e.g:\n            export JAVA_HOME=/usr/lib/jvm/java-8-oracle/\n            export JVM_PATH=/usr/lib/jvm/java-8-oracle/jre/lib/amd64/server/libjvm.so\n            # run your program\n        ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyterrier\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pt\u001b[39m.\u001b[39mstarted():\n\u001b[0;32m----> 6\u001b[0m   pt\u001b[39m.\u001b[39;49minit()\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nlp/lib/python3.10/site-packages/pyterrier/__init__.py:128\u001b[0m, in \u001b[0;36minit\u001b[0;34m(version, mem, packages, jvm_opts, redirect_io, logging, home_dir, boot_packages, tqdm, no_download, helper_version)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m mem \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     jnius_config\u001b[39m.\u001b[39madd_options(\u001b[39m'\u001b[39m\u001b[39m-Xmx\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(mem) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mm\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjnius\u001b[39;00m \u001b[39mimport\u001b[39;00m autoclass, cast\n\u001b[1;32m    130\u001b[0m \u001b[39m# we only accept Java version 11 and newer; so anything starting 1. or 9. is too old\u001b[39;00m\n\u001b[1;32m    131\u001b[0m java_version \u001b[39m=\u001b[39m autoclass(\u001b[39m\"\u001b[39m\u001b[39mjava.lang.System\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetProperty(\u001b[39m\"\u001b[39m\u001b[39mjava.version\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nlp/lib/python3.10/site-packages/jnius/__init__.py:45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mjnius\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mreflect\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# XXX monkey patch methods that cannot be in cython.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m# Cython doesn't allow to set new attribute on methods it compiled\u001b[39;00m\n\u001b[1;32m     50\u001b[0m HASHCODE_MAX \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m31\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nlp/lib/python3.10/site-packages/jnius/reflect.py:19\u001b[0m\n\u001b[1;32m     14\u001b[0m __all__ \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mautoclass\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mensureclass\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprotocol_map\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m log \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m'\u001b[39m\u001b[39mkivy\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mgetChild(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mClass\u001b[39;00m(JavaClass, metaclass\u001b[39m=\u001b[39mMetaJavaClass):\n\u001b[1;32m     20\u001b[0m     __javaclass__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mjava/lang/Class\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     22\u001b[0m     desiredAssertionStatus \u001b[39m=\u001b[39m JavaMethod(\u001b[39m'\u001b[39m\u001b[39m()Z\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32mjnius/jnius_export_class.pxi:118\u001b[0m, in \u001b[0;36mjnius.MetaJavaClass.__new__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mjnius/jnius_export_class.pxi:178\u001b[0m, in \u001b[0;36mjnius.MetaJavaClass.resolve_class\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mjnius/jnius_env.pxi:11\u001b[0m, in \u001b[0;36mjnius.get_jnienv\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mjnius/jnius_jvm_dlopen.pxi:95\u001b[0m, in \u001b[0;36mjnius.get_platform_jnienv\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mjnius/jnius_jvm_dlopen.pxi:55\u001b[0m, in \u001b[0;36mjnius.create_jnienv\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nlp/lib/python3.10/site-packages/jnius/env.py:173\u001b[0m, in \u001b[0;36mJavaLocation.get_jnius_lib_location\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m             log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mfound libjvm.so at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, full_lib_location)\n\u001b[1;32m    171\u001b[0m             \u001b[39mreturn\u001b[39;00m full_lib_location\n\u001b[0;32m--> 173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    174\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m    Unable to find libjvm.so, (tried %s)\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39m    you can use the JVM_PATH env variable with the absolute path\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m    to libjvm.so to override this lookup, if you know\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m    where pyjnius should look for it.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[39m    e.g:\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m        export JAVA_HOME=/usr/lib/jvm/java-8-oracle/\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m        export JVM_PATH=/usr/lib/jvm/java-8-oracle/jre/lib/amd64/server/libjvm.so\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m        # run your program\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[39m%\u001b[39m [join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhome, loc) \u001b[39mfor\u001b[39;00m loc \u001b[39min\u001b[39;00m lib_locations]\n\u001b[1;32m    186\u001b[0m )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \n        Unable to find libjvm.so, (tried ['/usr/bin/java/jre/lib/jli/libjli.dylib', '/usr/bin/java/lib/jli/libjli.dylib', '/usr/bin/java/lib/libjli.dylib'])\n        you can use the JVM_PATH env variable with the absolute path\n        to libjvm.so to override this lookup, if you know\n        where pyjnius should look for it.\n\n        e.g:\n            export JAVA_HOME=/usr/lib/jvm/java-8-oracle/\n            export JVM_PATH=/usr/lib/jvm/java-8-oracle/jre/lib/amd64/server/libjvm.so\n            # run your program\n        "
          ]
        }
      ],
      "source": [
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "04e95091-47d6-48fa-a756-2f11a337135f",
      "metadata": {
        "id": "04e95091-47d6-48fa-a756-2f11a337135f"
      },
      "source": [
        "## Introduction to document indexing and searching"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "402410d9-eba8-4abb-aee5-c9612f8b9ed2",
      "metadata": {
        "id": "402410d9-eba8-4abb-aee5-c9612f8b9ed2"
      },
      "source": [
        "Much of PyTerrier's view of the world is wrapped up in Pandas dataframes. Let's consider some textual documents in a dataframe.\n",
        "\n",
        "We can start importing Pandas, a well known Python library to manage tabular data. \n",
        "You can find Pandas' documentation at the following [link](https://pandas.pydata.org).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503a65d2-3950-4459-84c8-bc0f2acd614e",
      "metadata": {
        "id": "503a65d2-3950-4459-84c8-bc0f2acd614e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "HQaVcXchuEDP",
      "metadata": {
        "id": "HQaVcXchuEDP"
      },
      "source": [
        "We can set the length of the displayed output to avoid truncating too much text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PDR9QyDZuDnq",
      "metadata": {
        "id": "PDR9QyDZuDnq"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 150)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "sDZd6boSuQFZ",
      "metadata": {
        "id": "sDZd6boSuQFZ"
      },
      "source": [
        "### Documents\n",
        "\n",
        "Finally we can create some sample documents to store in a `DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "takh-p7DuDjs",
      "metadata": {
        "id": "takh-p7DuDjs"
      },
      "outputs": [],
      "source": [
        "docs_df = pd.DataFrame(\n",
        "    [\n",
        "        [\"d1\", \"This is the first article in my collection of articles.\"],\n",
        "        [\"d2\", \"This is another article in my collection.\"],\n",
        "        [\"d3\", \"The topic of this third article is unknown.\"]\n",
        "    ], \n",
        "    columns=[\"docno\", \"text\"]\n",
        ")\n",
        "docs_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d1e0ebcb-59d9-4277-931c-76cc14700033",
      "metadata": {
        "id": "d1e0ebcb-59d9-4277-931c-76cc14700033"
      },
      "source": [
        "### Indexing\n",
        "\n",
        "Before any search engine can estimate which documents are most likely to be relevant for a given query, it must index the documents. \n",
        "\n",
        "In the following cell, we index the dataframe's documents. The index, with all its data structures, is written into a directory that we will call `index_3docs`. You can see it in the Google colab files folder on the left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab5ea93-51d0-48c8-8d1b-14801fb89890",
      "metadata": {
        "id": "0ab5ea93-51d0-48c8-8d1b-14801fb89890"
      },
      "outputs": [],
      "source": [
        "indexer = pt.DFIndexer(\"./index_3docs\", overwrite=True)\n",
        "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
        "index_ref.toString()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3fc9f92c-0ded-4ad4-bed2-6f91316be1cf",
      "metadata": {
        "id": "3fc9f92c-0ded-4ad4-bed2-6f91316be1cf"
      },
      "source": [
        "`IndexRef` provides the location where the index is stored. Indeed, we can look in the `index_3docs` directory and see that it has created various files: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08475457-19a9-4718-8395-f3b45dbefcdb",
      "metadata": {
        "id": "08475457-19a9-4718-8395-f3b45dbefcdb"
      },
      "outputs": [],
      "source": [
        "!ls -lh index_3docs/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e78ca805-a0fe-4765-8c84-44f3f47948e6",
      "metadata": {
        "id": "e78ca805-a0fe-4765-8c84-44f3f47948e6"
      },
      "source": [
        "We can now load the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662c4845-2ffe-4045-bd35-e4c7ac7c7127",
      "metadata": {
        "id": "662c4845-2ffe-4045-bd35-e4c7ac7c7127"
      },
      "outputs": [],
      "source": [
        "index = pt.IndexFactory.of(index_ref)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f1e4331c-ed8c-439a-8579-c5b6cb7b920a",
      "metadata": {
        "id": "f1e4331c-ed8c-439a-8579-c5b6cb7b920a"
      },
      "source": [
        "This is a Terrier index structure, which provides methods such as:\n",
        " - `getCollectionStatistics()`\n",
        " - `getInvertedIndex()`\n",
        " - `getLexicon()`\n",
        "\n",
        "Note Terrier is written in Java. (Yes, yes... I know... Java... 🤢) \n",
        "The Javadoc documentation is here: \n",
        "http://terrier.org/docs/current/javadoc/org/terrier/structures/Index.html\n",
        "\n",
        "Let's see what is returned by the `CollectionStatistics()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0b17a5-6cca-4c36-8b1a-b98b3a3026cc",
      "metadata": {
        "id": "ea0b17a5-6cca-4c36-8b1a-b98b3a3026cc"
      },
      "outputs": [],
      "source": [
        "print(index.getCollectionStatistics().toString())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4a62177b-0e9c-4227-8198-a493410e144c",
      "metadata": {
        "id": "4a62177b-0e9c-4227-8198-a493410e144c"
      },
      "source": [
        "Ok, that seems fair – we have 3 documents. But why only 6 terms? \n",
        "Let's check the [`Lexicon`](http://terrier.org/docs/current/javadoc/org/terrier/structures/Lexicon.html), which is our vocabulary. The `Lexicon` contains a set of terms, so we need to iterate over it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbea7b73-f1f3-4dd5-b329-0b27574b73a2",
      "metadata": {
        "id": "bbea7b73-f1f3-4dd5-b329-0b27574b73a2"
      },
      "outputs": [],
      "source": [
        "for kv in index.getLexicon():\n",
        "    print(kv.getKey(),\": \", kv.getValue().toString())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b76ef441-763d-4170-86d5-101b88fd1334",
      "metadata": {
        "id": "b76ef441-763d-4170-86d5-101b88fd1334"
      },
      "source": [
        "We see the terms and the statistics collected about each.\n",
        "Note that stopwords were removed and Porter's stemmer has been applied.\n",
        "Here:\n",
        " - `Nt` is the number of unique documents that each term occurs in – this is useful for calculating IDF.\n",
        " - `TF` is the total number of occurrences of term in corpus.\n",
        "\n",
        "Finally, we can use the square bracket notation to lookup terms in the lexicon:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5bf49e4-d291-44c0-a219-5da36d971d4f",
      "metadata": {
        "id": "e5bf49e4-d291-44c0-a219-5da36d971d4f"
      },
      "outputs": [],
      "source": [
        "index.getLexicon()[\"articl\"].toString()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "14c7b305-9ace-40dd-b0a7-7c75ff481c8f",
      "metadata": {
        "id": "14c7b305-9ace-40dd-b0a7-7c75ff481c8f"
      },
      "source": [
        "And we can see how many times this term is used in each document that it occurs in (by iterating over the posting lists):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918b5ad1-0946-43bd-af0d-4739d2efb3a4",
      "metadata": {
        "id": "918b5ad1-0946-43bd-af0d-4739d2efb3a4"
      },
      "outputs": [],
      "source": [
        "pointer = index.getLexicon()[\"articl\"]\n",
        "for posting in index.getInvertedIndex().getPostings(pointer):\n",
        "    print(f'{posting.toString()} doclen = {posting.getDocumentLength()}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c52d23dd-0c01-4259-8889-cbad18beb782",
      "metadata": {
        "id": "c52d23dd-0c01-4259-8889-cbad18beb782"
      },
      "source": [
        "We can see that `\"article\"` occurs in each of the three documents. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e49e1406-1b4f-435d-8cfe-8af48e7072e4",
      "metadata": {
        "id": "e49e1406-1b4f-435d-8cfe-8af48e7072e4"
      },
      "source": [
        "### Searching an Index\n",
        "\n",
        "Now that we have indexed our documents, we can run a search over the document collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6qxpZKldzShR",
      "metadata": {
        "id": "6qxpZKldzShR"
      },
      "outputs": [],
      "source": [
        "query = \"articles\"\n",
        "\n",
        "br = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
        "br.search(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f6037a43-b38f-4c26-9940-a73086a6327a",
      "metadata": {
        "id": "f6037a43-b38f-4c26-9940-a73086a6327a"
      },
      "source": [
        "Here we used the TF-IDF weighting formula to rank the results. \n",
        "\n",
        "The `search()` method returns a dataframe with columns:\n",
        " - `qid`: the query identifier\n",
        " - `docid`: integer identifier for document \n",
        " - `docno`: string identifier for document\n",
        " - `rank`: rank position\n",
        " - `score`: tf-idf score\n",
        " - `query`: the input query"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "mC-K7o3M1WSf",
      "metadata": {
        "id": "mC-K7o3M1WSf"
      },
      "source": [
        "### Multiple searches"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e00cffe9-5aff-4f75-b22d-b08dae32337b",
      "metadata": {
        "id": "e00cffe9-5aff-4f75-b22d-b08dae32337b"
      },
      "source": [
        "We can also run multiple queries at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4cd440-a836-4ad6-84f6-6691beab2a8e",
      "metadata": {
        "id": "0f4cd440-a836-4ad6-84f6-6691beab2a8e"
      },
      "outputs": [],
      "source": [
        "queries = pd.DataFrame([[\"query1\", \"articles\"], [\"query2\", \"first article\"], [\"query3\", \"unknown\"]], columns=[\"qid\", \"query\"])\n",
        "br(queries)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "MboNLbOo27WC",
      "metadata": {
        "id": "MboNLbOo27WC"
      },
      "source": [
        "## Loading a real dataset\n",
        "\n",
        "We'll now load a real dataset of COVID-19 related scientific articles (called CORD19) that is available as an example in PyTerrier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u-7Vg8zw7B2Q",
      "metadata": {
        "id": "u-7Vg8zw7B2Q"
      },
      "outputs": [],
      "source": [
        "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "t5xjK0xq7CHG",
      "metadata": {
        "id": "t5xjK0xq7CHG"
      },
      "source": [
        "We just downloaded the data. Now we index and save the dataset.\n",
        "\n",
        "The CORD19 corpus contains articles about the COVID-19, we are going to retrieve only the abstract of these articles to compose our documents. The indexing will take a while to run since there are almost 200,000 articles. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x7b0vM0D7Cr5",
      "metadata": {
        "id": "x7b0vM0D7Cr5"
      },
      "outputs": [],
      "source": [
        "pt_index_path = './terrier_cord19'\n",
        "indexer = pt.index.IterDictIndexer(pt_index_path, overwrite=True, meta_reverse=[])\n",
        "index_ref = indexer.index(cord19.get_corpus_iter(), fields=('abstract',), meta=('docno',))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fED8Z2iSscGz",
      "metadata": {
        "id": "fED8Z2iSscGz"
      },
      "source": [
        "### Querying the index"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fQVi-8qS7C6e",
      "metadata": {
        "id": "fQVi-8qS7C6e"
      },
      "source": [
        "Now load the index and print the statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d04aa3d-3e2e-44e4-a59a-7e0a6f5dfa4b",
      "metadata": {
        "id": "2d04aa3d-3e2e-44e4-a59a-7e0a6f5dfa4b"
      },
      "outputs": [],
      "source": [
        "index = pt.IndexFactory.of(index_ref)\n",
        "print(index.getCollectionStatistics().toString())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "84f85be8-e40b-4b2e-a31c-50ce45da17d2",
      "metadata": {
        "id": "84f85be8-e40b-4b2e-a31c-50ce45da17d2"
      },
      "source": [
        "Let's run a query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "742f697a-024f-438e-bab8-e6d988566f8c",
      "metadata": {
        "id": "742f697a-024f-438e-bab8-e6d988566f8c"
      },
      "outputs": [],
      "source": [
        "query = \"chemical reactions\"\n",
        "\n",
        "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
        "tfidf.search(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "w6z7x4NjCpat",
      "metadata": {
        "id": "w6z7x4NjCpat"
      },
      "source": [
        "Instead of ranking documents by their TF-IDF score, we could also use the BM25 score which is known to produce high quality search results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RNuhtol_DAI8",
      "metadata": {
        "id": "RNuhtol_DAI8"
      },
      "outputs": [],
      "source": [
        "query = \"chemical reactions\"\n",
        "\n",
        "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
        "bm25.search(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Ea6siv1-DNMN",
      "metadata": {
        "id": "Ea6siv1-DNMN"
      },
      "source": [
        "Note that the first 5 documents in the ranking are the same as they were for TF-IDF. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0275cc8d-054e-40a5-b7ec-144221278d27",
      "metadata": {
        "id": "0275cc8d-054e-40a5-b7ec-144221278d27"
      },
      "source": [
        "### Evaluating retrieval\n",
        "\n",
        "So far, we have been creating search engine models, but we haven't decided if any of them are actually any good. In order to determine how good a ranking is, we need annotations telling us which documents are actually relevant for a particular query. \n",
        "\n",
        "The CORD19 dataset contains set of queries and relevance assessments (aka qrels)for this purpose. \n",
        "\n",
        "For historical reasons, the queries are called \"topics\". Let's have a look at the first 10 queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db23212b-0aae-496b-81e5-0b7ed7d83fcc",
      "metadata": {
        "id": "db23212b-0aae-496b-81e5-0b7ed7d83fcc"
      },
      "outputs": [],
      "source": [
        "queries = cord19.get_topics(variant='title')\n",
        "queries.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "i93myyCB7y8r",
      "metadata": {
        "id": "i93myyCB7y8r"
      },
      "source": [
        "And the first 10 relevance judgements (called qrels):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20696564-02f3-4a7c-a98f-8823e8ad9cf6",
      "metadata": {
        "id": "20696564-02f3-4a7c-a98f-8823e8ad9cf6"
      },
      "outputs": [],
      "source": [
        "qrels = cord19.get_qrels()\n",
        "qrels.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5oTfWKk1EO0V",
      "metadata": {
        "id": "5oTfWKk1EO0V"
      },
      "source": [
        "We can give a look at the distribution of the relevance scores in the data set. To do so we train the count of occurrences of each label, sort the values by label and plot these values in a bat plot (note the log-scale on the y-axis):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8X4lQsxLEN6Z",
      "metadata": {
        "id": "8X4lQsxLEN6Z"
      },
      "outputs": [],
      "source": [
        "qrels['label'].value_counts().plot(kind='bar', log=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "qHEz0AoqBKCJ",
      "metadata": {
        "id": "qHEz0AoqBKCJ"
      },
      "source": [
        "Note that there are some relevance judgements taking the value of -1. In the original documentation of the data set (https://ir-datasets.com/cord19.html) only the values 0, 1, and 2 are accepted, so the -1 must be errors in the annotation, since they are only 2 we can drop the corresponding qrels.\n",
        "\n",
        "To drop the undesired we simply have to retain all the qrels with a label different from -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fhukm7CnBDvt",
      "metadata": {
        "id": "fhukm7CnBDvt"
      },
      "outputs": [],
      "source": [
        "qrels = qrels[qrels['label'] != -1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "p8jkGZrzJDBl",
      "metadata": {
        "id": "p8jkGZrzJDBl"
      },
      "source": [
        "The qrels contain information on the relevance labels for the query-document pairs. To do our evaluation we need to load also the queries we are going to use to test our retrieval system. For historical reasons, queries are called \"topics\" in this context.\n",
        "\n",
        "Let's collect the topics in the CORD-19 corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NbzStci3JDPM",
      "metadata": {
        "id": "NbzStci3JDPM"
      },
      "outputs": [],
      "source": [
        "topics = cord19.get_topics(variant='title')\n",
        "topics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a5W37VWD_r5K",
      "metadata": {
        "id": "a5W37VWD_r5K"
      },
      "source": [
        "Now let us use these queries and relevance judgements to compare different retrieval functions to see how well they perform at ranking documents in the collection. We compare BM25 and TF-IDF in terms of two common ranking evaluation measures (MAP and NDCG), where higher values indicate a better ranking:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca22e3f-664e-44f8-93f1-f5012067b1c6",
      "metadata": {
        "id": "4ca22e3f-664e-44f8-93f1-f5012067b1c6"
      },
      "outputs": [],
      "source": [
        "pt.Experiment(\n",
        "    [tfidf, bm25],\n",
        "    topics,\n",
        "    qrels,\n",
        "    eval_metrics=[\"map\", \"ndcg\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "56a0e072-f4de-4d5e-8833-2ff9853e538e",
      "metadata": {
        "id": "56a0e072-f4de-4d5e-8833-2ff9853e538e"
      },
      "source": [
        "How were those scores calculated? We can have a look at the score and corresponding relevance labels for individual query document pairs as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UixfKFkGBalK",
      "metadata": {
        "id": "UixfKFkGBalK"
      },
      "outputs": [],
      "source": [
        "# Rank the documents for each query using the TF-IDF scoring function:\n",
        "results = tfidf(cord19.get_topics(variant='title'))\n",
        "# Add the relevance labels (qrels) to the table:\n",
        "results = results.merge(qrels, on=[\"qid\", \"docno\"], how=\"left\").fillna(0)\n",
        "# Display the output \n",
        "results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d40d782f-16c4-435f-afa7-6c1670a6e6a7",
      "metadata": {
        "id": "d40d782f-16c4-435f-afa7-6c1670a6e6a7"
      },
      "source": [
        "## Pipelines\n",
        "\n",
        "For this part of the notebook, we'll be using a different (pre-built) Terrier index with term position information, which was built uisng the following code. (We wont run it now since it would take a while to run.)\n",
        "```python\n",
        "pt_index_path = './terrier_cord19_blocks'\n",
        "indexer = pt.index.IterDictIndexer(pt_index_path blocks=True)\n",
        "index_ref = indexer.index(cord19.get_corpus_iter(), fields=('abstract',), meta=('docno',))\n",
        "```\n",
        "\n",
        "However, its just as quick to use the pre-built index from the Terrier Data Repository. We use the ['terrier_stemmed_positions'](http://data.terrier.org/trec-covid.dataset.html#terrier_stemmed_positions) index variant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b72f9561-0528-4fba-9bbd-6141893f7f81",
      "metadata": {
        "id": "b72f9561-0528-4fba-9bbd-6141893f7f81"
      },
      "outputs": [],
      "source": [
        "index = pt.IndexFactory.of(\n",
        "    pt.get_dataset('trec-covid').get_index('terrier_stemmed_positions')\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f8aca53a-7400-4e16-9376-3021aa2fa6f4",
      "metadata": {
        "id": "f8aca53a-7400-4e16-9376-3021aa2fa6f4"
      },
      "source": [
        "#### Operators\n",
        "\n",
        "`BatchRetrieve` objects can be combined using some special operators. These combinations are called pipelines. \n",
        "\n",
        "Hereafter we are going to see three operators:\n",
        "- Composition\n",
        "- Rank cut-off\n",
        "- Union\n",
        "\n",
        "Before moving to the actual operators se define three retrievers using the index we have just created. We are going to define retrievers using \n",
        "- TF\n",
        "- TF-IDF\n",
        "- BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ksj0ZbhfM0dN",
      "metadata": {
        "id": "ksj0ZbhfM0dN"
      },
      "outputs": [],
      "source": [
        "tf = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
        "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
        "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "GM-0tm5IKope",
      "metadata": {
        "id": "GM-0tm5IKope"
      },
      "source": [
        "##### Composition\n",
        "\n",
        "The first operator we are going to see is the composition. This operator allow to re-rank the output of one retriever using a second retriever. \n",
        "\n",
        "To use it, we simply need to compose two retrievers using the  `>>` operator. \n",
        "\n",
        "Given the first query in the topics we loaded before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SWyPXyNnMjvN",
      "metadata": {
        "id": "SWyPXyNnMjvN"
      },
      "outputs": [],
      "source": [
        "query = 'chemical'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Cco7fHUjRzD_",
      "metadata": {
        "id": "Cco7fHUjRzD_"
      },
      "source": [
        "Let's see first what TF alone would retrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "idCuPT4LR7ZR",
      "metadata": {
        "id": "idCuPT4LR7ZR"
      },
      "outputs": [],
      "source": [
        "tf.search(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "zrCD4nAUSFei",
      "metadata": {
        "id": "zrCD4nAUSFei"
      },
      "source": [
        "Now let's compose the TF with the BM25, in this way we will re-rank the TF-IDF results using BM25.\n",
        "\n",
        "First we create a composition an then run the search using the composition pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0J7XcB2SZ0c",
      "metadata": {
        "id": "c0J7XcB2SZ0c"
      },
      "outputs": [],
      "source": [
        "composition = tf >> bm25\n",
        "\n",
        "composition.search(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4q4WNwteTu3Z",
      "metadata": {
        "id": "4q4WNwteTu3Z"
      },
      "source": [
        "As you can see some documents have been further re-ordered by the BM25"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "mtgVU3MBKotE",
      "metadata": {
        "id": "mtgVU3MBKotE"
      },
      "source": [
        "##### Rank cut-off\n",
        "\n",
        "The second operator we are going to see is the rank cut-off. This operator allow to retain only the top *n* results. \n",
        "\n",
        "To use it, we simply need to compose a new retriever applying the `% n` operator to an existing retriever (with *n* being the number of results we want to retain. This is useful to do an early pruning of the the retreived results."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "vcLzf9upMkOt",
      "metadata": {
        "id": "vcLzf9upMkOt"
      },
      "source": [
        "Let's retain the top 10 results using the TF-IDF score.\n",
        "\n",
        "First we define the rank cut-off pipeline using the TF retriever, then we use it to answer the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GhQoxlhYMl2m",
      "metadata": {
        "id": "GhQoxlhYMl2m"
      },
      "outputs": [],
      "source": [
        "tf_idf_cut = tf_idf % 10\n",
        "\n",
        "tf_idf_cut.search(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "nQmuujHwUB6z",
      "metadata": {
        "id": "nQmuujHwUB6z"
      },
      "source": [
        "Of course these pipeline operators can be combined. For example we can re-rank with BM25 only the first 20 results from TF.\n",
        "\n",
        "Again we define first our pipeline with the two operations and then we search the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gKAk7mWeUCK0",
      "metadata": {
        "id": "gKAk7mWeUCK0"
      },
      "outputs": [],
      "source": [
        "combination_cut = (tf % 20) >> bm25\n",
        "\n",
        "combination_cut.search(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "MnU2jq-2Kofs",
      "metadata": {
        "id": "MnU2jq-2Kofs"
      },
      "source": [
        "##### Results union\n",
        "\n",
        "Finally we can also combine the results from different retreivers, merging them together. In this case we are going to define the pipeline using two retrievers interleaved by the `|` operator.\n",
        "\n",
        "For example we can combine the top 10 results from TF with the top 10 results from TF-IDF. As usual we define first our pipeline and then we run our query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b728468-4838-4eb2-82e6-593f7c4c25f4",
      "metadata": {
        "id": "1b728468-4838-4eb2-82e6-593f7c4c25f4"
      },
      "outputs": [],
      "source": [
        "union = (tf % 10) | (tf_idf % 10)\n",
        "\n",
        "union.search(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "BkTBiuUDWthN",
      "metadata": {
        "id": "BkTBiuUDWthN"
      },
      "source": [
        "There are some overlappings between the results of TF and TF-IDf, as a results the total number of retreived documents is lower than 20.\n",
        "Also nothe that we do not have scores here since TF and TF-IDF are not comparable.\n",
        "\n",
        "What we can do is add som re-ranking to our pipeline. We can re-score the output given by the union of TF (with cut-off at 10) and TF-IDF (with cut-off at 10) using BM25 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k7XRhcEIXOaa",
      "metadata": {
        "id": "k7XRhcEIXOaa"
      },
      "outputs": [],
      "source": [
        "pipeline = ((tf % 10) | (tf_idf % 10)) >> bm25\n",
        "\n",
        "pipeline.search(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f27e3fed-52f5-433b-af2c-4a2feec26369",
      "metadata": {
        "id": "f27e3fed-52f5-433b-af2c-4a2feec26369"
      },
      "source": [
        "#### Evaluating retireval pipelines\n",
        "\n",
        "How do these complex retrieval pipelines perform? \n",
        "\n",
        "We can compare the MAP and NDCG scores on the topics avaialble on the CORD-19 data set using the qrels we loaded before.\n",
        "\n",
        "Let's compare\n",
        "- BM25 (used as baseline)\n",
        "- TF-IDF re-scored with BM25\n",
        "- the union of TF-IDF and TF top 500 results re-scored with BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7fa3b49-e71f-482c-83ad-b2cd2c684810",
      "metadata": {
        "id": "c7fa3b49-e71f-482c-83ad-b2cd2c684810"
      },
      "outputs": [],
      "source": [
        "pt.Experiment(\n",
        "  [bm25, tf_idf >> bm25, ((tf % 500) | (tf_idf % 500)) >> bm25],\n",
        "  topics,\n",
        "  qrels,\n",
        "  eval_metrics=[\"map\", \"ndcg\"],\n",
        "  names=[\"BM25\", \"TF-IDF >> BM25\", \"((TF % 10) | (TF-IDF % 10)) >> BM25\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1284131b-b125-4322-adec-ffba6f6aa791",
      "metadata": {
        "id": "1284131b-b125-4322-adec-ffba6f6aa791"
      },
      "source": [
        "## Learning to Rank\n",
        "\n",
        "In this last part of the notebook, you will experience constructing, learning, evaluating and analysing learning to rank pipelines.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "373f9203-2f7a-4962-84ca-7c1103b6716f",
      "metadata": {
        "id": "373f9203-2f7a-4962-84ca-7c1103b6716f"
      },
      "source": [
        "Firstly, lets split out topics into train, validation and test sets. TREC Covid only has 50 topics, which isnt a lot for training. We'll split this 30 for training 5 for validation and 15 for test. We will also examine statistical significance, even if this is artificial for 15 topics.\n",
        "\n",
        "We're only going to-rank the top 10 documents for each query - hopefully learning to rank can help to re-rank the top 10 documents to be more effective."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "qY1PV92rL4k8",
      "metadata": {
        "id": "qY1PV92rL4k8"
      },
      "source": [
        "We define some constants to controthe cutoff and make the experiments reproducible "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-WiLHCGDMFGU",
      "metadata": {
        "id": "-WiLHCGDMFGU"
      },
      "outputs": [],
      "source": [
        "RANK_CUTOFF = 10\n",
        "SEED = 42"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6ATh25ITMFjx",
      "metadata": {
        "id": "6ATh25ITMFjx"
      },
      "source": [
        "Then we use the splitting utility from Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ce455c-2244-40d5-b179-6343b02ddaf8",
      "metadata": {
        "id": "a3ce455c-2244-40d5-b179-6343b02ddaf8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tr_va_topics, test_topics = train_test_split(topics, test_size=15, random_state=SEED)\n",
        "train_topics, valid_topics =  train_test_split(tr_va_topics, test_size=5, random_state=SEED)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bbe513d6-ea0b-4c23-87c6-b3621e85ae32",
      "metadata": {
        "id": "bbe513d6-ea0b-4c23-87c6-b3621e85ae32"
      },
      "source": [
        "### Feature Set\n",
        "\n",
        "Lets define our feature set.  We're going to have a total of 5 features:\n",
        "\n",
        "1.   the BM25 abstract score;\n",
        "2.   the BM25 score on the title;\n",
        "3.   whether the abstract contain 'coronavirus covid', scored by BM25;\n",
        "4.   whether the paper was released/published in 2020 (Recent papers were more useful for this task);\n",
        "5.   whether the paper is a formal publication? (we can get this information checking whether the paper has a special identifier called DOI)\n",
        "\n",
        "Several of these feature require additional metadata `[\"title\", \"date\", \"doi\"]`. Fortunately, the TREC Covid dataset allows us to obtain more metadata after indexing. We use `pt.text.get_text(cord19, [\"title\", \"date\", \"doi\"])` to retrieve these extra metadata columns.\n",
        "\n",
        "Note the complete pipeline definition is a bit complex, just keep in mind we are interested in extracting the features we just listed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "704e0590-3543-4062-8cc5-d7fcd935d7f9",
      "metadata": {
        "id": "704e0590-3543-4062-8cc5-d7fcd935d7f9"
      },
      "outputs": [],
      "source": [
        "ltr_feats = (bm25 % RANK_CUTOFF) >> pt.text.get_text(cord19, [\"title\", \"date\", \"doi\"]) >> (\n",
        "    pt.transformer.IdentityTransformer()\n",
        "    ** # score of title (not originally indexed)\n",
        "    (pt.text.scorer(body_attr=\"title\", takes='docs', wmodel='BM25') ) \n",
        "    ** # score of text for query 'coronavirus covid'\n",
        "    (pt.apply.query(lambda row: 'coronavirus covid') >> bm25)\n",
        "    ** # date 2020\n",
        "    (pt.apply.doc_score(lambda row: int(\"2020\" in row[\"date\"])))\n",
        "    ** # has doi\n",
        "    (pt.apply.doc_score(lambda row: int( row[\"doi\"] is not None and len(row[\"doi\"]) > 0)))\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "GUXfNxVnMVc1",
      "metadata": {
        "id": "GUXfNxVnMVc1"
      },
      "source": [
        "For reference, we create a list with the names of the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "skRMV9zxMVu2",
      "metadata": {
        "id": "skRMV9zxMVu2"
      },
      "outputs": [],
      "source": [
        "fnames=[\"BM25 abstract\", \"BM25 title\", \"coronavirus covid\", \"2020\", \"DOI\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "79b222dd-b5c5-475b-85b6-b1046c95673e",
      "metadata": {
        "id": "79b222dd-b5c5-475b-85b6-b1046c95673e"
      },
      "source": [
        "Lets see the output for a given query. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54da689a-87da-49de-9c29-3d9464d5dcd2",
      "metadata": {
        "id": "54da689a-87da-49de-9c29-3d9464d5dcd2"
      },
      "outputs": [],
      "source": [
        "ltr_feats.search(\"coronovirus origin\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "LyXgL6Ppb3nV",
      "metadata": {
        "id": "LyXgL6Ppb3nV"
      },
      "source": [
        "We can see that we now have extra document metadata columns `[\"title\", \"date\", \"doi\"]`, as well as the `\"features\"` column. This last column contains the array of features we are going to use in the learning step"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c208a118-b0ab-48e0-8cd5-b5239d28f735",
      "metadata": {
        "id": "c208a118-b0ab-48e0-8cd5-b5239d28f735"
      },
      "source": [
        "We can also look at the raw features values (in this case for the first ranked document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd0719cd-b1f7-45bf-8562-5e4e59705369",
      "metadata": {
        "id": "fd0719cd-b1f7-45bf-8562-5e4e59705369"
      },
      "outputs": [],
      "source": [
        "print('Features:', ltr_feats.search(\"coronovirus origin\").iloc[0][\"features\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b3ca43cd-0117-4d4b-97e0-d709544e0dc8",
      "metadata": {
        "id": "b3ca43cd-0117-4d4b-97e0-d709544e0dc8"
      },
      "source": [
        "### Learning \n",
        "\n",
        "In this part of the notebook, we apply three different learning to rank techniques:\n",
        "\n",
        " - coordinate ascent from FastRank, a listwise linear technique\n",
        " - random forests from `scikit-learn`, a pointwise regression tree technique\n",
        " - LambdaMART from LightGBM, a listwise regression tree technique\n",
        "\n",
        "In each case, we take our feature pipeline, `ltr_feats1`, and we compose it (`>>`) with the learned model. We use `pt.ltr.apply_learned_model()` which knows how to deal with different learners.\n",
        "\n",
        "The full pipeline is then fitted (learned) using `.fit()`, specifying the training topics and qrels. Importantly, the preceeding stages of the pipeline (retrieval and feature calculation) are applied to the training topics in order to obtained the results, which are then passed to the learning to rank technique. LightGBM has early stopping enabled, which uses a validation topics set – similarly the validation topics are transformed into validation results.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "eyoFDFtmOWLs",
      "metadata": {
        "id": "eyoFDFtmOWLs"
      },
      "source": [
        "#### Linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5db2e6-75da-4996-9d9a-a1fadd9bd0ac",
      "metadata": {
        "id": "1e5db2e6-75da-4996-9d9a-a1fadd9bd0ac"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "lr_pipe = ltr_feats >> pt.ltr.apply_learned_model(lr)\n",
        "\n",
        "lr_pipe.fit(train_topics, qrels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "J2sRScAAOYVN",
      "metadata": {
        "id": "J2sRScAAOYVN"
      },
      "source": [
        "#### Non-linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecdba77d-4ccb-4d8a-97c9-98ec613ca2c5",
      "metadata": {
        "id": "ecdba77d-4ccb-4d8a-97c9-98ec613ca2c5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=400, verbose=1, random_state=SEED, n_jobs=-1)\n",
        "\n",
        "rf_pipe = ltr_feats >> pt.ltr.apply_learned_model(rf)\n",
        "\n",
        "rf_pipe.fit(train_topics, qrels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "-lhoSLadOwRl",
      "metadata": {
        "id": "-lhoSLadOwRl"
      },
      "source": [
        "#### LambdaMART\n",
        "\n",
        "LambdaMART is a listwise appraoch to learning to rank documents. It makes use of Gradient Boosted Regression trees to optimize an approximation to the NDCG evaluation function. \n",
        "\n",
        "To learn a LambdaMART model we first need to install [LightGBM](https://lightgbm.readthedocs.io/en/latest/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef77b500-d9d1-4e18-955b-91b7bd1b87a2",
      "metadata": {
        "id": "ef77b500-d9d1-4e18-955b-91b7bd1b87a2"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade lightgbm==3.1.1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "S1_2kLbo_Zwj",
      "metadata": {
        "id": "S1_2kLbo_Zwj"
      },
      "source": [
        "Now we can train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b163cd2-50a8-4c7a-8c46-538e6b12fc18",
      "metadata": {
        "id": "4b163cd2-50a8-4c7a-8c46-538e6b12fc18"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# This configures LightGBM as LambdaMART\n",
        "lmart_l = lgb.LGBMRanker(\n",
        "    task=\"train\",\n",
        "    silent=False,\n",
        "    min_data_in_leaf=1,\n",
        "    min_sum_hessian_in_leaf=1,\n",
        "    max_bin=255,\n",
        "    num_leaves=31,\n",
        "    objective=\"lambdarank\",\n",
        "    metric=\"ndcg\",\n",
        "    ndcg_eval_at=[10],\n",
        "    ndcg_at=[10],\n",
        "    eval_at=[10],\n",
        "    learning_rate= .1,\n",
        "    importance_type=\"gain\",\n",
        "    num_iterations=100,\n",
        "    early_stopping_rounds=5\n",
        ")\n",
        "\n",
        "lmart_x_pipe = ltr_feats >> pt.ltr.apply_learned_model(lmart_l, form=\"ltr\", fit_kwargs={'eval_at':[10]})\n",
        "\n",
        "lmart_x_pipe.fit(train_topics, qrels, valid_topics, qrels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cd4e2ca1-b5dd-4efa-a5ba-b3d7ff96e41e",
      "metadata": {
        "id": "cd4e2ca1-b5dd-4efa-a5ba-b3d7ff96e41e"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Lets now compare our ranking pipelines on our 15 topics with the BM25 baseline. \n",
        "\n",
        "We'll report MAP, NDCG measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d679e187-5ee5-4317-a33e-3987be34fd01",
      "metadata": {
        "id": "d679e187-5ee5-4317-a33e-3987be34fd01"
      },
      "outputs": [],
      "source": [
        "pt.Experiment(\n",
        "    [bm25 % RANK_CUTOFF, lr_pipe, rf_pipe, lmart_x_pipe],\n",
        "    test_topics,\n",
        "    qrels, \n",
        "    names=[\"BM25\",  \"BM25 + LinearRegression\", \"BM25 + RandomForest\", \"BM25 + LambdaMART\"],\n",
        "    eval_metrics=[\"map\", \"ndcg\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "eb37d682-775a-490c-9fc9-2d4dcc86b374",
      "metadata": {
        "id": "eb37d682-775a-490c-9fc9-2d4dcc86b374"
      },
      "source": [
        "The non-linear regression model produced the best results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "72d55188-67d2-4756-8266-801dd5ecd707",
      "metadata": {
        "id": "72d55188-67d2-4756-8266-801dd5ecd707"
      },
      "source": [
        "### Analysis\n",
        "\n",
        "We can also analyze our learned models to understand the role of the different features we are using.\n",
        "\n",
        "One way to do this analysis is to plot feature weights or importance from the different learned models. For the Linear Regression model, we plot the feature weights, while for the Random Forest and LambdaMART we report the feature importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f82e7673-bd05-40ac-8520-4d995e2fc756",
      "metadata": {
        "id": "f82e7673-bd05-40ac-8520-4d995e2fc756"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create figure\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(12, 8))\n",
        "\n",
        "# Plot Linear Regression model weights\n",
        "ax0.bar(np.arange(len(fnames)), lr.coef_)\n",
        "ax0.set_xticks(np.arange(len(fnames)))\n",
        "ax0.set_xticklabels(fnames, rotation=45, ha='right')\n",
        "ax0.set_title(\"Linear Regression\")\n",
        "ax0.set_ylabel(\"Feature Weights\")\n",
        "# Plot Random Forest feature importance\n",
        "ax1.bar(np.arange(len(fnames)), rf.feature_importances_)\n",
        "ax1.set_xticks(np.arange(len(fnames)))\n",
        "ax1.set_xticklabels(fnames, rotation=45, ha='right')\n",
        "ax1.set_title(\"Random Forest\")\n",
        "ax1.set_ylabel(\"Feature Importance\")\n",
        "# Plot LmbdaMART feature importance\n",
        "ax2.bar(np.arange(len(fnames)), lmart_l.feature_importances_)\n",
        "ax2.set_xticks(np.arange(len(fnames)))\n",
        "ax2.set_xticklabels(fnames, rotation=45, ha='right')\n",
        "ax2.set_title(\"$\\lambda$MART\")\n",
        "ax2.set_ylabel(\"Feature Importance\")\n",
        "\n",
        "# Display figure\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dlwy5-BV1upE",
      "metadata": {
        "id": "dlwy5-BV1upE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
