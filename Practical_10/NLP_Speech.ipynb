{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial on Speech\n",
        "\n",
        "In this tutorial we will investigate the representation of speech using audio signals and spectrograms, before making use of trained speech to text models. Finally we will make use of text to speech models to generate new audio."
      ],
      "metadata": {
        "id": "ZwiGubwRWdmi"
      },
      "id": "ZwiGubwRWdmi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Representing Audio Signals\n",
        "In this section we will investigate audio signals and their various representations. (This part of the tutorial is based on code written by Dr Vincenzo Scotti.)"
      ],
      "metadata": {
        "id": "xbQvlJafUcmR"
      },
      "id": "xbQvlJafUcmR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Loading and Visualising a Signal \n",
        "\n",
        "We'll start by loading the audio signal in the WAV file that came with the tutorial. Upload the file to Google colab and then load it as follows:"
      ],
      "metadata": {
        "id": "XJHeCHUzgTUj"
      },
      "id": "XJHeCHUzgTUj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948fed02-e76e-4dfc-a22f-24de12462bc9",
      "metadata": {
        "id": "948fed02-e76e-4dfc-a22f-24de12462bc9"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "file_path = 'audio.wav'\n",
        "audio, sr = librosa.load(file_path, sr=16000)\n",
        "audio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the audio signal is just a big array of floats, repersenting a time series of pressure values. The librosa library that we used to load the signal has resampled the signal at a sampling rate we specified with the parameter 'sr'.\n",
        "\n",
        "We can visualise the time series as a graph:"
      ],
      "metadata": {
        "id": "9IU7rWYKXFK5"
      },
      "id": "9IU7rWYKXFK5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b5cd0c-18dc-466c-bca7-657f601db33b",
      "metadata": {
        "id": "a8b5cd0c-18dc-466c-bca7-657f601db33b"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "matplotlib.rcParams.update({'font.size': 12}) ## make the font of the axes ticks/labels a bit bigger\n",
        "\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(audio, linewidth=0.4)\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlabel('Samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see a typical time series for speech. \n",
        "The x-axis is the sample and the y-axis is the amplitude of the signal. \n",
        "\n",
        "We can change the x-axis to be time once we know the duration of the time series, which we can get as follows:"
      ],
      "metadata": {
        "id": "XFjlHMieaatR"
      },
      "id": "XFjlHMieaatR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "495f03c0-6888-402f-9c1d-9396f61f1e9f",
      "metadata": {
        "id": "495f03c0-6888-402f-9c1d-9396f61f1e9f"
      },
      "outputs": [],
      "source": [
        "duration = librosa.get_duration(y=audio, sr=sr)\n",
        "duration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the audio clip runs for just over 4 seconds. \n",
        "\n",
        "We can now regenerate the graph with the x-axis being time in seconds:"
      ],
      "metadata": {
        "id": "FQrVgr3XcdzX"
      },
      "id": "FQrVgr3XcdzX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9dcf54-b823-4dc5-8466-1538dc4d0230",
      "metadata": {
        "id": "5b9dcf54-b823-4dc5-8466-1538dc4d0230"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "num_samples = audio.shape[0]\n",
        "time = np.linspace(0, duration, num_samples)\n",
        "\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(time, audio, linewidth=0.4)\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can listen to the audio signal in colab as follows:"
      ],
      "metadata": {
        "id": "QnRYbSB9a2wH"
      },
      "id": "QnRYbSB9a2wH"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio(audio, rate=sr)"
      ],
      "metadata": {
        "id": "_c4JKYPqa9ER"
      },
      "id": "_c4JKYPqa9ER",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit goes to Dr Vincenzo Scotti for the choice of the audio clip ;-)"
      ],
      "metadata": {
        "id": "Bxi9ea1MbAGG"
      },
      "id": "Bxi9ea1MbAGG"
    },
    {
      "cell_type": "markdown",
      "id": "ad8fb25c-1b69-41e3-9730-a40f621b2b50",
      "metadata": {
        "tags": [],
        "id": "ad8fb25c-1b69-41e3-9730-a40f621b2b50"
      },
      "source": [
        "### 1.2 Representing the signal in the Frequency domain\n",
        "\n",
        "When representing audio signals we often convert them to the frequency domain using the Fast Fourier Transform algorithm. We'll do that now to have a look at the signal in terms of its frequencies. (Note we'll take a small slice of the signal to do this.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55be224b-7b67-4f22-a262-1dab8866a8cc",
      "metadata": {
        "id": "55be224b-7b67-4f22-a262-1dab8866a8cc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "w = int(math.ceil(0.025 * sr)) # use a window of size 25ms\n",
        "d = int(math.ceil(0.01 * sr)) # and a delay between consecutive windows of 10ms\n",
        "t = int(math.ceil(1.2 * sr))\n",
        "\n",
        "n_fft = 512 # use a Fast Fourier transform of size 512 samples\n",
        "spec = librosa.stft(y=audio, n_fft=n_fft, win_length=w, hop_length=d,)\n",
        "freq = np.linspace(0,sr/2,spec.shape[0])\n",
        "s = int(math.ceil(t/d))\n",
        "\n",
        "fig = plt.figure(figsize=(8,4))\n",
        "plt.plot(freq, np.abs(spec)[:, s])\n",
        "plt.ylabel('Magnitude')\n",
        "plt.xlabel('Frequency [Hz]')\n",
        "plt.xlim([freq[0], freq[-1]])\n",
        "plt.ylim([-1, 15])\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically we represent the importance of each frequency by the number of decibells, which is related to the logarithm of the amplitute of each frequency, (more precisily it is 10 times the logarithm of the squared amplitude of the signal, which is the engergy of the signal). "
      ],
      "metadata": {
        "id": "vBUZtldL730s"
      },
      "id": "vBUZtldL730s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc4cc30-5599-4de4-8a3e-3781847ff28b",
      "metadata": {
        "id": "5cc4cc30-5599-4de4-8a3e-3781847ff28b"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8,4))\n",
        "plt.plot(freq, librosa.power_to_db(np.abs(spec) ** 2, ref=np.max)[:, s])\n",
        "plt.ylabel('Power [dB]')\n",
        "plt.xlabel('Frequency [Hz]')\n",
        "plt.xlim([freq[0], freq[-1]])\n",
        "plt.ylim([-80, 0])\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the higher frequency components of the signal are then highlighted by the decibel conversion. "
      ],
      "metadata": {
        "id": "S3uSDaiP8fCq"
      },
      "id": "S3uSDaiP8fCq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speech consists of many different sounds one after another. So the typical representation of a speech signal is to divide the original signal up into short overlapping segments and then calculate the frequency distribution for each. Each segment is usually smoothed with a (Hann) windowing function. We can visualise this process as follows. First let's define the window size, delay and start time:"
      ],
      "metadata": {
        "id": "PFcN2dH5yWPA"
      },
      "id": "PFcN2dH5yWPA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50ed956d-d604-4b5e-94f3-4f317dc181d0",
      "metadata": {
        "id": "50ed956d-d604-4b5e-94f3-4f317dc181d0"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "w = int(math.ceil(0.025 * sr)) # use a window of size 25ms\n",
        "d = int(math.ceil(0.01 * sr)) # and a delay between consecutive windows of 10ms\n",
        "t = int(math.ceil(1.2 * sr)) # start at 1.2 seconds into the signal "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plot a small segment of the audio signal with the windowing applied:"
      ],
      "metadata": {
        "id": "srifLnhwzO5p"
      },
      "id": "srifLnhwzO5p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21044a18-d4fc-4964-a297-c1e9e5833142",
      "metadata": {
        "id": "21044a18-d4fc-4964-a297-c1e9e5833142"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(16, 8), sharex=True, sharey=True)\n",
        "axes[0].plot(time[t:t+(2*w)], audio[t:t+(2*w)])\n",
        "axes[0].set_ylabel('Amplitude')\n",
        "axes[0].set_xlim([time[t], time[t+(2*w)]])\n",
        "axes[0].grid(True)\n",
        "\n",
        "for i in range(3):\n",
        "    axes[i + 1].plot(time[t+(i*d):t+(i*d)+w], audio[t+(i*d):t+(i*d)+w], c='tab:blue', label='Speech segment')\n",
        "    axes[i + 1].plot(time[t+(i*d):t+(i*d)+w], audio[t+(i*d):t+(i*d)+w] * librosa.filters.get_window('hann', w), c='tab:red', label='Hann windowed')\n",
        "    axes[i + 1].set_ylabel('Amplitude')\n",
        "    axes[i + 1].grid(True)\n",
        "    axes[i + 1].legend(loc='best')\n",
        "\n",
        "axes[3].set_xlabel('Time [s]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having seen the visualisation, and understanding what the ... we can now directly call a method that will create the Spectrogram. "
      ],
      "metadata": {
        "id": "loZbvzrcnmva"
      },
      "id": "loZbvzrcnmva"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98434d94-12d5-4b54-83d5-de318cb08511",
      "metadata": {
        "id": "98434d94-12d5-4b54-83d5-de318cb08511"
      },
      "outputs": [],
      "source": [
        "n_fft = 512 # use a Fast Fourier transform of size 512 samples\n",
        "spec = librosa.stft(y=audio, n_fft=n_fft, win_length=w, hop_length=d,)\n",
        "spec.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have not converted a one-dimensional time series (audio signal) into a 2 dimenionsional matrix (spectrogram). \n",
        "We can visualise the spectrogram as follows:"
      ],
      "metadata": {
        "id": "rd7WdY6R0y9e"
      },
      "id": "rd7WdY6R0y9e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921b9e4b-c99b-407d-99b5-c2775dd47f65",
      "metadata": {
        "id": "921b9e4b-c99b-407d-99b5-c2775dd47f65"
      },
      "outputs": [],
      "source": [
        "freq = np.linspace(0, sr / 2, spec.shape[0])\n",
        "spec_time = np.linspace(0, duration, spec.shape[1])\n",
        "\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "plt.pcolormesh(spec_time, freq, librosa.power_to_db(np.abs(spec) ** 2, ref=np.max))\n",
        "plt.colorbar(label='Power [dB]')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0608aa4f-5163-4646-ae1c-16c3d18bbf2c",
      "metadata": {
        "tags": [],
        "id": "0608aa4f-5163-4646-ae1c-16c3d18bbf2c"
      },
      "source": [
        "### 1.3 The Mel spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39ed5a4-b56d-4ec4-a505-de618ade1a35",
      "metadata": {
        "id": "e39ed5a4-b56d-4ec4-a505-de618ade1a35"
      },
      "outputs": [],
      "source": [
        "n_mel = 80 \n",
        "lin_freq = librosa.mel_frequencies(n_mels=n_mel, fmax=sr / 2)\n",
        "\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(lin_freq, np.arange(n_mel))\n",
        "plt.ylabel('Mel frequency')\n",
        "plt.xlabel('Linear frequency [Hz]')\n",
        "plt.xlim([0, lin_freq[-1]])\n",
        "plt.ylim([0, 80])\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79bc6434-9ffd-4f26-b7c5-bc2497d3bff1",
      "metadata": {
        "id": "79bc6434-9ffd-4f26-b7c5-bc2497d3bff1"
      },
      "outputs": [],
      "source": [
        "mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=n_fft, win_length=w, hop_length=d, n_mels=n_mel)\n",
        "mel_spec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5353d68-213c-4aa0-9610-9cb67aca0706",
      "metadata": {
        "id": "d5353d68-213c-4aa0-9610-9cb67aca0706"
      },
      "outputs": [],
      "source": [
        "mel_comp = np.arange(mel_spec.shape[0]) + 1\n",
        "mel_spec_time = np.linspace(0, duration, mel_spec.shape[1])\n",
        "\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "plt.pcolormesh(mel_spec_time, mel_comp, librosa.power_to_db(mel_spec, ref=np.max))\n",
        "plt.colorbar(label='Power [dB]')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.yticks(ticks=mel_comp[9::10], labels=[f'{freq:.0f}' for freq in lin_freq[9::10]])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Record voice"
      ],
      "metadata": {
        "id": "CFd0kNAZeo_v"
      },
      "id": "CFd0kNAZeo_v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use code to record audio from here: https://colab.research.google.com/gist/ricardodeazambuja/03ac98c31e87caf284f7b06286ebf7fd/microphone-to-numpy-array-from-your-browser-in-colab.ipynb"
      ],
      "metadata": {
        "id": "U_tgefl_ktbL"
      },
      "id": "U_tgefl_ktbL"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg-python"
      ],
      "metadata": {
        "id": "xvdND400evT2"
      },
      "id": "xvdND400evT2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "To write this piece of code I took inspiration/code from a lot of places.\n",
        "It was late night, so I'm not sure how much I created or just copied o.O\n",
        "Here are some of the possible references:\n",
        "https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/\n",
        "https://stackoverflow.com/a/18650249\n",
        "https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/\n",
        "https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/\n",
        "https://stackoverflow.com/a/49019356\n",
        "\"\"\"\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };            \n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "  \n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr"
      ],
      "metadata": {
        "id": "74J-RuQCfHHU"
      },
      "id": "74J-RuQCfHHU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio, sr = get_audio()"
      ],
      "metadata": {
        "id": "eqGN7XAfhQPF"
      },
      "id": "eqGN7XAfhQPF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = audio.shape[0]\n",
        "time = np.linspace(0, duration, num_samples)\n",
        "\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(time, audio, linewidth=0.4)\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A7ORPJsfmHTm"
      },
      "id": "A7ORPJsfmHTm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can save this audio recording for later use ..."
      ],
      "metadata": {
        "id": "4aRodRTQ--d2"
      },
      "id": "4aRodRTQ--d2"
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "scipy.io.wavfile.write('recording.wav', sr, audio)"
      ],
      "metadata": {
        "id": "PEjqh4bynf7I"
      },
      "id": "PEjqh4bynf7I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy code from above to generate the Mel Spectrogram for your own voice. "
      ],
      "metadata": {
        "id": "6RAzo59V_Ibi"
      },
      "id": "6RAzo59V_Ibi"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MPTaw-bx_QtP"
      },
      "id": "MPTaw-bx_QtP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Speech Recognition\n",
        "\n",
        "Now that we know how to represent audio signals in a way that highlights the different components of the speech, we can try to make use of techniques for recognising speech sounds in the signal. \n",
        "\n",
        "We will make use of a state-of-the-art transformer-based speech recognition system from OpanAI called Whisper: https://github.com/openai/whisper\n",
        "\n",
        "\n",
        "\n",
        "Let's first install the Python library:"
      ],
      "metadata": {
        "id": "ZEoQElNoKDkB"
      },
      "id": "ZEoQElNoKDkB"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper"
      ],
      "metadata": {
        "id": "GQBSPk2IKiJO"
      },
      "id": "GQBSPk2IKiJO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whisper models come pretrained with various model sizes. We will download one of the smaller models, called \"base\", which is only 139MB. Feel free to try a larger model!"
      ],
      "metadata": {
        "id": "ZjsGliqeAcBE"
      },
      "id": "ZjsGliqeAcBE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "913a523a-a5a6-4f7e-b0de-7db82a0c8b2b",
      "metadata": {
        "id": "913a523a-a5a6-4f7e-b0de-7db82a0c8b2b"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the model, we can use it for transcribing audio to text. Let's start with our original WAV file. "
      ],
      "metadata": {
        "id": "kJ03X6weBBaY"
      },
      "id": "kJ03X6weBBaY"
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\"audio.wav\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "KCRRb0XgKf0g"
      },
      "id": "KCRRb0XgKf0g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you see a warning (about FP16 not being supported) just ignore it. It's because you are running Whisper on the CPU rather than a GPU, but there's no need for a GPU for the moment. \n",
        "\n",
        "How did it go? Did it transcribe the text correctly? \n",
        "(If not, why not, and what could be done about it?) "
      ],
      "metadata": {
        "id": "NW3kknogBO3n"
      },
      "id": "NW3kknogBO3n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Detecting the spoken language\n",
        "\n",
        "The model we downloaded for Whisper can transcribe text not only in English but in MANY languages. \n",
        "\n",
        "As part of the functionality Whisper can detect the spoken language, so let's try it. \n",
        "\n",
        "We will now follow the pipeline for processing the audio that was hidden from us when we directly called the \"transcribe()\" method above. The first step is to detect the speech in the audio signal and trim or pad the signal to a certain length (30 seconds):"
      ],
      "metadata": {
        "id": "uwA0iVK-LNje"
      },
      "id": "uwA0iVK-LNje"
    },
    {
      "cell_type": "code",
      "source": [
        "# load audio and pad/trim it to fit 30 seconds\n",
        "audio = whisper.load_audio(\"audio.wav\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "audio"
      ],
      "metadata": {
        "id": "EuWez8clLrZ7"
      },
      "id": "EuWez8clLrZ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(audio, linewidth=0.4)\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlabel('Samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rykfDt5kHN87"
      },
      "id": "rykfDt5kHN87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to compute the Mel Spectrogram:"
      ],
      "metadata": {
        "id": "zA4rilE5C1cd"
      },
      "id": "zA4rilE5C1cd"
    },
    {
      "cell_type": "code",
      "source": [
        "# make log-Mel spectrogram and move to the same device as the model\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "mel"
      ],
      "metadata": {
        "id": "tpw8AqYWLx8U"
      },
      "id": "tpw8AqYWLx8U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualise the spectrogram:"
      ],
      "metadata": {
        "id": "NzxaS5kCDFFW"
      },
      "id": "NzxaS5kCDFFW"
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "plt.pcolormesh(mel.cpu().numpy())\n",
        "plt.colorbar(label='Power [dB]')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.xlabel('Time [10ms]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0vmxkUd_CgAm"
      },
      "id": "0vmxkUd_CgAm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the signal has been padded out to 30 seconds.\n",
        "\n",
        "Now let's use Whisper to detect the language based on the audio clip:"
      ],
      "metadata": {
        "id": "sdNKNA6ZD8oe"
      },
      "id": "sdNKNA6ZD8oe"
    },
    {
      "cell_type": "code",
      "source": [
        "_, probs = model.detect_language(mel)\n",
        "lang = max(probs, key=probs.get)\n",
        "print(f\"Detected language: {lang}, confidence: {probs[lang]}\")"
      ],
      "metadata": {
        "id": "bAPjupEAMI32"
      },
      "id": "bAPjupEAMI32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the model is pretty certain that the language is English!\n",
        "\n",
        "Although it had quite a few languages to choose from. Have a look:"
      ],
      "metadata": {
        "id": "AQRWyPPfFL2B"
      },
      "id": "AQRWyPPfFL2B"
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "id": "6JNZEx4rNOys"
      },
      "id": "6JNZEx4rNOys",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try loading the second audio sample from the tutorial \"italiano.mp3\" and see if the model can correctly determine the language. "
      ],
      "metadata": {
        "id": "aodS21gKFkA9"
      },
      "id": "aodS21gKFkA9"
    },
    {
      "cell_type": "code",
      "source": [
        "audio2 = whisper.load_audio(\"italiano.mp3\")\n",
        "\n",
        "Audio(audio2, rate=sr)"
      ],
      "metadata": {
        "id": "BtxaS5YkFxGT"
      },
      "id": "BtxaS5YkFxGT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened?? Try changing the sample rate (sr) to 16000. -- Whisper has resampled the audio at a new rate of 16kHz. \n",
        "\n",
        "Any better?"
      ],
      "metadata": {
        "id": "Cvp52SZjGr6t"
      },
      "id": "Cvp52SZjGr6t"
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(audio2, rate=16000)"
      ],
      "metadata": {
        "id": "4kNJuke0G3rs"
      },
      "id": "4kNJuke0G3rs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now copy the code from above to predict the language: "
      ],
      "metadata": {
        "id": "WOO3DSXEGLmf"
      },
      "id": "WOO3DSXEGLmf"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vmBWCNoSIR8l"
      },
      "id": "vmBWCNoSIR8l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did it work? How confident was the model that the language was Italian?\n",
        "\n",
        "Now we can try to transcribe the Italian audio:"
      ],
      "metadata": {
        "id": "r9nCNXj-IRPU"
      },
      "id": "r9nCNXj-IRPU"
    },
    {
      "cell_type": "code",
      "source": [
        "audio2 = whisper.pad_or_trim(audio2)\n",
        "mel = whisper.log_mel_spectrogram(audio2).to(model.device)\n",
        "\n",
        "# decode the audio\n",
        "options = whisper.DecodingOptions(fp16 = False)\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(result.text)\n"
      ],
      "metadata": {
        "id": "WrHQ7Z3_Mqna"
      },
      "id": "WrHQ7Z3_Mqna",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How good is Whisper at understanding Fantozzi?"
      ],
      "metadata": {
        "id": "Losokx3sULBf"
      },
      "id": "Losokx3sULBf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Transcribing own audio\n",
        "\n",
        "Try transcribing the audio that you produced before. Does it work?"
      ],
      "metadata": {
        "id": "ygix5A-rJzo7"
      },
      "id": "ygix5A-rJzo7"
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\"recording.wav\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "Kt0V_91fnoDP"
      },
      "id": "Kt0V_91fnoDP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 Text to Speech\n"
      ],
      "metadata": {
        "id": "kGFIRmVQWRX_"
      },
      "id": "kGFIRmVQWRX_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll make use of the Tacotron2 system following this notebook: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/nvidia_deeplearningexamples_tacotron2.ipynb\n",
        "\n",
        "To run the following code you will need to use a GPU, so change the runtime above to include one if you haven't already."
      ],
      "metadata": {
        "id": "Ou1IOcaYooaR"
      },
      "id": "Ou1IOcaYooaR"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy scipy librosa unidecode inflect\n",
        "#!apt-get update\n",
        "#!apt-get install -y libsndfile1"
      ],
      "metadata": {
        "id": "TXH8Ue_JWXdp"
      },
      "id": "TXH8Ue_JWXdp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Tacotron model from Torchhub:"
      ],
      "metadata": {
        "id": "E7sutintWa_P"
      },
      "id": "E7sutintWa_P"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\n",
        "tacotron2 = tacotron2.to('cuda')\n",
        "tacotron2.eval()"
      ],
      "metadata": {
        "id": "DM8oU14cpVyv"
      },
      "id": "DM8oU14cpVyv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tacotron is a \"Spectrogram predictor\". It has been trained to input a text (or phonetic sequence) and output a mel spectrogram. A Vocoder is then needed to convert the spectrogram into an audio signal. In our case we will make use of the Waveglow vocoder for this purpose: https://github.com/NVIDIA/waveglow\n",
        "\n",
        "Load the waveglow model as follows:"
      ],
      "metadata": {
        "id": "InzoT7e4K6UL"
      },
      "id": "InzoT7e4K6UL"
    },
    {
      "cell_type": "code",
      "source": [
        "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp16')\n",
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow = waveglow.to('cuda')\n",
        "waveglow.eval()"
      ],
      "metadata": {
        "id": "5suFBguBqLGN"
      },
      "id": "5suFBguBqLGN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have all we need. Let's write some text that we can use to generate the audio:"
      ],
      "metadata": {
        "id": "RuRegWM6L0fH"
      },
      "id": "RuRegWM6L0fH"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello crazy world, I missed you so much.\""
      ],
      "metadata": {
        "id": "3h3_-UnHqgZe"
      },
      "id": "3h3_-UnHqgZe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That text is read in by the model and converted to a numeric sequence:"
      ],
      "metadata": {
        "id": "hkm_lnmPL6uk"
      },
      "id": "hkm_lnmPL6uk"
    },
    {
      "cell_type": "code",
      "source": [
        "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')\n",
        "sequences, lengths = utils.prepare_input_sequence([text])\n",
        "sequences"
      ],
      "metadata": {
        "id": "cpFHBA8dqqfi"
      },
      "id": "cpFHBA8dqqfi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Producing a Spectrogram\n",
        "\n",
        "Now convert the sequence into a mel spectrogram with Tacotron2"
      ],
      "metadata": {
        "id": "J8FrGO_KMIKN"
      },
      "id": "J8FrGO_KMIKN"
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    mel, _, _ = tacotron2.infer(sequences, lengths)"
      ],
      "metadata": {
        "id": "IBpnP8pyrTay"
      },
      "id": "IBpnP8pyrTay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a spectrogram:"
      ],
      "metadata": {
        "id": "pcd-bYIzMYTz"
      },
      "id": "pcd-bYIzMYTz"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "plt.pcolormesh(mel[0].cpu().numpy())\n",
        "plt.colorbar(label='Power [dB]')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.xlabel('Time [10ms]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UB6NkHwCMh48"
      },
      "id": "UB6NkHwCMh48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Producing Audio\n",
        "\n",
        "Now use Waveglow to produce the audio signal"
      ],
      "metadata": {
        "id": "JcAb5G0bM4_T"
      },
      "id": "JcAb5G0bM4_T"
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    audio = waveglow.infer(mel)\n",
        "audio_numpy = audio[0].data.cpu().numpy()\n",
        "rate = 22050\n",
        "audio_numpy"
      ],
      "metadata": {
        "id": "6M5MtK0KMURN"
      },
      "id": "6M5MtK0KMURN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look at the audio signal:"
      ],
      "metadata": {
        "id": "zzPl6cr-Ndj2"
      },
      "id": "zzPl6cr-Ndj2"
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(audio_numpy, linewidth=0.4)\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlabel('Samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m3UQP5BCP_gy"
      },
      "id": "m3UQP5BCP_gy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's listen to it!"
      ],
      "metadata": {
        "id": "xV6DqTXON6l0"
      },
      "id": "xV6DqTXON6l0"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio(audio_numpy, rate=rate)"
      ],
      "metadata": {
        "id": "UWYfTRBcsmLm"
      },
      "id": "UWYfTRBcsmLm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why not save the audio and see if the speech-to-text system can recognise it?"
      ],
      "metadata": {
        "id": "vufKJt_jNvyZ"
      },
      "id": "vufKJt_jNvyZ"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io.wavfile import write\n",
        "write(\"audio.wav\", rate, audio_numpy)"
      ],
      "metadata": {
        "id": "1-5yj3VcsdRJ"
      },
      "id": "1-5yj3VcsdRJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Want to try to generate other types of voices? Check out this code from Vincenzo Scotti: https://github.com/vincenzo-scotti/tts_mellotron_api"
      ],
      "metadata": {
        "id": "NRsCDxo8OW7V"
      },
      "id": "NRsCDxo8OW7V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The END. Hope you liked the Speech tutorial!"
      ],
      "metadata": {
        "id": "GN2rKPPlOE_b"
      },
      "id": "GN2rKPPlOE_b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}