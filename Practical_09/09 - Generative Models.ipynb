{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ee5ea9f-7715-4e08-b572-fa8116eb67f4",
   "metadata": {
    "id": "7ee5ea9f-7715-4e08-b572-fa8116eb67f4"
   },
   "source": [
    "# Generative models\n",
    "\n",
    "This tutorial will show how to use generative models to solve:\n",
    "- Question answering\n",
    "- Dialogue generation\n",
    "However, the same prinicples are applicable to \n",
    "\n",
    "Yay we made it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "zSNGW-JpWqVO",
   "metadata": {
    "id": "zSNGW-JpWqVO"
   },
   "source": [
    "Before starting I need to connect the drive storage to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IKB0LWYCWiku",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20598,
     "status": "ok",
     "timestamp": 1684163525503,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "IKB0LWYCWiku",
    "outputId": "e3396475-0fc7-4ae5-fce7-de2c9e2ca54e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80PFRRJ5XsLD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1684163525914,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "80PFRRJ5XsLD",
    "outputId": "d2efe73b-28a0-44b5-fb52-ddef17ec8131",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/NLP')\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf357018-5db6-4509-8ae8-dd3784f9023e",
   "metadata": {
    "id": "bf357018-5db6-4509-8ae8-dd3784f9023e"
   },
   "source": [
    "## Generative Question Answering (QA)\n",
    "\n",
    "Up to now we have seen how to retrieve relevant passages that may contain the answer to a question.\n",
    "- What if the answer is not written explicitly in the passage?\n",
    "- What if the passage is too long to read (and our lives are too dynamic to spend more than one minute reading)?\n",
    "\n",
    "We can train a model to output the answer to a question given \n",
    "- a relevant passage (and we know how to gather relevant documents)\n",
    "- the question (yes, the question contains relevant information to answer the question)\n",
    "... Or we can put a pre-trained model on top of our retrieval pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "809d9c7d-7c90-4144-b463-db21b2e21248",
   "metadata": {
    "id": "809d9c7d-7c90-4144-b463-db21b2e21248"
   },
   "source": [
    "### QA data preparation\n",
    "\n",
    "In this section we will be using the [WikiQA](https://aclanthology.org/D15-1237/) data set.\n",
    "It's a data set for open domain generative QA.\n",
    "\n",
    "It's avaialble via the HuggingFace data set package, let's install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db5f1d-1eb1-4f52-8c90-1e819bd311d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16426,
     "status": "ok",
     "timestamp": 1684162476749,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "65db5f1d-1eb1-4f52-8c90-1e819bd311d8",
    "outputId": "fd1f9b1c-a432-461a-d72f-699a3afc4e56",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cc87bcb-d94c-4060-8c2f-b13ab75fd3f4",
   "metadata": {
    "id": "6cc87bcb-d94c-4060-8c2f-b13ab75fd3f4"
   },
   "source": [
    "Let's download the validation split of the WikiQA data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b582af4f-05fb-4085-aaa8-2d83ea0d9212",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1684162586060,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "b582af4f-05fb-4085-aaa8-2d83ea0d9212",
    "outputId": "c6fea0ae-8df2-432c-86ad-cfd7421f33ab",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_qa (/home/arcslab/.cache/huggingface/datasets/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question_id': ['Q8', 'Q8', 'Q8', 'Q8', 'Q8', 'Q8', 'Q8', 'Q8', 'Q8', 'Q11'],\n",
       " 'question': ['How are epithelial tissues joined together?',\n",
       "  'How are epithelial tissues joined together?',\n",
       "  'How are epithelial tissues joined together?',\n",
       "  'How are epithelial tissues joined together?',\n",
       "  'How are epithelial tissues joined together?',\n",
       "  'How are epithelial tissues joined together?',\n",
       "  'How are epithelial tissues joined together?',\n",
       "  'How are epithelial tissues joined together?',\n",
       "  'How are epithelial tissues joined together?',\n",
       "  'how big is bmc software in houston, tx'],\n",
       " 'document_title': ['Tissue (biology)',\n",
       "  'Tissue (biology)',\n",
       "  'Tissue (biology)',\n",
       "  'Tissue (biology)',\n",
       "  'Tissue (biology)',\n",
       "  'Tissue (biology)',\n",
       "  'Tissue (biology)',\n",
       "  'Tissue (biology)',\n",
       "  'Tissue (biology)',\n",
       "  'BMC Software'],\n",
       " 'answer': ['Cross section of sclerenchyma fibers in plant ground tissue',\n",
       "  'Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .',\n",
       "  'In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .',\n",
       "  'A tissue is an ensemble of similar cells from the same origin that together carry out a specific function.',\n",
       "  'Organs are then formed by the functional grouping together of multiple tissues.',\n",
       "  'The study of tissue is known as histology or, in connection with disease, histopathology .',\n",
       "  'The classical tools for studying tissues are the paraffin block in which tissue is embedded and then sectioned, the histological stain , and the optical microscope .',\n",
       "  'In the last couple of decades, developments in electron microscopy , immunofluorescence , and the use of frozen tissue sections have enhanced the detail that can be observed in tissues.',\n",
       "  'With these tools, the classical appearances of tissues can be examined in health and disease, enabling considerable refinement of clinical diagnosis and prognosis .',\n",
       "  'BMC Software, Inc. is an American company specializing in Business Service Management (BSM) software.'],\n",
       " 'label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "wiki_qa = datasets.load_dataset('wiki_qa', split='validation')\n",
    "wiki_qa[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "G2h4Bb0T6OfR",
   "metadata": {
    "id": "G2h4Bb0T6OfR"
   },
   "source": [
    "The data set contains questions, title of documents in Wikipedia containing the answers and answers.\n",
    "Additionally the data set contains distractor answers for a given question.\n",
    "We can distinguish the two types of responses from the `label` value. \n",
    "\n",
    "Let's filter the data to retain only correct question-response pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee165bb0-5ed0-4d08-a566-d19af55d471b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1684163052329,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "ee165bb0-5ed0-4d08-a566-d19af55d471b",
    "outputId": "525e4918-37ed-4554-9426-35e0586652e7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7fea5e0ae950> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009022235870361328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 32,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd8c20ac24e41b49751452896ba2984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question_id': 'Q11',\n",
       " 'question': 'how big is bmc software in houston, tx',\n",
       " 'document_title': 'BMC Software',\n",
       " 'answer': 'Employing over 6,000, BMC is often credited with pioneering the BSM concept as a way to help better align IT operations with business needs.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_qa_correct = wiki_qa.filter(lambda x: x['label'] == 1)\n",
    "wiki_qa_correct[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "JZmYkfWU8Yyj",
   "metadata": {
    "id": "JZmYkfWU8Yyj"
   },
   "source": [
    "Ok now we have a lot of samples to try our system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3a3cd78-2102-4b0d-b1c6-2a4e7f18e635",
   "metadata": {
    "id": "c3a3cd78-2102-4b0d-b1c6-2a4e7f18e635"
   },
   "source": [
    "### Knowlege retrieval\n",
    "\n",
    "We have the questions (and the target answers), now we need to prepare our knowledge source and the retrieval system.\n",
    "We can re-use the simple Wikipedia and the two encoder models from last tutorial.\n",
    "\n",
    "Let's start loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0f600-1279-44d7-a4ef-b4ca4197c78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install transformers==4.22.2\n",
    "!pip -q install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727c3a4a-1164-4d66-9b8b-0a03190d0791",
   "metadata": {
    "id": "727c3a4a-1164-4d66-9b8b-0a03190d0791",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import util\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "JRYKGJY39O3X",
   "metadata": {
    "id": "JRYKGJY39O3X"
   },
   "source": [
    "We can try indexing all the paragraphs this time (hopefully it won't explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "EdifHBpo9PLX",
   "metadata": {
    "id": "EdifHBpo9PLX",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 509663 passages\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "# NOTE: Change this flag to use only first paragraph\n",
    "only_first = False\n",
    "\n",
    "passages = []\n",
    "# Open the file with the dump of Simple Wikipedia\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as f:\n",
    "    # Iterate over the lines\n",
    "    for line in f:\n",
    "        # Parse the document using JSON\n",
    "        data = json.loads(line.strip())\n",
    "        if only_first:\n",
    "            # Only add the first paragraph\n",
    "            passages.append(data['paragraphs'][0])\n",
    "        else:\n",
    "            # Add all paragraphs\n",
    "            passages.extend(data['paragraphs'])\n",
    "\n",
    "print(f\"Retrieved {len(passages)} passages\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "O_stcXFV80eL",
   "metadata": {
    "id": "O_stcXFV80eL"
   },
   "source": [
    "Now we can import the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Q2HlwF1P81I3",
   "metadata": {
    "id": "Q2HlwF1P81I3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "semb_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "xenc_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "thO1wWEq9ylz",
   "metadata": {
    "id": "thO1wWEq9ylz"
   },
   "source": [
    "Now let's embed the retrieved passages (We can checkpoint the embeddings to avoid repeating the computation each time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8DO9OtdA9xxf",
   "metadata": {
    "id": "8DO9OtdA9xxf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define hnswlib index path\n",
    "embeddings_cache_path = './qa_embeddings_cache.pkl'\n",
    "\n",
    "# Load cache if available\n",
    "if os.path.exists(embeddings_cache_path):\n",
    "    print('Loading embeddings cache')\n",
    "    with open(embeddings_cache_path, 'rb') as f:\n",
    "        corpus_embeddings = pickle.load(f)\n",
    "# Else compute embeddings\n",
    "else:\n",
    "    print('Computing embeddings')\n",
    "    corpus_embeddings = semb_model.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n",
    "    # Save the index to a file for future loading\n",
    "    print(f'Saving index to: \\'{embeddings_cache_path}\\'')\n",
    "    with open(embeddings_cache_path, 'wb') as f:\n",
    "        pickle.dump(corpus_embeddings, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1PmCJqc_x_S",
   "metadata": {
    "id": "d1PmCJqc_x_S"
   },
   "source": [
    "Finally let's index the embeddings (and let's sve the index hoping it works this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YEvmHHaiAF1E",
   "metadata": {
    "id": "YEvmHHaiAF1E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "uf3R0pcO_yN2",
   "metadata": {
    "id": "uf3R0pcO_yN2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hnswlib\n",
    "\n",
    "# Create empthy index\n",
    "index = hnswlib.Index(space='cosine', dim=384)\n",
    "\n",
    "# Define hnswlib index path\n",
    "index_path = './qa_hnswlib.index'\n",
    "\n",
    "# Load index if available\n",
    "if os.path.exists(index_path):\n",
    "    print('Loading index...')\n",
    "    index.load_index(index_path)\n",
    "# Else index data collection\n",
    "else:\n",
    "    # Initialise the index\n",
    "    print('Start creating HNSWLIB index')\n",
    "    index.init_index(max_elements=corpus_embeddings.size(0), ef_construction=400, M=64)\n",
    "    #  Compute the HNSWLIB index (it may take a while)\n",
    "    index.add_items(corpus_embeddings.cpu(), list(range(len(corpus_embeddings))))\n",
    "    # Save the index to a file for future loading\n",
    "    print(f'Saving index to: {index_path}')\n",
    "    index.save_index(index_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "IsuUSgT6AMX1",
   "metadata": {
    "id": "IsuUSgT6AMX1"
   },
   "source": [
    "Now we have almost all the tools to answer a question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "427fc98d-1d8b-412f-a804-574d6eac9201",
   "metadata": {
    "id": "427fc98d-1d8b-412f-a804-574d6eac9201"
   },
   "source": [
    "### Answering a question\n",
    "\n",
    "We are still missing the core of the QA system, the answering model.\n",
    "We are going to re-use a pre-trained model.\n",
    "\n",
    "[FLAN T5](https://arxiv.org/abs/2210.11416) is a pre-trained encode-decoder model trained to be used on different tasks in zero-shot or few-shots leaning settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Va63G93lJfSE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24213,
     "status": "ok",
     "timestamp": 1684218458947,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "Va63G93lJfSE",
    "outputId": "830e64e7-cd67-431d-9982-4621598f6b5b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install transformers sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "GssOsdFAGT9N",
   "metadata": {
    "id": "GssOsdFAGT9N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7sKz5f1TJKhr",
   "metadata": {
    "id": "7sKz5f1TJKhr"
   },
   "source": [
    "Let's check quickly that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wId9d-_7JK7m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1575,
     "status": "ok",
     "timestamp": 1684218910698,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "wId9d-_7JK7m",
    "outputId": "aac9fabd-cd23-4485-9c99-3ea78419a4ae",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> \"Vincenzo è il migliore\"</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Translate the following sentence from English to Italian: \\\"Vincenzo is the best\\\"\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=32)\n",
    "output_text = tokenizer.decode(output_ids[0])\n",
    "print(output_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8qKLHXYMGUKd",
   "metadata": {
    "id": "8qKLHXYMGUKd"
   },
   "source": [
    "Now we can test our pipeline. \n",
    "First select randomly a question from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f270d8-1cf4-4b39-8bd9-5312646378c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1684165929394,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "a0f270d8-1cf4-4b39-8bd9-5312646378c1",
    "outputId": "adde407b-9935-47d5-acc5-de00c8828359",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 116: What was Captain Ahab's Ship in the novel \"Moby Dick\"?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1995)\n",
    "\n",
    "idx = random.choice(range(len(wiki_qa_correct)))\n",
    "\n",
    "sample = wiki_qa_correct[idx]\n",
    "question = sample['question']\n",
    "target_answer = sample['answer']\n",
    "\n",
    "print(f'Question {idx}: {question}?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3waS9PiCSBcg",
   "metadata": {
    "id": "3waS9PiCSBcg"
   },
   "source": [
    "Embed the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "QfP51mwmSBwG",
   "metadata": {
    "id": "QfP51mwmSBwG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_embedding = semb_model.encode(question, convert_to_tensor=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "AtmId6HLGB8r",
   "metadata": {
    "id": "AtmId6HLGB8r"
   },
   "source": [
    "Retrieve relevant documents keeping top $k$ matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03e342af-1b52-46b4-8c44-d83f4c854a80",
   "metadata": {
    "id": "03e342af-1b52-46b4-8c44-d83f4c854a80",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity model search results\n",
      "Query: \"What was Captain Ahab's Ship in the novel \"Moby Dick\"\"\n",
      "---------------------------------------\n",
      "Score: 0.8052\n",
      "Document: \"Moby-Dick is a novel written by Herman Melville. It was first published in 1851. The story is told by a seaman named Ishmael. He sails on a whaling ship called the \"Pequod\". Ahab is the captain of the ship. He wants to kill a white whale called Moby Dick. The whale bit his leg off. The book received mixed reviews. \"Moby-Dick\" is now thought to be one of the greatest novels ever written.\"\n",
      "\n",
      "\n",
      "Score: 0.5937\n",
      "Document: \"Richard Basehart was an American actor. He played Admiral Harriman Nelson in the television series \"Voyage To The Bottom Of The Sea\". He played Ishmael in the 1956 movie \"Moby Dick\".\"\n",
      "\n",
      "\n",
      "Score: 0.5623\n",
      "Document: \"Herman Melville (August 1, 1819 – September 28, 1891) was an American novelist, short story writer, essayist and poet. He is best known for writing \"Moby-Dick\".\"\n",
      "\n",
      "\n",
      "Score: 0.5183\n",
      "Document: \"The book \"A Captain's Duty: Somali Pirates, Navy SEALs, and Dangerous Days at Sea\" (2010) by Stephan Talty and Captain Richard Phillips was published soon afterwards. The hijacking also inspired the 2013 movie \"Captain Phillips\" starring Tom Hanks.\"\n",
      "\n",
      "\n",
      "Score: 0.5030\n",
      "Document: \"A sperm whale rammed and sank the Nantucket whaleship Essex on 20 November 1820. The crew arrived at Henderson on 20 December in three small whaleboats. They found the island's only known drinkable water-source – a brackish spring on the north shore, exposed at half tide – and ate fish, birds, eggs, crabs and peppergrass, but they had largely exhausted the available food within a week. On 27 December the three boats set sail for South America, leaving behind Thomas Chappel, Seth Weeks, and William Wright, who chose to stay. They survived until their rescue on 9 April 1821. In his account of the ordeal, Chappel reported having seen human skeletons in a cave. The report of sperm whale ramming the ship may have inspired Herman Melville to write \"Moby-Dick\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_ids, distances = index.knn_query(question_embedding.cpu(), k=64)\n",
    "scores = 1 - distances\n",
    "\n",
    "print(\"Cosine similarity model search results\")\n",
    "print(f\"Query: \\\"{question}\\\"\")\n",
    "print(\"---------------------------------------\")\n",
    "for idx, score in zip(corpus_ids[0][:5], scores[0][:5]):\n",
    "    print(f\"Score: {score:.4f}\\nDocument: \\\"{passages[idx]}\\\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aH0PbWSGCxy",
   "metadata": {
    "id": "0aH0PbWSGCxy"
   },
   "source": [
    "Re-rank retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "_fdj_udmGD3G",
   "metadata": {
    "id": "_fdj_udmGD3G",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder model re-ranking results\n",
      "Query: \"What was Captain Ahab's Ship in the novel \"Moby Dick\"\"\n",
      "---------------------------------------\n",
      "Score: 9.1982\n",
      "Document: \"Moby-Dick is a novel written by Herman Melville. It was first published in 1851. The story is told by a seaman named Ishmael. He sails on a whaling ship called the \"Pequod\". Ahab is the captain of the ship. He wants to kill a white whale called Moby Dick. The whale bit his leg off. The book received mixed reviews. \"Moby-Dick\" is now thought to be one of the greatest novels ever written.\"\n",
      "\n",
      "\n",
      "Score: 0.2596\n",
      "Document: \"Richard Basehart was an American actor. He played Admiral Harriman Nelson in the television series \"Voyage To The Bottom Of The Sea\". He played Ishmael in the 1956 movie \"Moby Dick\".\"\n",
      "\n",
      "\n",
      "Score: 0.0798\n",
      "Document: \"Herman Melville wrote the famous novel Moby Dick, about a whaling in America. Moby Dick is often called the greatest American novels. Emily Dickinson is one of the greatest American poets. Dickinson wrote many lyrical, religious, philosophical poems. Walt Whitman wrote the epic poem Leaves of Grass. Masterpieces of Gothic fiction include Nathaniel Hawthorn's Fall of the House of Usher and The Scarlet Letter). Charlotte Perkins Gilman's feminist classic \"The Yellow Wallpaper\" is also Gothic fiction. Ralph Waldo Emerson and Henry David Thoreau wrote philosophical essays. Their philosophical school of thought is called Transcendentalism.\"\n",
      "\n",
      "\n",
      "Score: -0.3549\n",
      "Document: \"A sperm whale rammed and sank the Nantucket whaleship Essex on 20 November 1820. The crew arrived at Henderson on 20 December in three small whaleboats. They found the island's only known drinkable water-source – a brackish spring on the north shore, exposed at half tide – and ate fish, birds, eggs, crabs and peppergrass, but they had largely exhausted the available food within a week. On 27 December the three boats set sail for South America, leaving behind Thomas Chappel, Seth Weeks, and William Wright, who chose to stay. They survived until their rescue on 9 April 1821. In his account of the ordeal, Chappel reported having seen human skeletons in a cave. The report of sperm whale ramming the ship may have inspired Herman Melville to write \"Moby-Dick\"\"\n",
      "\n",
      "\n",
      "Score: -0.6143\n",
      "Document: \"Herman Melville (August 1, 1819 – September 28, 1891) was an American novelist, short story writer, essayist and poet. He is best known for writing \"Moby-Dick\".\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_inputs = [(question, passages[idx]) for idx in corpus_ids[0]]\n",
    "cross_scores = xenc_model.predict(model_inputs)\n",
    "\n",
    "print(\"Cross-encoder model re-ranking results\")\n",
    "print(f\"Query: \\\"{question}\\\"\")\n",
    "print(\"---------------------------------------\")\n",
    "for idx in np.argsort(-cross_scores)[:5]:\n",
    "    print(f\"Score: {cross_scores[idx]:.4f}\\nDocument: \\\"{passages[corpus_ids[0][idx]]}\\\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "-_hmhwoiGEXm",
   "metadata": {
    "id": "-_hmhwoiGEXm"
   },
   "source": [
    "Use best match to answer (and compare to reference answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "n0veVOPWGEtN",
   "metadata": {
    "id": "n0veVOPWGEtN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following passage, answer the related question.\n",
      "\n",
      "Passage:\n",
      "\n",
      "Moby-Dick is a novel written by Herman Melville. It was first published in 1851. The story is told by a seaman named Ishmael. He sails on a whaling ship called the \"Pequod\". Ahab is the captain of the ship. He wants to kill a white whale called Moby Dick. The whale bit his leg off. The book received mixed reviews. \"Moby-Dick\" is now thought to be one of the greatest novels ever written.\n",
      "\n",
      "Q: What was Captain Ahab's Ship in the novel \"Moby Dick\"? \n",
      "\n",
      "Pequod \n",
      "\n",
      "A (target): The story tells the adventures of wandering sailor Ishmael , and his voyage on the whaleship Pequod , commanded by Captain Ahab.\n"
     ]
    }
   ],
   "source": [
    "passage_idx = np.argsort(-cross_scores)[0]\n",
    "passage = passages[corpus_ids[0][passage_idx]]\n",
    "\n",
    "input_text = f\"Given the following passage, answer the related question.\\n\\nPassage:\\n\\n{passage}\\n\\nQ: {question}?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "print(input_text, \"\\n\")\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=32)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(output_text, \"\\n\")\n",
    "\n",
    "print(f\"A (target): {target_answer}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "XBCsWWE6KpAA",
   "metadata": {
    "id": "XBCsWWE6KpAA"
   },
   "source": [
    "How do we know if the passage was useful and the model haven't exploited weights memorisation?\n",
    "Let's try to generate directly the response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sIYDlm5kKpOC",
   "metadata": {
    "id": "sIYDlm5kKpOC",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question.\n",
      "\n",
      "Q: What was Captain Ahab's Ship in the novel \"Moby Dick\"?\n",
      "\n",
      "A: samuel\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"Answer the following question.\\n\\nQ: {question}?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "print(input_text)\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=32)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(f\"\\nA: {output_text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32ad7316-03ba-4f90-a847-774bc3543bd8",
   "metadata": {
    "id": "32ad7316-03ba-4f90-a847-774bc3543bd8"
   },
   "source": [
    "### Putting all together\n",
    "\n",
    "We can finally set up an entire question answering pipeline:\n",
    "- We have the knowledge\n",
    "- We have the retrieval system\n",
    "    - We also have the re-ranking system\n",
    "- We have the asnwering system\n",
    "\n",
    "Let's define a function that puts everything together and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88153175-be77-451b-8955-f5851a45d2a3",
   "metadata": {
    "id": "88153175-be77-451b-8955-f5851a45d2a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qa_pipeline(\n",
    "    question, \n",
    "    similarity_model=semb_model, \n",
    "    embeddings_index=index, \n",
    "    re_ranking_model=xenc_model, \n",
    "    generative_model=model,\n",
    "    device=device\n",
    "): \n",
    "    if not question.endswith('?'):\n",
    "        question = question + '?'\n",
    "    # Embed question\n",
    "    question_embedding = semb_model.encode(question, convert_to_tensor=True)\n",
    "    # Search documents similar to question in index \n",
    "    corpus_ids, distances = index.knn_query(question_embedding.cpu(), k=64)\n",
    "    # Re-rank results\n",
    "    xenc_model_inputs = [(question, passages[idx]) for idx in corpus_ids[0]]\n",
    "    cross_scores = xenc_model.predict(model_inputs)\n",
    "    # Get best matching passage\n",
    "    passage_idx = np.argsort(-cross_scores)[0]\n",
    "    passage = passages[corpus_ids[0][passage_idx]]\n",
    "    # Encode input\n",
    "    input_text = f\"Given the following passage, answer the related question.\\n\\nPassage:\\n\\n{passage}\\n\\nQ: {question}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Generate output\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=64)\n",
    "    # Decode output\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return result\n",
    "    return f\"Passage:\\n\\n{passage}\\n\\nQ: {question}\\n\\nA: {output_text}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3db6bb86-3d6c-4b2f-a963-5da9764593b6",
   "metadata": {
    "id": "3db6bb86-3d6c-4b2f-a963-5da9764593b6",
    "tags": []
   },
   "source": [
    "Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "kVX3SXdtFijU",
   "metadata": {
    "id": "kVX3SXdtFijU",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question >>>  What's the meaning of life?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Passage:\n",
      "\n",
      "Many have different opinions on what the meaning is. Some say life is a war zone where we are the soldiers fighting in that war for survival. Some think it is all about the relationships that we make in our life. Some people say that life is full of violence and hatred but some say that life is full of hope and happiness. Still, other people say that the meaning of life is to achieve the goals you set in life. According to Douglas Adams, the answer to the question is 42. The biological answer is to have children, which is to pass on your genes. Others say the meaning of life is simply to live your life to the fullest.\n",
      "\n",
      "Q: What's the meaning of life?\n",
      "\n",
      "A: To live your life to the fullest\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Ask a question >>> \")  # e.g., \"How many fingers in a hand?\", \"What's the meaning of life?\", ...\n",
    "print()\n",
    "\n",
    "print(qa_pipeline(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e15b6bb6-2714-4654-a2be-7fe4f9db2e1f",
   "metadata": {
    "id": "e15b6bb6-2714-4654-a2be-7fe4f9db2e1f"
   },
   "source": [
    "## Generative chatbots\n",
    "\n",
    "Language models can be used to generate text in dialogues.\n",
    "Now we are going to see how to use transformer language models as generative chatbots.\n",
    "\n",
    "As usual we are going to use the Transformer library from HuggingFace. \n",
    "All generative models there implement a `generate()` methods we are going to use.\n",
    "You can find the documentation here: https://huggingface.co/docs/transformers/main_classes/text_generation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42e15b7-4cbf-4953-9bba-e7f48d5bc4af",
   "metadata": {
    "id": "d42e15b7-4cbf-4953-9bba-e7f48d5bc4af"
   },
   "source": [
    "### Pretrained models\n",
    "\n",
    "For starter let's play around with a pre-trained model.\n",
    "We can load the [DialoGPT](https://arxiv.org/abs/1911.00536) chatbot, a fine-tuning of GPT-2 trained o large collections of conversations crawled from Reddit.\n",
    "\n",
    "We can start seeing different ways to decode (generate) responses using this autoregressive model.\n",
    "What we want to do is use the output probability distribution to select a token compsing a response.\n",
    "Hopefully we select the most probable sequence, actually that's not feasible.\n",
    "\n",
    "Let's proceed step-by-step.\n",
    "First of all get model and tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add29d0-c57b-4543-85b5-d85ce5a4923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e38cc8-c30a-4fe6-97eb-e24e99401d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip -q install transformers==4.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d201fd9-c66d-429f-84f5-ec7090c13f93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229,
     "referenced_widgets": [
      "ea6e7a8957754378bb4e4a87f460207a",
      "f012d92822dc46bebd6881fe48d0312c",
      "5bd9e91e79814b54a7a6545ab57d4e41",
      "d618a30185c74432aee3907ec8b1225d",
      "1d70dcadb8f74194b088e4dfdea0ae55",
      "736c682a759242888297a3c1b7504c65",
      "58f3cb124dcf45438a749da2678e6a32",
      "32629bccb57048c4a28cee33eefdf8de",
      "7e53b5a609514709afd747a034f53400",
      "f99f6290ca3640a18a9fbe41980f3411",
      "f43e68bb14bc49f4a78491a525461689",
      "a20c80af13fb4037852605f82f714cd1",
      "cf40ac805ffd421d9ecd1db6894efee0",
      "0022b9d7c82b478e8c17bc56edd3cab5",
      "eacb7cc951434c72bddcd5754a4ae78a",
      "843c830297594fe7aa7cacd4fea6bfc3",
      "405205e401bf4972b3ed1ff0e11e5af8",
      "afec3e01ffaf438a92cb11b358694f42",
      "34730f6f972c47a18809b1bacd550364",
      "4cfd24194cde45d2967b17808dea993e",
      "76105038289540a68a560379a6c361e5",
      "de1d5b5b97364df28408441b99209c99",
      "94075dee90e2425689176dbe91cef4ee",
      "0e70451ca4c3478ea8bd32b628f5ea8a",
      "48a8325dc4a443e5a9bc467bf3df2f78",
      "3511be86d8934ab4ac64ed3d8e02a65f",
      "8495ad273edf4ebfadc8bc0d377ae8dd",
      "c62e94fde69f46afa505eb24ba03be3d",
      "6ce125927e8245e990d5606a516b472f",
      "6ba8155b35924dfcbdce02828e4ed64c",
      "7c14cc02a6c144bc9f8b491973b719a7",
      "37784f58d647426698ba34a9e96bbb34",
      "c84d5275fe0845a29916d77adf34630c",
      "757e467c74f14465b743219e68a65326",
      "0f3643c0b3d644b2889e27d51b302fbe",
      "a1aa44f1301d4295b63e3cbefcbc86f3",
      "2acba93c4b21404ca367f44c13dc1002",
      "e316fa98c07241f38057e42c471fdee7",
      "664782bff2714ee4b365de17130e1245",
      "6d05e6d754b84e49ab70b21b34a276f3",
      "dc91ac92549546f2b528acdc5d7cd86f",
      "5190a6e308084dafb92ca9e8efb26e48",
      "9e21e71dd8a3454fbebf7d3ca25df966",
      "a60568f613ca4bcd8da9150a54f60eb3",
      "83177b6659ba46ee8a9b8a88dc90f098",
      "8c6650c299b24c37b0c95b6e60c08125",
      "e220b17e036e41f8807230e321baf524",
      "f244f885a9fa4265b20b88dddbe7f37e",
      "15d33819bd21408898dd600aff77323c",
      "93207531a2d04714ba2c0a955c61a67d",
      "552aa9a8d19947b9bb8ef71769649a22",
      "b9cd29f61c8b428f97e32e140c0250ef",
      "34f84e904398483092b6e64d3e382a6f",
      "1cb3e1aedf8a416380ba288e333ce925",
      "328ae82f282c4cccbc324c30a409d536",
      "6043c4db0f2f45209fb146fc8914ece8",
      "981cf3b90fa642819254afc204a4e6ae",
      "18a1b79006ca47d8b1e7e74385ace43b",
      "1a3aea788dfc42c884296fbde19757dc",
      "5a2300a1029c4a4bb28bea94a09c72de",
      "7a696bac8dee40f1b3d8010636b254ed",
      "5a23e7c48eb34a65bd5b07b6db00a3da",
      "4bfb2488f3284728be62cf92e3782ead",
      "1c807338e6e74c3195213ddb03868617",
      "f15b974cbc8847be9047faff6a86a630",
      "5ca0ea332a754fedaba0df7fd3f8f0f8"
     ]
    },
    "executionInfo": {
     "elapsed": 28952,
     "status": "ok",
     "timestamp": 1684220909991,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "8d201fd9-c66d-429f-84f5-ec7090c13f93",
    "outputId": "18137c3d-2dca-4ede-8b78-b5230e5e76af",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\", device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f7a3e35-8a2c-420c-86d1-bd832c1c1b8c",
   "metadata": {
    "id": "8f7a3e35-8a2c-420c-86d1-bd832c1c1b8c"
   },
   "source": [
    "#### How does it work?\n",
    "\n",
    "First we need to understand how to provide data to our model\n",
    "\n",
    "Up to a couple of years ago, the standard appraoch to present the input to these models was to separate each utterance with a `end-of-sequence` token.\n",
    "The model would generate an answer and stop every time the `end-of-sequence` tokens is generated.\n",
    "\n",
    "```\n",
    "\"<|endoftext|>Summer loving had me a blast<|endoftext|>Summer loving happened so fast<|endoftext|>I met a girl crazy for me<|endoftext|>Met a boy cute as can be<|endoftext|>\"\n",
    "```\n",
    "\n",
    "Nowadays the appraoch is to have an uninterrupted stream of text, like a movie script\n",
    "\n",
    "```\n",
    "\"\n",
    "A: Hello.\n",
    "B: Is it me you're looking for?\n",
    "A: I can see it in your eyes...\n",
    "B: I can see it in your smile!\n",
    "\"\n",
    "```\n",
    "\n",
    "DialoGPT uses the `end-of-sequence` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "BmPTK6_FaWab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1684221375540,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "BmPTK6_FaWab",
    "outputId": "f50a3b7b-fad5-4271-d8f4-e93ebeb62f50",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "CNa-cqd6aW7s",
   "metadata": {
    "id": "CNa-cqd6aW7s"
   },
   "source": [
    "Let's create a context to use as input for our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9017b027-48e2-49c9-9947-efc7175899f4",
   "metadata": {
    "id": "9017b027-48e2-49c9-9947-efc7175899f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = [\n",
    "    \"Hello, how are you?\", \n",
    "    \"I'm fine thanks, how about you?\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "krHwj4g2Zo1M",
   "metadata": {
    "id": "krHwj4g2Zo1M"
   },
   "source": [
    "Not let's create an input string from the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bJ5YikpZpKD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "executionInfo": {
     "elapsed": 1538,
     "status": "ok",
     "timestamp": 1684221262540,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "5bJ5YikpZpKD",
    "outputId": "2e90930c-e83b-4295-e27c-e717467c7fec",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|endoftext|>Hello, how are you?<|endoftext|>I'm fine thanks, how about you?<|endoftext|>\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = tokenizer.eos_token \n",
    "if len(context) > 0:\n",
    "    input_string = tokenizer.eos_token + tokenizer.eos_token.join(context) + input_string\n",
    "\n",
    "input_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c8eae1-5c9a-4c31-8a18-119e5fb35f41",
   "metadata": {
    "id": "66c8eae1-5c9a-4c31-8a18-119e5fb35f41"
   },
   "source": [
    "Encode input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61b00b79-1ac5-4eff-8b9f-92c20afd5601",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1684221517239,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "61b00b79-1ac5-4eff-8b9f-92c20afd5601",
    "outputId": "d6f6e207-0009-4d1d-9b9c-8e40df9dbce2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 15496,    11,   703,   389,   345,    30, 50256,    40,  1101,\n",
      "          3734,  5176,    11,   703,   546,   345,    30, 50256]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 18])\n"
     ]
    }
   ],
   "source": [
    "input_encoding = tokenizer(input_string, return_tensors='pt').to(device)\n",
    "print(input_encoding.input_ids)\n",
    "print(input_encoding.input_ids.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4b824b7-f7f9-4c43-ab88-54edb088faff",
   "metadata": {
    "id": "a4b824b7-f7f9-4c43-ab88-54edb088faff"
   },
   "source": [
    "If we run the sequence through the model, we get a series of logits as output.\n",
    "Since we are using an autogressive models, in the last position we will have the logits of next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1e39f77-7c7c-4760-bba3-6a27399813ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1233,
     "status": "ok",
     "timestamp": 1684221533383,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "a1e39f77-7c7c-4760-bba3-6a27399813ab",
    "outputId": "946a891b-d196-4bbb-cf42-2bacfe77df19",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -1.0938, -18.4688, -17.6875,  ..., -14.8672, -13.5625,   0.9443],\n",
      "         [ -6.1445, -16.7656, -15.6094,  ..., -10.8203, -10.4062,   1.5098],\n",
      "         [ -3.1699, -12.5938, -11.6328,  ...,  -7.0820,  -7.3398,   2.1406],\n",
      "         ...,\n",
      "         [  1.9648, -10.8828,  -8.4453,  ...,  -3.9512,  -2.6504,  15.2500],\n",
      "         [  1.0215, -10.5859,  -7.7070,  ...,  -4.1914,  -1.7578,  19.2656],\n",
      "         [  3.8809, -10.0625,  -9.3047,  ...,  -5.8164,  -5.3867,   3.1348]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 18, 50257])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**input_encoding)\n",
    "print(outputs.logits)\n",
    "print(outputs.logits.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36be972b-5239-453f-ba21-5848608d1b11",
   "metadata": {
    "id": "36be972b-5239-453f-ba21-5848608d1b11"
   },
   "source": [
    "We can run these logits through a $\\mathrm{softmax}(\\cdot)$ and obtain the probability distribution over tokens:\n",
    "- for each possible token we have the probability of it being the next in the sequence\n",
    "- We can sample a token from this probability distribution and recurr itin input to get a new token\n",
    "- We can iterate this process to compose a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dadf5181-49e9-4a77-9aa6-e5396f9f130d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1684221627332,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "dadf5181-49e9-4a77-9aa6-e5396f9f130d",
    "outputId": "45c9a2b5-3597-4c33-f6de-4226810c7ccd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5570e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.2159e-05]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(1., device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)\n",
      "torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "p_dist_next = torch.softmax(outputs.logits[:, -1], dim=1)\n",
    "print(p_dist_next)\n",
    "print(p_dist_next.sum())\n",
    "print(p_dist_next.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "XZTr4ZoYbuZQ",
   "metadata": {
    "id": "XZTr4ZoYbuZQ"
   },
   "source": [
    "What is the most probable next token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "MLx4yISpbu0P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 582,
     "status": "ok",
     "timestamp": 1684221861114,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "MLx4yISpbu0P",
    "outputId": "1bfaf646-5e52-45e9-e4e6-c952a0ffd51e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_max_idx = torch.argmax(p_dist_next)\n",
    "print(arg_max_idx)\n",
    "tokenizer.decode(arg_max_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0wAujEcgb5ab",
   "metadata": {
    "id": "0wAujEcgb5ab"
   },
   "source": [
    "Note that we don't need to run the $\\mathrm{softmax}(\\cdot)$ operation if we just want to find the token with highest probability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6599151a-ae4e-4442-90be-bae9d3d36342",
   "metadata": {
    "id": "6599151a-ae4e-4442-90be-bae9d3d36342"
   },
   "source": [
    "#### Deterministic decoding\n",
    "\n",
    "Deterministic appraoches yield always the same output for a given input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01ac99e9-8968-4e17-84d2-629609a3ead4",
   "metadata": {
    "id": "01ac99e9-8968-4e17-84d2-629609a3ead4"
   },
   "source": [
    "##### Greedy decoding\n",
    "\n",
    "The most starightforward way is to pick each time the most probable token and recurr it as next step in input.\n",
    "Very suboptimal solution, usually yields dull responses like `\"I don't know\"` or causes degenerate generation (e.g., repeating the same token many times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7406082-e460-44af-beca-c256023c1e09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1684222694094,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "b7406082-e460-44af-beca-c256023c1e09",
    "outputId": "a631a33d-ae1d-4742-b88a-1857d64c84ad",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm good, thanks.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a59866a-0764-4b86-a055-099fc99a1f83",
   "metadata": {
    "id": "8a59866a-0764-4b86-a055-099fc99a1f83"
   },
   "source": [
    "##### Beam search\n",
    "\n",
    "We cannot do an exhaustuve search, but we can keep the top $n$ most probable sequences up to now.\n",
    "This is what beam search does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcbf9b6e-7e5c-4e5b-9dd1-597512bed9af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1684223360428,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "dcbf9b6e-7e5c-4e5b-9dd1-597512bed9af",
    "outputId": "8490aeaa-39dd-4f33-b4a8-b686a185284b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm good, how about you?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, num_beams=8, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "233a3836-857e-4e43-ae25-40fc042ae980",
   "metadata": {
    "id": "233a3836-857e-4e43-ae25-40fc042ae980"
   },
   "source": [
    "#### Sampling\n",
    "\n",
    "Sampling based decoding adds more spice to the output sampling the next token with a certain probability given by the language model.\n",
    "The nice thing is that given the same input the generated content may change (higher diversity in the text of responses), the bad thing is that given the same input the generated content may change (possibly inconsistent behaviour)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3b9ebd2-8734-49fa-bfa9-7c5066a3acfd",
   "metadata": {
    "id": "f3b9ebd2-8734-49fa-bfa9-7c5066a3acfd",
    "tags": []
   },
   "source": [
    "##### Top-k\n",
    "\n",
    "Consider only first $k$ most probable tokens and zero out others probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "YhXiOu96iGrS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1684223385533,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "YhXiOu96iGrS",
    "outputId": "d50655f7-14cf-4d09-9b24-2b7e1bec6496",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not so well...\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, top_k=16, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7a4e683-53d5-429e-b0a4-36f532327e4b",
   "metadata": {
    "id": "f7a4e683-53d5-429e-b0a4-36f532327e4b"
   },
   "source": [
    "##### Top-p (nucleus sampling)\n",
    "\n",
    "Consider only first most probable tokens so that their probability sum up to $p \\in [0, 1] \\subseteq \\mathbb{R}$ and zero out others probabilities \n",
    "Similar to top-$k$ but variable window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0o1qdXnuiYYM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 1135,
     "status": "ok",
     "timestamp": 1684223435749,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "0o1qdXnuiYYM",
    "outputId": "74634fa8-1195-481f-8838-641d1a381a75",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Quite boring. It's got a tickle herons and a jazz\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, top_p=0.95, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ec864c9-a34f-4083-9938-47754700d4e9",
   "metadata": {
    "id": "0768d0db-61f4-4095-a6f8-169c5ebf54b0"
   },
   "source": [
    "##### Temperature rescoring\n",
    "\n",
    "Divide the logits by a value $\\tau \\in \\mathbb{R}^+_0$:\n",
    "- if $\\tau > 1$ (high temprature) the distribution get softer (reduces probability of most probable tokens and increases that of least probable)\n",
    "- if $\\tau = 1$ the distribution is unchanged\n",
    "- if $\\tau < 1$ (low temperature) the distribution get sharper (reduces probability of least probable tokens and increases that of most probable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a50d6b87-6852-46c3-91b5-f6cf25bd3ca7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 1210,
     "status": "ok",
     "timestamp": 1684223610679,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "a50d6b87-6852-46c3-91b5-f6cf25bd3ca7",
    "outputId": "474b6c48-d8ab-4992-9ec2-bfe7ba1efc9e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me too thanks'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, temperature=0.8, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "# output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, temperature=1.2, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51f6ebb9-3a97-4c16-9f79-5578fd186fc7",
   "metadata": {
    "id": "51f6ebb9-3a97-4c16-9f79-5578fd186fc7"
   },
   "source": [
    "##### Sample multiple candidates\n",
    "\n",
    "You can also sample muliple candidates an pick one according to some criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "NmgGJhnqjQye",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1651,
     "status": "ok",
     "timestamp": 1684223754559,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "NmgGJhnqjQye",
    "outputId": "25daff45-6df1-4331-e3a4-bb24e84a7335",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pretty good how about you?',\n",
       " 'Not too good. But thanks anyways',\n",
       " \"I don't know what to say to that, my man.\",\n",
       " \"It's fine, I'm chilling.\",\n",
       " 'Hey man, how are you?',\n",
       " 'Good thanks',\n",
       " 'Just fine with some tendons.',\n",
       " \"I'm also fine thanks\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, num_return_sequences=8, top_p=0.9, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(output_ids[:, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "j0O6N-ugj6er",
   "metadata": {
    "id": "j0O6N-ugj6er"
   },
   "source": [
    "For example if you combine sampling and beam search the `generate()` method will output automatically the most probable of the samples sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6o87VHHYkIg-",
   "metadata": {
    "id": "6o87VHHYkIg-",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Good, can't ask you anything that is too personal for you.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, temperature=0.8, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "BwYbVIRflJzv",
   "metadata": {
    "id": "BwYbVIRflJzv"
   },
   "source": [
    "#### Chatting\n",
    "\n",
    "Now pick your favourite appraoch and chat with DialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J7oVpe87lRPN",
   "metadata": {
    "id": "J7oVpe87lRPN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Maximum dialogue length (in turn pairs)\n",
    "max_len = 5\n",
    "# Initialise dialogue history\n",
    "dialogue_history = []\n",
    "\n",
    "for i in range(max_len):\n",
    "    # Read user message\n",
    "    user_message = input(\"User: \")\n",
    "    # Append message to dialogue history\n",
    "    dialogue_history.append(user_message)\n",
    "    # Convert dialogue to string\n",
    "    input_string = tokenizer.eos_token \n",
    "    if len(context) > 0:\n",
    "        input_string = tokenizer.eos_token + tokenizer.eos_token.join(dialogue_history) + input_string\n",
    "    # Encode input\n",
    "    input_encoding = tokenizer(input_string, return_tensors='pt').to(device)\n",
    "    # Generate DialoGPT response\n",
    "    output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, top_p=0.9, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "    chatbot_response = tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)\n",
    "    # Append chatbot response to dialogue history\n",
    "    dialogue_history.append(chatbot_response)\n",
    "    # Print chatbot response\n",
    "    print(f\"DialoGPT: {chatbot_response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "qZKpwC1hlWNh",
   "metadata": {
    "id": "qZKpwC1hlWNh"
   },
   "source": [
    "Notes\n",
    "1. As you go forward the model will start gettin slower. There is a way to cache the hidden outputs to avoid re-computing attention on past \n",
    "2. The context of the model is limited, at some point you should start dropping older utterances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e7fe2bd-3208-401e-a2f2-875dbc66910f",
   "metadata": {
    "id": "9e7fe2bd-3208-401e-a2f2-875dbc66910f"
   },
   "source": [
    "### Fine-tuning\n",
    "\n",
    "Now you are ready, you can finally fine-tune a generative chatbot and chat with it instead of studying for the exams! (I am not responsable for your choices, I am just offering you an alterantive)\n",
    "\n",
    "We will fine-tune a vanilla GPT-2 (pick your favourite version).\n",
    "Let's load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70472be8-8ef5-486b-a71d-8b95d977963f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_id = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7883fed-35f6-4220-90bb-c1699ea709e5",
   "metadata": {},
   "source": [
    "Let's set the padding token to be the `eos_token` to simplify some passages later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af333157-15fe-44ce-aac8-7c96cb7fc9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c6f452a-378d-4d1f-9059-6de7c163a85e",
   "metadata": {
    "id": "5c6f452a-378d-4d1f-9059-6de7c163a85e"
   },
   "source": [
    "#### Data preparation\n",
    "\n",
    "We are going to use the [Persona-Chat](https://arxiv.org/abs/1801.07243) corpus (It was used in the ConvAI 2 challenge).\n",
    "It's a data set where conversations are grounded in the persona description of the two participants.\n",
    "\n",
    "We are going to use the [ParlAI](https://parl.ai/docs/index.html) package to get the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83703547-032e-404d-9faa-f2310f1fcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install parlai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "285f6aed-edad-4d1b-88b9-83b7aad0e661",
   "metadata": {},
   "source": [
    "Now let's download the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bccf187-0798-4851-8c8b-2ff3530a8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.tasks.convai2.build import build\n",
    "\n",
    "build({'datapath': './data/'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ded26e1b-864f-4b26-a63b-d95615ea7ade",
   "metadata": {},
   "source": [
    "Now the samples are stored in `.txt` files we need to parse.\n",
    "We can build a simple function that given the path to one of the files parse the content into Python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99bfba5a-5f76-4ddd-becb-45d840c493ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pc(path):\n",
    "    # Open file\n",
    "    with open(path) as f:\n",
    "        # Read raw file lines\n",
    "        data = [line.strip() for line in f]\n",
    "    # Data set container\n",
    "    persona_chat = list()\n",
    "    # Now we iterate through lines and build the data set\n",
    "    for line in data:\n",
    "        # Split line data from initial index\n",
    "        line_idx, line_data = line.split(' ', 1)\n",
    "        # Check if new conversation is started\n",
    "        if line_idx == '1':\n",
    "            # Add new empthy dialogue in data set\n",
    "            persona_chat.append(\n",
    "                {'persona_a': list(), 'persona_b': list(), 'utterances': list()}\n",
    "            )\n",
    "        # If the line is from Speaker A persona\n",
    "        if line_data.startswith('your persona: '):\n",
    "            # Append it to Persona A\n",
    "            persona_chat[-1]['persona_a'].append(line_data[len('your persona: '):])\n",
    "        # Else if the line is from Speaker B persona\n",
    "        elif line_data.startswith(\"partner's persona\"):\n",
    "            # Append it to Persona B\n",
    "            persona_chat[-1]['persona_b'].append(line_data[len(\"partner's persona: \"):])\n",
    "        # Else the line is a regular dialogue line\n",
    "        else:\n",
    "            # Split utterances from distractors and separate A and B\n",
    "            utt_a, utt_b = line_data.split('\\t\\t')[0].split('\\t')\n",
    "            # Append to dialogue utterances\n",
    "            persona_chat[-1]['utterances'].append(\n",
    "                {'speaker': 'A', 'text': utt_a}\n",
    "            )\n",
    "            persona_chat[-1]['utterances'].append(\n",
    "                {'speaker': 'B', 'text': utt_b}\n",
    "            )\n",
    "        \n",
    "    return persona_chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "398b99f5-7756-4a36-a69f-bf5f4048dbae",
   "metadata": {},
   "source": [
    "Let's load trainign and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9727ae76-9ffe-4cce-addb-44caec48cc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona_a': ['i like to remodel homes.',\n",
       "  'i like to go hunting.',\n",
       "  'i like to shoot a bow.',\n",
       "  'my favorite holiday is halloween.'],\n",
       " 'persona_b': ['i like canning and whittling.',\n",
       "  'to stay in shape , i chase cheetahs at the zoo.',\n",
       "  'in high school , i came in 6th in the 100 meter dash.',\n",
       "  'i eat exclusively meat.'],\n",
       " 'utterances': [{'speaker': 'A',\n",
       "   'text': \"hi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape .\"},\n",
       "  {'speaker': 'B',\n",
       "   'text': 'you must be very fast . hunting is one of my favorite hobbies .'},\n",
       "  {'speaker': 'A',\n",
       "   'text': 'i am ! for my hobby i like to do canning or some whittling .'},\n",
       "  {'speaker': 'B',\n",
       "   'text': 'i also remodel homes when i am not out bow hunting .'},\n",
       "  {'speaker': 'A',\n",
       "   'text': \"that's neat . when i was in high school i placed 6th in 100m dash !\"},\n",
       "  {'speaker': 'B',\n",
       "   'text': \"that's awesome . do you have a favorite season or time of year ?\"},\n",
       "  {'speaker': 'A',\n",
       "   'text': 'i do not . but i do have a favorite meat since that is all i eat exclusively .'},\n",
       "  {'speaker': 'B', 'text': 'what is your favorite meat to eat ?'},\n",
       "  {'speaker': 'A',\n",
       "   'text': 'i would have to say its prime rib . do you have any favorite foods ?'},\n",
       "  {'speaker': 'B', 'text': 'i like chicken or macaroni and cheese .'},\n",
       "  {'speaker': 'A',\n",
       "   'text': 'do you have anything planned for today ? i think i am going to do some canning .'},\n",
       "  {'speaker': 'B',\n",
       "   'text': 'i am going to watch football . what are you canning ?'},\n",
       "  {'speaker': 'A',\n",
       "   'text': 'i think i will can some jam . do you also play footfall for fun ?'},\n",
       "  {'speaker': 'B',\n",
       "   'text': 'if i have time outside of hunting and remodeling homes . which is not much !'}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = parse_pc('./data/ConvAI2/train_both_original.txt')\n",
    "validation_data = parse_pc('./data/ConvAI2/valid_both_original.txt')\n",
    "\n",
    "training_data[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5832ea5a-868d-4067-bc6e-d4c838175301",
   "metadata": {},
   "source": [
    "Now we are going to convert to strings all the samples using the `eos_token` as separator.\n",
    "We will include also personae.\n",
    "\n",
    "Let's define another function to do that on a single dialogue, then we will apply it to the entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c534ee88-fda2-4f9a-ba4f-d1cfd11c726f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_to_string(sample, eos_token):\n",
    "    # Join strings of Persona A\n",
    "    persona_a = ' '.join(sample['persona_a'])\n",
    "    # Join strings of Persona B\n",
    "    persona_b = ' '.join(sample['persona_b'])\n",
    "    # Join dialogue strings\n",
    "    dialogue = eos_token.join(f\"{utterance['speaker']}: {utterance['text']}\" for utterance in sample['utterances'])\n",
    "    # Build the dialogue string\n",
    "    dialogue_string = f\"Persona A: {persona_a}{eos_token}Persona B: {persona_b}{eos_token}{dialogue}{eos_token}\"\n",
    "    \n",
    "    return dialogue_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "343d207d-58d2-449b-829b-0395df6f3ba0",
   "metadata": {},
   "source": [
    "Apply the funtion to all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efd2412d-51d2-4494-86d8-a520dd85c1fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Persona A: i like to remodel homes. i like to go hunting. i like to shoot a bow. my favorite holiday is halloween.<|endoftext|>Persona B: i like canning and whittling. to stay in shape , i chase cheetahs at the zoo. in high school , i came in 6th in the 100 meter dash. i eat exclusively meat.<|endoftext|>A: hi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape .<|endoftext|>B: you must be very fast . hunting is one of my favorite hobbies .<|endoftext|>A: i am ! for my hobby i like to do canning or some whittling .<|endoftext|>B: i also remodel homes when i am not out bow hunting .<|endoftext|>A: that's neat . when i was in high school i placed 6th in 100m dash !<|endoftext|>B: that's awesome . do you have a favorite season or time of year ?<|endoftext|>A: i do not . but i do have a favorite meat since that is all i eat exclusively .<|endoftext|>B: what is your favorite meat to eat ?<|endoftext|>A: i would have to say its prime rib . do you have any favorite foods ?<|endoftext|>B: i like chicken or macaroni and cheese .<|endoftext|>A: do you have anything planned for today ? i think i am going to do some canning .<|endoftext|>B: i am going to watch football . what are you canning ?<|endoftext|>A: i think i will can some jam . do you also play footfall for fun ?<|endoftext|>B: if i have time outside of hunting and remodeling homes . which is not much !<|endoftext|>\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_str = [sample_to_string(dialogue, tokenizer.eos_token) for dialogue in training_data]\n",
    "validation_data_str = [sample_to_string(dialogue, tokenizer.eos_token) for dialogue in validation_data]\n",
    "\n",
    "training_data_str[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c212aafb-33fa-4400-85e2-382ffbabfe69",
   "metadata": {},
   "source": [
    "And now are samples are ready"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e1fb3ec-83a3-4cf9-9aaf-1791e0a50e54",
   "metadata": {
    "id": "3e1fb3ec-83a3-4cf9-9aaf-1791e0a50e54"
   },
   "source": [
    "#### Training\n",
    "\n",
    "Ok we made it to fine-tuning a language model.\n",
    "We can use HuggingFace trainer to do the training.\n",
    "\n",
    "First we need to build trainer compatible Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9315c5ea-cccc-4ecf-acd4-01096118282f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data = Dataset.from_dict({'text': training_data_str})\n",
    "valid_data = Dataset.from_dict({'text': validation_data_str})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13d57afa-f9fc-4219-b339-e7a7e1c00f20",
   "metadata": {},
   "source": [
    "... and trainer compatible DatasetDict object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7803cf6-8b2b-49da-a088-343e4bbf34ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['validation'] = valid_data\n",
    "data['test'] = valid_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d18ff25-2deb-487c-bac4-d13cbfa5fd24",
   "metadata": {},
   "source": [
    "Finally use the tokenizer to convert the input strings into sequences of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0b878-f29d-423c-aa25-2f5401ce9a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cba4743-3eda-4d59-a712-bffdbce39b61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007527828216552734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 32,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 18,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010c49eb0b4947beb69418512240fc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005942583084106445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 32,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e66cbe2a8bf460090d30746a3162a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005991697311401367,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 32,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5905934cf5ad4072afde730a3e3e5543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36e8d62c-e923-40e2-9bb4-2d1d99ba47b9",
   "metadata": {},
   "source": [
    "The last step we are missing is to create a collator that gets together all the sequences in the same batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a49b65f8-ce68-44d4-9c3f-990aa2a26bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "557a6a6e-f8aa-497c-80a8-a3f4f093cf56",
   "metadata": {},
   "source": [
    "Now we can create an instance of the trainer specifying the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d7fa0fb-424e-409f-97fa-723aac4a6cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"cooler_trainer_name\", \n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=6.25e-5,\n",
    "    lr_scheduler_type=\"linear\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a3f7a2f-36af-4e09-bccb-b6eec2d406e5",
   "metadata": {},
   "source": [
    "Create the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a097398f-6c22-47af-9711-3ddfd95cd123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=tokenized_data['train'], \n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8c2e37-bbca-4afa-9c66-5a1a400bbe6d",
   "metadata": {},
   "source": [
    "And now let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "935c620f-9751-4f1f-a78f-33b502f5103a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "/home/arcslab/anaconda3/envs/rp_3_1/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 17878\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1674\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1674' max='1674' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1674/1674 1:07:12, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.390300</td>\n",
       "      <td>2.482409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.088700</td>\n",
       "      <td>2.541473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.991300</td>\n",
       "      <td>2.557334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cooler_trainer_name/checkpoint-500\n",
      "Configuration saved in cooler_trainer_name/checkpoint-500/config.json\n",
      "Model weights saved in cooler_trainer_name/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cooler_trainer_name/checkpoint-1000\n",
      "Configuration saved in cooler_trainer_name/checkpoint-1000/config.json\n",
      "Model weights saved in cooler_trainer_name/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cooler_trainer_name/checkpoint-1500\n",
      "Configuration saved in cooler_trainer_name/checkpoint-1500/config.json\n",
      "Model weights saved in cooler_trainer_name/checkpoint-1500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1674, training_loss=2.1370191858945637, metrics={'train_runtime': 4035.429, 'train_samples_per_second': 13.291, 'train_steps_per_second': 0.415, 'total_flos': 1.9251608041728e+16, 'train_loss': 2.1370191858945637, 'epoch': 3.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cdfcbb0-90c6-4608-8e4b-1a699ab0845e",
   "metadata": {},
   "source": [
    "Finally let's save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0951341f-1524-48f5-8f5c-068254c1bd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in persona_chat_fine_tuning_2023_05_22_17_18_33/tokenizer_config.json\n",
      "Special tokens file saved in persona_chat_fine_tuning_2023_05_22_17_18_33/special_tokens_map.json\n",
      "Configuration saved in persona_chat_fine_tuning_2023_05_22_17_18_33/config.json\n",
      "Model weights saved in persona_chat_fine_tuning_2023_05_22_17_18_33/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at: 'persona_chat_fine_tuning_2023_05_22_17_18_33'\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = f\"persona_chat_fine_tuning_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "tokenizer.save_pretrained(checkpoint_path)\n",
    "model.save_pretrained(checkpoint_path)\n",
    "print(f\"Checkpoint saved at: \\'{checkpoint_path}\\'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5dfc6b7-46a9-4e2f-bf22-62de72cdb4a0",
   "metadata": {
    "id": "a5dfc6b7-46a9-4e2f-bf22-62de72cdb4a0"
   },
   "source": [
    "#### Testing\n",
    "\n",
    "We can compute some automatc metrics to assess the quality of the chatbot.\n",
    "We can use the ParlAI utilities to compute the metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9415b82-6942-4fc6-bfa0-4e8938d063a9",
   "metadata": {},
   "source": [
    "Let's pick a random test dialogue and let's generate response.\n",
    "We will compare the original target response to the generated one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2531a04a-a9ce-40c3-a35e-ae1727ab655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "739\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1995)\n",
    "\n",
    "idx = random.choice(range(len(validation_data)))\n",
    "print(idx)\n",
    "dialogue = validation_data[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "389e7d8d-cb62-4eaa-994b-6655836abd65",
   "metadata": {},
   "source": [
    "Now we can pick a turn from the middle of a dialogue to be our target response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcdcce83-a332-43a0-a42d-cd1617d79c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A: oh did you enjoy it ?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_idx = len(dialogue['utterances']) // 2\n",
    "\n",
    "original_response = dialogue['utterances'][response_idx]\n",
    "original_response_string = f\"{original_response['speaker']}: {original_response['text']}\"\n",
    "original_response_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1046041-ee6a-42a0-8d43-c19adc0fd1d3",
   "metadata": {},
   "source": [
    "And we can drop the response and all the following ones to build ourr context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "425e8a1e-d2fc-4321-bb5d-12471b3f73a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Persona A: i'm interested in photography and like taking pictures. my boyfriend and i are moving into an apartment together next week. i am an elementary school teacher. i am fluent in english spanish and french.<|endoftext|>Persona B: i enjoy poetry. i am a huge star wars fan. i try various coffees as a hobby. i played football for a division a college.<|endoftext|>A: hey , jefferson here , i love poetry .<|endoftext|>B: hey mike here my boyfriend and i are moving in together next week<|endoftext|>A: wow , does he like star wars ?<|endoftext|>B: yeah i am interested in taking pictures , of yoda for sure<|endoftext|>A: wow . i try coffee as a hobby<|endoftext|>B: yeah i am fluent in english spanish and french<|endoftext|>A: wow , all i did was play football in college .<|endoftext|>B: cool , i taught a few players being an elementary school teacher<|endoftext|>\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = {\n",
    "    'persona_a': dialogue['persona_a'],\n",
    "    'persona_b': dialogue['persona_b'],\n",
    "    'utterances': dialogue['utterances'][:response_idx]\n",
    "}\n",
    "context_string = sample_to_string(context, tokenizer.eos_token)\n",
    "context_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6118d7d-21a7-4df5-9867-afb76c4dfb69",
   "metadata": {},
   "source": [
    "And let's load back the model from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1efaec23-1bd6-46ab-a6c3-7583a609dd47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file persona_chat_fine_tuning_2023_05_22_17_18_33/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"persona_chat_fine_tuning_2023_05_22_17_18_33\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file persona_chat_fine_tuning_2023_05_22_17_18_33/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at persona_chat_fine_tuning_2023_05_22_17_18_33.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_path = 'persona_chat_fine_tuning_2023_05_22_17_18_33'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path , device_map=\"auto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7bc98ad-cbbd-4b8c-8182-50045e33ef59",
   "metadata": {},
   "source": [
    "Generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ba42ba8-bb7f-4812-814c-0b8dcd266d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A: oh, wow. i'm also a huge fan of the game of thrones. i love that show. i enjoy it. i like it too. coffee. you? own a truck? i'm on it. i'm a huge fan too. drink. it pays the bills. you like coffee too\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode context\n",
    "input_encoding = tokenizer(context_string, return_tensors='pt').to(device)\n",
    "# Generate response\n",
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=64, do_sample=True, temperature=0.7, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "# Decode generated response\n",
    "generated_response = tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)\n",
    "generated_response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ifOnlKvRl3e9",
   "metadata": {
    "id": "ifOnlKvRl3e9"
   },
   "source": [
    "##### Perplexity\n",
    "\n",
    "Perplexity can be computed using the cross entropy on the generated response.\n",
    "First let's process the the context and the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0rjR2Q9-l3zQ",
   "metadata": {
    "id": "0rjR2Q9-l3zQ"
   },
   "outputs": [],
   "source": [
    "# Encode dialogue\n",
    "input_encoding = tokenizer(context_string + original_response_string + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "# Compute model outputs\n",
    "outputs = model(**input_encoding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6a7f630-c5e5-4279-a963-67cb7044b510",
   "metadata": {},
   "source": [
    "We get the target labels (the ids of the response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91787b2b-5889-4d2c-ad3a-45f64baa0133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tokenizer(original_response_string + tokenizer.eos_token, return_tensors='pt').input_ids.to(device)\n",
    "labels.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e3f6f81-3318-42d5-bd73-562f1e1d4014",
   "metadata": {},
   "source": [
    "And then we retain only the logits from the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f91c772-a045-4fe0-ade9-3442387c0bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 50257])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits[:, -labels.size(1):]\n",
    "logits.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff72d697-7d5d-488f-9955-0dc8ad3bcba7",
   "metadata": {},
   "source": [
    "Compute the average cross-entropy shifting the inputs and the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65f3f8ab-082e-4ae0-900c-3f55accff807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6102, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Shift logits to exclude the last element\n",
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "# shift labels to exclude the first element\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "# Compute loss\n",
    "lm_loss = F.cross_entropy(\n",
    "    shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    ")\n",
    "lm_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a6d30b-7e04-4fc8-99ec-c7bc6e5026e3",
   "metadata": {},
   "source": [
    "Exponentiate to have PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57cf90c3-2cfd-4e3d-a210-4637e8cd7205",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.9717, device='cuda:0', grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl = torch.exp(lm_loss)\n",
    "ppl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ef11911-30a6-494d-848d-7c44addb75fe",
   "metadata": {},
   "source": [
    "The process can be simplified but at least now you have seen all the steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "T9by6tNYl548",
   "metadata": {
    "id": "T9by6tNYl548"
   },
   "source": [
    "BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "993AnB2Fl6Gv",
   "metadata": {
    "id": "993AnB2Fl6Gv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 3.409e-08\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.metrics import BleuMetric\n",
    "\n",
    "bleu = BleuMetric.compute(generated_response, [original_response_string])\n",
    "print(f\"BLEU: {bleu}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d33d5fa-8503-455e-9e50-af0753a6df0f",
   "metadata": {},
   "source": [
    "##### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36be237c-13f4-4f40-bb28-3c719d4d85e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.1667\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.metrics import F1Metric\n",
    "\n",
    "f1_score = F1Metric.compute(generated_response, [original_response_string])\n",
    "print(f\"F1: {f1_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "hjEBLEHxmFYY",
   "metadata": {
    "id": "hjEBLEHxmFYY"
   },
   "source": [
    "##### Chatting\n",
    "\n",
    "There is no better way to evalaute a generative chatbot than using it to chat.\n",
    "Define a custom persona for you and the chatbot (or sample two from the data set) and write a chatting loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qGArtuODmGBy",
   "metadata": {
    "id": "qGArtuODmGBy"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "dialogue = {\n",
    "    # Write here your persona\n",
    "    'persona_a': [\n",
    "        \"i am vincenzo\",\n",
    "        \"i come from italy\",\n",
    "        \"i like pizza\"\n",
    "    ], \n",
    "    # Write here chatbot persona\n",
    "    'persona_b': [\n",
    "        \"i am an ai chatbot\",\n",
    "        \"i like chatting with other people\",\n",
    "        \"i want to take over human kind\"\n",
    "    ], \n",
    "    # Here we will acumulate the history\n",
    "    'utterances': list()\n",
    "}\n",
    "\n",
    "# Maximum dialogue length (in turn pairs)\n",
    "max_len = 5\n",
    "\n",
    "for i in range(max_len):\n",
    "    # Read user message\n",
    "    user_message = input(\"User: \")\n",
    "    # Append message to dialogue history\n",
    "    dialogue['utterances'].append(\n",
    "        {'speaker': 'A', 'text': user_message.lower()}\n",
    "    )\n",
    "    # Convert dialogue to string\n",
    "    input_string = sample_to_string(dialogue, tokenizer.eos_token)\n",
    "    # Encode input\n",
    "    input_encoding = tokenizer(input_string, return_tensors='pt').to(device)\n",
    "    # Generate DialoGPT response\n",
    "    output_ids = model.generate(input_encoding.input_ids, max_new_tokens=64, do_sample=True, top_p=0.9, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "    chatbot_response = tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)\n",
    "    # Crop initial speaker token\n",
    "    chatbot_response = chatbot_response[3:]\n",
    "    # Append chatbot response to dialogue history\n",
    "    dialogue['utterances'].append(\n",
    "        {'speaker': 'B', 'text': chatbot_response}\n",
    "    )\n",
    "    # Print chatbot response\n",
    "    print(f\"Chatbot: {chatbot_response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c027b0f6-1e3a-4db1-b826-2b1d73a3993d",
   "metadata": {
    "id": "c027b0f6-1e3a-4db1-b826-2b1d73a3993d"
   },
   "source": [
    "## ELIZA meets DialoGPT\n",
    "\n",
    "In the 70s they made ELIZA and PARRY meet each other: https://www.theatlantic.com/technology/archive/2014/06/when-parry-met-eliza-a-ridiculous-chatbot-conversation-from-1972/372428/.\n",
    "We could you have ELIZA meet ChatGPT, but since we are humble we will settle with DialoGPT.\n",
    "\n",
    "The is this implementation of ELIZA in Python we can use: https://github.com/wadetb/eliza\n",
    "Let's start by cloning the repository and adding it to our path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b86ea-6590-4772-ad70-3cd1d78a3758",
   "metadata": {
    "id": "c426eff4-335d-4b33-b5ec-fa5ebd43102a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/wadetb/eliza.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "934b810a-66e6-4bce-aa10-a523d125d360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/arcslab/Documents/vincenzo_scotti_polimi/rp_3_1/notebooks/eliza/eliza.py')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f09c196a-feb5-4540-a96f-1619dcaa2e22",
   "metadata": {
    "id": "f09c196a-feb5-4540-a96f-1619dcaa2e22"
   },
   "source": [
    "Now we should be able to import the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3460802-729f-4fd2-95d0-745801b2e144",
   "metadata": {
    "id": "734f32d3-8fee-4316-b717-eb67bf5e6c15",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from eliza.eliza import Eliza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33559208-537b-436d-b4c3-a103fe951bff",
   "metadata": {
    "id": "33559208-537b-436d-b4c3-a103fe951bff"
   },
   "source": [
    "Now let's create an instance of ELIZA using the available rules (https://github.com/wadetb/eliza/blob/master/doctor.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb2e1c85-7db1-4e52-9f4f-c955176c0488",
   "metadata": {
    "id": "7aad7262-3e1c-42a1-b16c-8b3b492865c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "eliza = Eliza()\n",
    "eliza.load('./eliza/doctor.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f0622c5-9600-419b-83c2-45cefb0a2901",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now you can chat with ELIZA.\n",
    "Note that ELIZA manages its context interanlly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ea019-de3b-4b2b-b2dd-a976f8eb7145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Maximum dialogue length (in turn pairs)\n",
    "max_len = 5\n",
    "\n",
    "for i in range(max_len):\n",
    "    # Read user message\n",
    "    user_message = input(\"User: \")\n",
    "    # Ask ELIZA for response\n",
    "    response = eliza.respond(user_message)\n",
    "    # Print ELIZA response\n",
    "    print(f\"ELIZA: {response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a453651-0b0e-4b4f-bf3d-49873ddc5d35",
   "metadata": {},
   "source": [
    "But that's not as fun as having DialoGPT chat with ELIZA, right?\n",
    "Let's load back DialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d272120-4469-4353-a970-a6df53c5717f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/arcslab/.cache/huggingface/hub/models--microsoft--DialoGPT-medium/snapshots/9d5c5fadcc072b693fb5a5e29416bbf3f503c26c/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"microsoft/DialoGPT-medium\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"conversational\": {\n",
      "      \"max_length\": 1000\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/arcslab/.cache/huggingface/hub/models--microsoft--DialoGPT-medium/snapshots/9d5c5fadcc072b693fb5a5e29416bbf3f503c26c/vocab.json\n",
      "loading file merges.txt from cache at /home/arcslab/.cache/huggingface/hub/models--microsoft--DialoGPT-medium/snapshots/9d5c5fadcc072b693fb5a5e29416bbf3f503c26c/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/arcslab/.cache/huggingface/hub/models--microsoft--DialoGPT-medium/snapshots/9d5c5fadcc072b693fb5a5e29416bbf3f503c26c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/arcslab/.cache/huggingface/hub/models--microsoft--DialoGPT-medium/snapshots/9d5c5fadcc072b693fb5a5e29416bbf3f503c26c/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"microsoft/DialoGPT-medium\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"conversational\": {\n",
      "      \"max_length\": 1000\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/arcslab/.cache/huggingface/hub/models--microsoft--DialoGPT-medium/snapshots/9d5c5fadcc072b693fb5a5e29416bbf3f503c26c/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"microsoft/DialoGPT-medium\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"conversational\": {\n",
      "      \"max_length\": 1000\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/arcslab/.cache/huggingface/hub/models--microsoft--DialoGPT-medium/snapshots/9d5c5fadcc072b693fb5a5e29416bbf3f503c26c/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"microsoft/DialoGPT-medium\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"conversational\": {\n",
      "      \"max_length\": 1000\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/arcslab/.cache/huggingface/hub/models--microsoft--DialoGPT-medium/snapshots/9d5c5fadcc072b693fb5a5e29416bbf3f503c26c/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_id = 'microsoft/DialoGPT-medium'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id , device_map=\"auto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff2cea5b-faa4-414d-8868-8263e0e41200",
   "metadata": {},
   "source": [
    "Now we can re-use the same chatting script from before, but instead of asking input to the user, we are going to ask ELIZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77561898-ae6f-459c-bc75-5bc8cae3726d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Hello\n",
      "ELIZA: How do you do. Please state your problem.\n",
      "DialoGPT: i dont know what happened... i am trying to get the message that the app is closed\n",
      "ELIZA: Is it because you are trying to get the message that the app is closed that you came to me ?\n",
      "DialoGPT: the app closed\n",
      "ELIZA: Do you feel strongly about discussing such things ?\n",
      "DialoGPT: no, i am just expressing my opinions on the matter.\n",
      "ELIZA: Why do you say your opinions on the matter ?\n",
      "DialoGPT: and why does it annoy you\n",
      "ELIZA: Does that question interest you ?\n",
      "DialoGPT: It does\n"
     ]
    }
   ],
   "source": [
    "# Reset ELIZA\n",
    "eliza = Eliza()\n",
    "eliza.load('./eliza/doctor.txt')\n",
    "# Maximum dialogue length (in turn pairs)\n",
    "max_len = 5\n",
    "# Initialise dialogue history\n",
    "dialogue_history = [\"Hello\"]\n",
    "# Print first message\n",
    "print(f\"DialoGPT: {dialogue_history[0]}\")\n",
    "\n",
    "\n",
    "for i in range(max_len):\n",
    "    # Generate ELIZA response\n",
    "    eliza_message = eliza.respond(dialogue_history[-1])\n",
    "    # Append message to dialogue history\n",
    "    dialogue_history.append(eliza_message)\n",
    "    # Print ELIZA response\n",
    "    print(f\"ELIZA: {eliza_message}\")\n",
    "    # Convert dialogue to string\n",
    "    input_string = tokenizer.eos_token \n",
    "    if len(context) > 0:\n",
    "        input_string = tokenizer.eos_token + tokenizer.eos_token.join(dialogue_history) + input_string\n",
    "    # Encode input\n",
    "    input_encoding = tokenizer(input_string, return_tensors='pt').to(device)\n",
    "    # Generate DialoGPT response\n",
    "    output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, temperature=0.7, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "    dialogpt_message = tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)\n",
    "    # Append DialoGPT response to dialogue history\n",
    "    dialogue_history.append(dialogpt_message)\n",
    "    # Print DialoGPT response\n",
    "    print(f\"DialoGPT: {dialogpt_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0053dff-63f9-40d7-8be7-bd9ee29366b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0022b9d7c82b478e8c17bc56edd3cab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34730f6f972c47a18809b1bacd550364",
      "max": 642,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4cfd24194cde45d2967b17808dea993e",
      "value": 642
     }
    },
    "0e70451ca4c3478ea8bd32b628f5ea8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c62e94fde69f46afa505eb24ba03be3d",
      "placeholder": "​",
      "style": "IPY_MODEL_6ce125927e8245e990d5606a516b472f",
      "value": "Downloading (…)olve/main/vocab.json: 100%"
     }
    },
    "0f3643c0b3d644b2889e27d51b302fbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_664782bff2714ee4b365de17130e1245",
      "placeholder": "​",
      "style": "IPY_MODEL_6d05e6d754b84e49ab70b21b34a276f3",
      "value": "Downloading (…)olve/main/merges.txt: 100%"
     }
    },
    "15d33819bd21408898dd600aff77323c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18a1b79006ca47d8b1e7e74385ace43b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4bfb2488f3284728be62cf92e3782ead",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1c807338e6e74c3195213ddb03868617",
      "value": 124
     }
    },
    "1a3aea788dfc42c884296fbde19757dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f15b974cbc8847be9047faff6a86a630",
      "placeholder": "​",
      "style": "IPY_MODEL_5ca0ea332a754fedaba0df7fd3f8f0f8",
      "value": " 124/124 [00:00&lt;00:00, 6.66kB/s]"
     }
    },
    "1c807338e6e74c3195213ddb03868617": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1cb3e1aedf8a416380ba288e333ce925": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d70dcadb8f74194b088e4dfdea0ae55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2acba93c4b21404ca367f44c13dc1002": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e21e71dd8a3454fbebf7d3ca25df966",
      "placeholder": "​",
      "style": "IPY_MODEL_a60568f613ca4bcd8da9150a54f60eb3",
      "value": " 456k/456k [00:00&lt;00:00, 22.4MB/s]"
     }
    },
    "32629bccb57048c4a28cee33eefdf8de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "328ae82f282c4cccbc324c30a409d536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34730f6f972c47a18809b1bacd550364": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34f84e904398483092b6e64d3e382a6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3511be86d8934ab4ac64ed3d8e02a65f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37784f58d647426698ba34a9e96bbb34",
      "placeholder": "​",
      "style": "IPY_MODEL_c84d5275fe0845a29916d77adf34630c",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 10.4MB/s]"
     }
    },
    "37784f58d647426698ba34a9e96bbb34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "405205e401bf4972b3ed1ff0e11e5af8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48a8325dc4a443e5a9bc467bf3df2f78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ba8155b35924dfcbdce02828e4ed64c",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7c14cc02a6c144bc9f8b491973b719a7",
      "value": 1042301
     }
    },
    "4bfb2488f3284728be62cf92e3782ead": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cfd24194cde45d2967b17808dea993e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5190a6e308084dafb92ca9e8efb26e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "552aa9a8d19947b9bb8ef71769649a22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "58f3cb124dcf45438a749da2678e6a32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a2300a1029c4a4bb28bea94a09c72de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a23e7c48eb34a65bd5b07b6db00a3da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bd9e91e79814b54a7a6545ab57d4e41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32629bccb57048c4a28cee33eefdf8de",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e53b5a609514709afd747a034f53400",
      "value": 26
     }
    },
    "5ca0ea332a754fedaba0df7fd3f8f0f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6043c4db0f2f45209fb146fc8914ece8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_981cf3b90fa642819254afc204a4e6ae",
       "IPY_MODEL_18a1b79006ca47d8b1e7e74385ace43b",
       "IPY_MODEL_1a3aea788dfc42c884296fbde19757dc"
      ],
      "layout": "IPY_MODEL_5a2300a1029c4a4bb28bea94a09c72de"
     }
    },
    "664782bff2714ee4b365de17130e1245": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ba8155b35924dfcbdce02828e4ed64c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ce125927e8245e990d5606a516b472f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d05e6d754b84e49ab70b21b34a276f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "736c682a759242888297a3c1b7504c65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "757e467c74f14465b743219e68a65326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f3643c0b3d644b2889e27d51b302fbe",
       "IPY_MODEL_a1aa44f1301d4295b63e3cbefcbc86f3",
       "IPY_MODEL_2acba93c4b21404ca367f44c13dc1002"
      ],
      "layout": "IPY_MODEL_e316fa98c07241f38057e42c471fdee7"
     }
    },
    "76105038289540a68a560379a6c361e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a696bac8dee40f1b3d8010636b254ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c14cc02a6c144bc9f8b491973b719a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e53b5a609514709afd747a034f53400": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83177b6659ba46ee8a9b8a88dc90f098": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c6650c299b24c37b0c95b6e60c08125",
       "IPY_MODEL_e220b17e036e41f8807230e321baf524",
       "IPY_MODEL_f244f885a9fa4265b20b88dddbe7f37e"
      ],
      "layout": "IPY_MODEL_15d33819bd21408898dd600aff77323c"
     }
    },
    "843c830297594fe7aa7cacd4fea6bfc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8495ad273edf4ebfadc8bc0d377ae8dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c6650c299b24c37b0c95b6e60c08125": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93207531a2d04714ba2c0a955c61a67d",
      "placeholder": "​",
      "style": "IPY_MODEL_552aa9a8d19947b9bb8ef71769649a22",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "93207531a2d04714ba2c0a955c61a67d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94075dee90e2425689176dbe91cef4ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0e70451ca4c3478ea8bd32b628f5ea8a",
       "IPY_MODEL_48a8325dc4a443e5a9bc467bf3df2f78",
       "IPY_MODEL_3511be86d8934ab4ac64ed3d8e02a65f"
      ],
      "layout": "IPY_MODEL_8495ad273edf4ebfadc8bc0d377ae8dd"
     }
    },
    "981cf3b90fa642819254afc204a4e6ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a696bac8dee40f1b3d8010636b254ed",
      "placeholder": "​",
      "style": "IPY_MODEL_5a23e7c48eb34a65bd5b07b6db00a3da",
      "value": "Downloading (…)neration_config.json: 100%"
     }
    },
    "9e21e71dd8a3454fbebf7d3ca25df966": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1aa44f1301d4295b63e3cbefcbc86f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc91ac92549546f2b528acdc5d7cd86f",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5190a6e308084dafb92ca9e8efb26e48",
      "value": 456318
     }
    },
    "a20c80af13fb4037852605f82f714cd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf40ac805ffd421d9ecd1db6894efee0",
       "IPY_MODEL_0022b9d7c82b478e8c17bc56edd3cab5",
       "IPY_MODEL_eacb7cc951434c72bddcd5754a4ae78a"
      ],
      "layout": "IPY_MODEL_843c830297594fe7aa7cacd4fea6bfc3"
     }
    },
    "a60568f613ca4bcd8da9150a54f60eb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "afec3e01ffaf438a92cb11b358694f42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9cd29f61c8b428f97e32e140c0250ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c62e94fde69f46afa505eb24ba03be3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c84d5275fe0845a29916d77adf34630c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf40ac805ffd421d9ecd1db6894efee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_405205e401bf4972b3ed1ff0e11e5af8",
      "placeholder": "​",
      "style": "IPY_MODEL_afec3e01ffaf438a92cb11b358694f42",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "d618a30185c74432aee3907ec8b1225d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f99f6290ca3640a18a9fbe41980f3411",
      "placeholder": "​",
      "style": "IPY_MODEL_f43e68bb14bc49f4a78491a525461689",
      "value": " 26.0/26.0 [00:00&lt;00:00, 1.55kB/s]"
     }
    },
    "dc91ac92549546f2b528acdc5d7cd86f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de1d5b5b97364df28408441b99209c99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e220b17e036e41f8807230e321baf524": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9cd29f61c8b428f97e32e140c0250ef",
      "max": 1752292117,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_34f84e904398483092b6e64d3e382a6f",
      "value": 1752292117
     }
    },
    "e316fa98c07241f38057e42c471fdee7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea6e7a8957754378bb4e4a87f460207a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f012d92822dc46bebd6881fe48d0312c",
       "IPY_MODEL_5bd9e91e79814b54a7a6545ab57d4e41",
       "IPY_MODEL_d618a30185c74432aee3907ec8b1225d"
      ],
      "layout": "IPY_MODEL_1d70dcadb8f74194b088e4dfdea0ae55"
     }
    },
    "eacb7cc951434c72bddcd5754a4ae78a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76105038289540a68a560379a6c361e5",
      "placeholder": "​",
      "style": "IPY_MODEL_de1d5b5b97364df28408441b99209c99",
      "value": " 642/642 [00:00&lt;00:00, 26.3kB/s]"
     }
    },
    "f012d92822dc46bebd6881fe48d0312c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_736c682a759242888297a3c1b7504c65",
      "placeholder": "​",
      "style": "IPY_MODEL_58f3cb124dcf45438a749da2678e6a32",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "f15b974cbc8847be9047faff6a86a630": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f244f885a9fa4265b20b88dddbe7f37e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cb3e1aedf8a416380ba288e333ce925",
      "placeholder": "​",
      "style": "IPY_MODEL_328ae82f282c4cccbc324c30a409d536",
      "value": " 1.75G/1.75G [00:20&lt;00:00, 180MB/s]"
     }
    },
    "f43e68bb14bc49f4a78491a525461689": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f99f6290ca3640a18a9fbe41980f3411": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
