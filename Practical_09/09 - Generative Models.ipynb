{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ee5ea9f-7715-4e08-b572-fa8116eb67f4",
   "metadata": {
    "id": "7ee5ea9f-7715-4e08-b572-fa8116eb67f4"
   },
   "source": [
    "# Generative models\n",
    "\n",
    "This tutorial will show how to use generative models to solve:\n",
    "- Question answering\n",
    "- Dialogue generation\n",
    "However, the same prinicples are applicable to \n",
    "\n",
    "Yay we made it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf357018-5db6-4509-8ae8-dd3784f9023e",
   "metadata": {
    "id": "bf357018-5db6-4509-8ae8-dd3784f9023e"
   },
   "source": [
    "## Generative Question Answering (QA)\n",
    "\n",
    "Up to now we have seen how to retrieve relevant passages that may contain the answer to a question.\n",
    "- What if the answer is not written explicitly in the passage?\n",
    "- What if the passage is too long to read (and our lives are too dynamic to spend more than one minute reading)?\n",
    "\n",
    "We can train a model to output the answer to a question given \n",
    "- a relevant passage (and we know how to gather relevant documents)\n",
    "- the question (yes, the question contains relevant information to answer the question)\n",
    "... Or we can put a pre-trained model on top of our retreival pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "809d9c7d-7c90-4144-b463-db21b2e21248",
   "metadata": {
    "id": "809d9c7d-7c90-4144-b463-db21b2e21248"
   },
   "source": [
    "### QA data preparation\n",
    "\n",
    "In this section we will be using the [WikiQA](https://aclanthology.org/D15-1237/) data set.\n",
    "It's a data set for open domain generative QA.\n",
    "\n",
    "It's avaialble via the HuggingFace data set package, let's install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db5f1d-1eb1-4f52-8c90-1e819bd311d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16426,
     "status": "ok",
     "timestamp": 1684162476749,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "65db5f1d-1eb1-4f52-8c90-1e819bd311d8",
    "outputId": "fd1f9b1c-a432-461a-d72f-699a3afc4e56",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip -q install datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cc87bcb-d94c-4060-8c2f-b13ab75fd3f4",
   "metadata": {
    "id": "6cc87bcb-d94c-4060-8c2f-b13ab75fd3f4"
   },
   "source": [
    "Let's download the validation split of the WikiQA data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582af4f-05fb-4085-aaa8-2d83ea0d9212",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1684162586060,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "b582af4f-05fb-4085-aaa8-2d83ea0d9212",
    "outputId": "c6fea0ae-8df2-432c-86ad-cfd7421f33ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "wiki_qa = datasets.load_dataset('wiki_qa', split='validation')\n",
    "wiki_qa[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "G2h4Bb0T6OfR",
   "metadata": {
    "id": "G2h4Bb0T6OfR"
   },
   "source": [
    "The data set contains questions, title of documents in Wikipedia containing the answers and answers.\n",
    "Additionally the data set contains distractor answers for a given question.\n",
    "We can distinguish the two types of responses from the `label` value. \n",
    "\n",
    "Let's filter the data to retain only correct question-response pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee165bb0-5ed0-4d08-a566-d19af55d471b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1684163052329,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "ee165bb0-5ed0-4d08-a566-d19af55d471b",
    "outputId": "525e4918-37ed-4554-9426-35e0586652e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_qa_correct = wiki_qa.filter(lambda x: x['label'] == 1)\n",
    "wiki_qa_correct[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "JZmYkfWU8Yyj",
   "metadata": {
    "id": "JZmYkfWU8Yyj"
   },
   "source": [
    "Ok now we have a lot of samples to try our system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3a3cd78-2102-4b0d-b1c6-2a4e7f18e635",
   "metadata": {
    "id": "c3a3cd78-2102-4b0d-b1c6-2a4e7f18e635"
   },
   "source": [
    "### Knowlege retreival\n",
    "\n",
    "We have the questions (and the target answers), now we need to prepare our knowledge source and the retrieval system.\n",
    "We can re-use the simple Wikipedia and the two encoder models from last tutorial.\n",
    "\n",
    "Let's start loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0f600-1279-44d7-a4ef-b4ca4197c78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip -q install transformers==4.22.2\n",
    "#!pip -q install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c3a4a-1164-4d66-9b8b-0a03190d0791",
   "metadata": {
    "id": "727c3a4a-1164-4d66-9b8b-0a03190d0791",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import util\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "JRYKGJY39O3X",
   "metadata": {
    "id": "JRYKGJY39O3X"
   },
   "source": [
    "We can try indexing all the paragraphs this time (hopefully it won't explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EdifHBpo9PLX",
   "metadata": {
    "id": "EdifHBpo9PLX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "# NOTE: Change this flag to use only first paragraph\n",
    "only_first = False\n",
    "\n",
    "passages = []\n",
    "# Open the file with the dump of Simple Wikipedia\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as f:\n",
    "    # Iterate over the lines\n",
    "    for line in f:\n",
    "        # Parse the document using JSON\n",
    "        data = json.loads(line.strip())\n",
    "        if only_first:\n",
    "            # Only add the first paragraph\n",
    "            passages.append(data['paragraphs'][0])\n",
    "        else:\n",
    "            # Add all paragraphs\n",
    "            passages.extend(data['paragraphs'])\n",
    "\n",
    "print(f\"Retreived {len(passages)} passages\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "O_stcXFV80eL",
   "metadata": {
    "id": "O_stcXFV80eL"
   },
   "source": [
    "Now we can import the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q2HlwF1P81I3",
   "metadata": {
    "id": "Q2HlwF1P81I3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "semb_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "xenc_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "thO1wWEq9ylz",
   "metadata": {
    "id": "thO1wWEq9ylz"
   },
   "source": [
    "Now let's embed the retreived passages (We can checkpoint the embeddings to avoid repeating the computation each time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8DO9OtdA9xxf",
   "metadata": {
    "id": "8DO9OtdA9xxf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define hnswlib index path\n",
    "embeddings_cache_path = './qa_embeddings_cache.pkl'\n",
    "\n",
    "# Load cache if available\n",
    "if os.path.exists(embeddings_cache_path):\n",
    "    print('Loading embeddings cache')\n",
    "    with open(embeddings_cache_path, 'rb') as f:\n",
    "        corpus_embeddings = pickle.load(f)\n",
    "# Else compute embeddings\n",
    "else:\n",
    "    print('Computing embeddings')\n",
    "    corpus_embeddings = semb_model.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n",
    "    # Save the index to a file for future loading\n",
    "    print(f'Saving index to: \\'{embeddings_cache_path}\\'')\n",
    "    with open(embeddings_cache_path, 'wb') as f:\n",
    "        pickle.dump(corpus_embeddings, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1PmCJqc_x_S",
   "metadata": {
    "id": "d1PmCJqc_x_S"
   },
   "source": [
    "Finally let's index the embeddings (and let's sve the index hoping it works this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YEvmHHaiAF1E",
   "metadata": {
    "id": "YEvmHHaiAF1E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip -q install hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uf3R0pcO_yN2",
   "metadata": {
    "id": "uf3R0pcO_yN2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import hnswlib\n",
    "\n",
    "# Create empthy index\n",
    "index = hnswlib.Index(space='cosine', dim=384)\n",
    "\n",
    "# Define hnswlib index path\n",
    "index_path = './qa_hnswlib.index'\n",
    "\n",
    "# Load index if available\n",
    "if os.path.exists(index_path):\n",
    "    print('Loading index...')\n",
    "    index.load_index(index_path)\n",
    "# Else index data collection\n",
    "else:\n",
    "    # Initialise the index\n",
    "    print('Start creating HNSWLIB index')\n",
    "    index.init_index(max_elements=corpus_embeddings.size(0), ef_construction=400, M=64)\n",
    "    #  Compute the HNSWLIB index (it may take a while)\n",
    "    index.add_items(corpus_embeddings.cpu(), list(range(len(corpus_embeddings))))\n",
    "    # Save the index to a file for future loading\n",
    "    print(f'Saving index to: {index_path}')\n",
    "    index.save_index(index_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "IsuUSgT6AMX1",
   "metadata": {
    "id": "IsuUSgT6AMX1"
   },
   "source": [
    "Now we have almost all the tools to answer a question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "427fc98d-1d8b-412f-a804-574d6eac9201",
   "metadata": {
    "id": "427fc98d-1d8b-412f-a804-574d6eac9201"
   },
   "source": [
    "### Answering a question\n",
    "\n",
    "We are still missing the core of the QA system, the answering model.\n",
    "We are going to re-use a pre-trained model.\n",
    "\n",
    "[FLAN T5](https://arxiv.org/abs/2210.11416) is a pre-trained encode-decoder model trained to be used on different tasks in zero-shot or few-shots leaning settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Va63G93lJfSE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24213,
     "status": "ok",
     "timestamp": 1684218458947,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "Va63G93lJfSE",
    "outputId": "830e64e7-cd67-431d-9982-4621598f6b5b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip -q install transformers sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GssOsdFAGT9N",
   "metadata": {
    "id": "GssOsdFAGT9N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7sKz5f1TJKhr",
   "metadata": {
    "id": "7sKz5f1TJKhr"
   },
   "source": [
    "Let's check quickly that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wId9d-_7JK7m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1575,
     "status": "ok",
     "timestamp": 1684218910698,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "wId9d-_7JK7m",
    "outputId": "aac9fabd-cd23-4485-9c99-3ea78419a4ae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"Translate the following sentence from English to Italian: \\\"Vincenzo is the best\\\"\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=32)\n",
    "output_text = tokenizer.decode(output_ids[0])\n",
    "print(output_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8qKLHXYMGUKd",
   "metadata": {
    "id": "8qKLHXYMGUKd"
   },
   "source": [
    "Now we can test our pipeline. \n",
    "First select randomly a question from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f270d8-1cf4-4b39-8bd9-5312646378c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1684165929394,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "a0f270d8-1cf4-4b39-8bd9-5312646378c1",
    "outputId": "adde407b-9935-47d5-acc5-de00c8828359",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1995)\n",
    "\n",
    "idx = random.choice(range(len(wiki_qa_correct)))\n",
    "\n",
    "sample = wiki_qa_correct[idx]\n",
    "question = sample['question']\n",
    "target_answer = sample['answer']\n",
    "\n",
    "print(f'Question {idx}: {question}?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3waS9PiCSBcg",
   "metadata": {
    "id": "3waS9PiCSBcg"
   },
   "source": [
    "Embed the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QfP51mwmSBwG",
   "metadata": {
    "id": "QfP51mwmSBwG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_embedding = semb_model.encode(question, convert_to_tensor=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "AtmId6HLGB8r",
   "metadata": {
    "id": "AtmId6HLGB8r"
   },
   "source": [
    "Retreive relevant documents keeping top $k$ matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e342af-1b52-46b4-8c44-d83f4c854a80",
   "metadata": {
    "id": "03e342af-1b52-46b4-8c44-d83f4c854a80",
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_ids, distances = index.knn_query(question_embedding.cpu(), k=64)\n",
    "scores = 1 - distances\n",
    "\n",
    "print(\"Cosine similarity model search results\")\n",
    "print(f\"Query: \\\"{question}\\\"\")\n",
    "print(\"---------------------------------------\")\n",
    "for idx, score in zip(corpus_ids[0][:5], scores[0][:5]):\n",
    "    print(f\"Score: {score:.4f}\\nDocument: \\\"{passages[idx]}\\\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aH0PbWSGCxy",
   "metadata": {
    "id": "0aH0PbWSGCxy"
   },
   "source": [
    "Re-rank retreived documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_fdj_udmGD3G",
   "metadata": {
    "id": "_fdj_udmGD3G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_inputs = [(question, passages[idx]) for idx in corpus_ids[0]]\n",
    "cross_scores = xenc_model.predict(model_inputs)\n",
    "\n",
    "print(\"Cross-encoder model re-ranking results\")\n",
    "print(f\"Query: \\\"{question}\\\"\")\n",
    "print(\"---------------------------------------\")\n",
    "for idx in np.argsort(-cross_scores)[:5]:\n",
    "    print(f\"Score: {cross_scores[idx]:.4f}\\nDocument: \\\"{passages[corpus_ids[0][idx]]}\\\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "-_hmhwoiGEXm",
   "metadata": {
    "id": "-_hmhwoiGEXm"
   },
   "source": [
    "Use best match to answer (and compare to reference answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n0veVOPWGEtN",
   "metadata": {
    "id": "n0veVOPWGEtN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "passage_idx = np.argsort(-cross_scores)[0]\n",
    "passage = passages[corpus_ids[0][passage_idx]]\n",
    "\n",
    "input_text = f\"Given the following passage, answer the related question.\\n\\nPassage:\\n\\n{passage}\\n\\nQ: {question}?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "print(input_text, \"\\n\")\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=32)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(output_text, \"\\n\")\n",
    "\n",
    "print(f\"A (target): {target_answer}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "XBCsWWE6KpAA",
   "metadata": {
    "id": "XBCsWWE6KpAA"
   },
   "source": [
    "How do we know if the passage was useful and the model haven't exploited weights memorisation?\n",
    "Let's try to generate directly the response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sIYDlm5kKpOC",
   "metadata": {
    "id": "sIYDlm5kKpOC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = f\"Answer the following question.\\n\\nQ: {question}?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "print(input_text)\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=32)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32ad7316-03ba-4f90-a847-774bc3543bd8",
   "metadata": {
    "id": "32ad7316-03ba-4f90-a847-774bc3543bd8"
   },
   "source": [
    "### Putting all together\n",
    "\n",
    "We can finally set up an entire question answering pipeline:\n",
    "- We have the knowledge\n",
    "- We have the retreival system\n",
    "    - We also have the re-ranking system\n",
    "- We have the asnwering system\n",
    "\n",
    "Let's define a function that puts everything together and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88153175-be77-451b-8955-f5851a45d2a3",
   "metadata": {
    "id": "88153175-be77-451b-8955-f5851a45d2a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qa_pipeline(\n",
    "    question, \n",
    "    similarity_model=semb_model, \n",
    "    embeddings_index=index, \n",
    "    re_ranking_model=xenc_model, \n",
    "    generative_model=model,\n",
    "    device=device\n",
    "): \n",
    "    if not question.endswith('?'):\n",
    "        question = question + '?'\n",
    "    # Embed question\n",
    "    question_embedding = semb_model.encode(question, convert_to_tensor=True)\n",
    "    # Search documents similar to question in index \n",
    "    corpus_ids, distances = index.knn_query(question_embedding.cpu(), k=64)\n",
    "    # Re-rank results\n",
    "    xenc_model_inputs = [(question, passages[idx]) for idx in corpus_ids[0]]\n",
    "    cross_scores = xenc_model.predict(model_inputs)\n",
    "    # Get best matching passage\n",
    "    passage_idx = np.argsort(-cross_scores)[0]\n",
    "    passage = passages[corpus_ids[0][passage_idx]]\n",
    "    # Encode input\n",
    "    input_text = f\"Given the following passage, answer the related question.\\n\\nPassage:\\n\\n{passage}\\n\\nQ: {question}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Generate output\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=64)\n",
    "    # Decode output\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return result\n",
    "    return f\"Passage:\\n\\n{passage}\\n\\nQ: {question}\\n\\nA: {output_text}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3db6bb86-3d6c-4b2f-a963-5da9764593b6",
   "metadata": {
    "id": "3db6bb86-3d6c-4b2f-a963-5da9764593b6",
    "tags": []
   },
   "source": [
    "Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kVX3SXdtFijU",
   "metadata": {
    "id": "kVX3SXdtFijU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = input(\"Ask a question >>> \")\n",
    "print()\n",
    "\n",
    "print(qa_pipeline(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e15b6bb6-2714-4654-a2be-7fe4f9db2e1f",
   "metadata": {
    "id": "e15b6bb6-2714-4654-a2be-7fe4f9db2e1f"
   },
   "source": [
    "## Generative chatbots\n",
    "\n",
    "Language models can be used to generate text in dialogues.\n",
    "Now we are going to see how to use transformer language models as generative chatbots.\n",
    "\n",
    "As usual we are going to use the Transformer library from HuggingFace. \n",
    "All generative models there implement a `generate()` methods we are going to use.\n",
    "You can find the documentation here: https://huggingface.co/docs/transformers/main_classes/text_generation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42e15b7-4cbf-4953-9bba-e7f48d5bc4af",
   "metadata": {
    "id": "d42e15b7-4cbf-4953-9bba-e7f48d5bc4af"
   },
   "source": [
    "### Pretrained models\n",
    "\n",
    "For starter let's play around with a pre-trained model.\n",
    "We can load the [DialoGPT](https://arxiv.org/abs/1911.00536) chatbot, a fine-tuning of GPT-2 trained o large collections of conversations crawled from Reddit.\n",
    "\n",
    "We can start seeing different ways to decode (generate) responses using this autoregressive model.\n",
    "What we want to do is use the output probability distribution to select a token compsing a response.\n",
    "Hopefully we select the most probable sequence, actually that's not feasible.\n",
    "\n",
    "Let's proceed step-by-step.\n",
    "First of all get model and tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add29d0-c57b-4543-85b5-d85ce5a4923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e38cc8-c30a-4fe6-97eb-e24e99401d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip uninstall -y transformers\n",
    "#!pip -q install transformers==4.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d201fd9-c66d-429f-84f5-ec7090c13f93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229,
     "referenced_widgets": [
      "ea6e7a8957754378bb4e4a87f460207a",
      "f012d92822dc46bebd6881fe48d0312c",
      "5bd9e91e79814b54a7a6545ab57d4e41",
      "d618a30185c74432aee3907ec8b1225d",
      "1d70dcadb8f74194b088e4dfdea0ae55",
      "736c682a759242888297a3c1b7504c65",
      "58f3cb124dcf45438a749da2678e6a32",
      "32629bccb57048c4a28cee33eefdf8de",
      "7e53b5a609514709afd747a034f53400",
      "f99f6290ca3640a18a9fbe41980f3411",
      "f43e68bb14bc49f4a78491a525461689",
      "a20c80af13fb4037852605f82f714cd1",
      "cf40ac805ffd421d9ecd1db6894efee0",
      "0022b9d7c82b478e8c17bc56edd3cab5",
      "eacb7cc951434c72bddcd5754a4ae78a",
      "843c830297594fe7aa7cacd4fea6bfc3",
      "405205e401bf4972b3ed1ff0e11e5af8",
      "afec3e01ffaf438a92cb11b358694f42",
      "34730f6f972c47a18809b1bacd550364",
      "4cfd24194cde45d2967b17808dea993e",
      "76105038289540a68a560379a6c361e5",
      "de1d5b5b97364df28408441b99209c99",
      "94075dee90e2425689176dbe91cef4ee",
      "0e70451ca4c3478ea8bd32b628f5ea8a",
      "48a8325dc4a443e5a9bc467bf3df2f78",
      "3511be86d8934ab4ac64ed3d8e02a65f",
      "8495ad273edf4ebfadc8bc0d377ae8dd",
      "c62e94fde69f46afa505eb24ba03be3d",
      "6ce125927e8245e990d5606a516b472f",
      "6ba8155b35924dfcbdce02828e4ed64c",
      "7c14cc02a6c144bc9f8b491973b719a7",
      "37784f58d647426698ba34a9e96bbb34",
      "c84d5275fe0845a29916d77adf34630c",
      "757e467c74f14465b743219e68a65326",
      "0f3643c0b3d644b2889e27d51b302fbe",
      "a1aa44f1301d4295b63e3cbefcbc86f3",
      "2acba93c4b21404ca367f44c13dc1002",
      "e316fa98c07241f38057e42c471fdee7",
      "664782bff2714ee4b365de17130e1245",
      "6d05e6d754b84e49ab70b21b34a276f3",
      "dc91ac92549546f2b528acdc5d7cd86f",
      "5190a6e308084dafb92ca9e8efb26e48",
      "9e21e71dd8a3454fbebf7d3ca25df966",
      "a60568f613ca4bcd8da9150a54f60eb3",
      "83177b6659ba46ee8a9b8a88dc90f098",
      "8c6650c299b24c37b0c95b6e60c08125",
      "e220b17e036e41f8807230e321baf524",
      "f244f885a9fa4265b20b88dddbe7f37e",
      "15d33819bd21408898dd600aff77323c",
      "93207531a2d04714ba2c0a955c61a67d",
      "552aa9a8d19947b9bb8ef71769649a22",
      "b9cd29f61c8b428f97e32e140c0250ef",
      "34f84e904398483092b6e64d3e382a6f",
      "1cb3e1aedf8a416380ba288e333ce925",
      "328ae82f282c4cccbc324c30a409d536",
      "6043c4db0f2f45209fb146fc8914ece8",
      "981cf3b90fa642819254afc204a4e6ae",
      "18a1b79006ca47d8b1e7e74385ace43b",
      "1a3aea788dfc42c884296fbde19757dc",
      "5a2300a1029c4a4bb28bea94a09c72de",
      "7a696bac8dee40f1b3d8010636b254ed",
      "5a23e7c48eb34a65bd5b07b6db00a3da",
      "4bfb2488f3284728be62cf92e3782ead",
      "1c807338e6e74c3195213ddb03868617",
      "f15b974cbc8847be9047faff6a86a630",
      "5ca0ea332a754fedaba0df7fd3f8f0f8"
     ]
    },
    "executionInfo": {
     "elapsed": 28952,
     "status": "ok",
     "timestamp": 1684220909991,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "8d201fd9-c66d-429f-84f5-ec7090c13f93",
    "outputId": "18137c3d-2dca-4ede-8b78-b5230e5e76af",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\", device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f7a3e35-8a2c-420c-86d1-bd832c1c1b8c",
   "metadata": {
    "id": "8f7a3e35-8a2c-420c-86d1-bd832c1c1b8c"
   },
   "source": [
    "#### How does it work?\n",
    "\n",
    "First we need to understand how to provide data to our model\n",
    "\n",
    "Up to a couple of years ago, the standard appraoch to present the input to these models was to separate each utterance with a `end-of-sequence` token.\n",
    "The model would generate an answer and stop every time the `end-of-sequence` tokens is generated.\n",
    "\n",
    "```\n",
    "\"<|endoftext|>Summer loving had me a blast<|endoftext|>Summer loving happened so fast<|endoftext|>I met a girl crazy for me<|endoftext|>Met a boy cute as can be<|endoftext|>\"\n",
    "```\n",
    "\n",
    "Nowadays the appraoch is to have an uninterrupted stream of text, like a movie script\n",
    "\n",
    "```\n",
    "\"\n",
    "A: Hello.\n",
    "B: Is it me you're looking for?\n",
    "A: I can see it in your eyes...\n",
    "B: I can see it in your smile!\n",
    "\"\n",
    "```\n",
    "\n",
    "DialoGPT uses the `end-of-sequence` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BmPTK6_FaWab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1684221375540,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "BmPTK6_FaWab",
    "outputId": "f50a3b7b-fad5-4271-d8f4-e93ebeb62f50",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "CNa-cqd6aW7s",
   "metadata": {
    "id": "CNa-cqd6aW7s"
   },
   "source": [
    "Let's create a context to use as input for our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017b027-48e2-49c9-9947-efc7175899f4",
   "metadata": {
    "id": "9017b027-48e2-49c9-9947-efc7175899f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = [\n",
    "    \"Hello, how are you?\", \n",
    "    \"I'm fine thanks, how about you?\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "krHwj4g2Zo1M",
   "metadata": {
    "id": "krHwj4g2Zo1M"
   },
   "source": [
    "Not let's create an input string from the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bJ5YikpZpKD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "executionInfo": {
     "elapsed": 1538,
     "status": "ok",
     "timestamp": 1684221262540,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "5bJ5YikpZpKD",
    "outputId": "2e90930c-e83b-4295-e27c-e717467c7fec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_string = tokenizer.eos_token \n",
    "if len(context) > 0:\n",
    "    input_string = tokenizer.eos_token + tokenizer.eos_token.join(context) + input_string\n",
    "\n",
    "input_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c8eae1-5c9a-4c31-8a18-119e5fb35f41",
   "metadata": {
    "id": "66c8eae1-5c9a-4c31-8a18-119e5fb35f41"
   },
   "source": [
    "Encode input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b00b79-1ac5-4eff-8b9f-92c20afd5601",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1684221517239,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "61b00b79-1ac5-4eff-8b9f-92c20afd5601",
    "outputId": "d6f6e207-0009-4d1d-9b9c-8e40df9dbce2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_encoding = tokenizer(input_string, return_tensors='pt').to(device)\n",
    "print(input_encoding.input_ids)\n",
    "print(input_encoding.input_ids.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4b824b7-f7f9-4c43-ab88-54edb088faff",
   "metadata": {
    "id": "a4b824b7-f7f9-4c43-ab88-54edb088faff"
   },
   "source": [
    "If we run the sequence through the model, we get a series of logits as output.\n",
    "Since we are using an autogressive models, in the last position we will have the logits of next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e39f77-7c7c-4760-bba3-6a27399813ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1233,
     "status": "ok",
     "timestamp": 1684221533383,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "a1e39f77-7c7c-4760-bba3-6a27399813ab",
    "outputId": "946a891b-d196-4bbb-cf42-2bacfe77df19",
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(**input_encoding)\n",
    "print(outputs.logits)\n",
    "print(outputs.logits.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36be972b-5239-453f-ba21-5848608d1b11",
   "metadata": {
    "id": "36be972b-5239-453f-ba21-5848608d1b11"
   },
   "source": [
    "We can run these logits through a $\\mathrm{softmax}(\\cdot)$ and obtain the probability distribution over tokens:\n",
    "- for each possible token we have the probability of it being the next in the sequence\n",
    "- We can sample a token from this probability distribution and recurr itin input to get a new token\n",
    "- We can iterate this process to compose a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf5181-49e9-4a77-9aa6-e5396f9f130d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1684221627332,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "dadf5181-49e9-4a77-9aa6-e5396f9f130d",
    "outputId": "45c9a2b5-3597-4c33-f6de-4226810c7ccd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_dist_next = torch.softmax(outputs.logits[:, -1], dim=1)\n",
    "print(p_dist_next)\n",
    "print(p_dist_next.sum())\n",
    "print(p_dist_next.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "XZTr4ZoYbuZQ",
   "metadata": {
    "id": "XZTr4ZoYbuZQ"
   },
   "source": [
    "What is the most probable next token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MLx4yISpbu0P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 582,
     "status": "ok",
     "timestamp": 1684221861114,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "MLx4yISpbu0P",
    "outputId": "1bfaf646-5e52-45e9-e4e6-c952a0ffd51e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "arg_max_idx = torch.argmax(p_dist_next)\n",
    "print(arg_max_idx)\n",
    "tokenizer.decode(arg_max_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0wAujEcgb5ab",
   "metadata": {
    "id": "0wAujEcgb5ab"
   },
   "source": [
    "Note that we don't need to run the $\\mathrm{softmax}(\\cdot)$ operation if we just want to find the token with highest probability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6599151a-ae4e-4442-90be-bae9d3d36342",
   "metadata": {
    "id": "6599151a-ae4e-4442-90be-bae9d3d36342"
   },
   "source": [
    "#### Deterministic decoding\n",
    "\n",
    "Deterministic appraoches yield always the same output for a given input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01ac99e9-8968-4e17-84d2-629609a3ead4",
   "metadata": {
    "id": "01ac99e9-8968-4e17-84d2-629609a3ead4"
   },
   "source": [
    "##### Greedy decoding\n",
    "\n",
    "The most starightforward way is to pick each time the most probable token and recurr it as next step in input.\n",
    "Very suboptimal solution, usually yields dull responses like `\"I don't know\"` or causes degenerate generation (e.g., repeating the same token many times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7406082-e460-44af-beca-c256023c1e09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1684222694094,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "b7406082-e460-44af-beca-c256023c1e09",
    "outputId": "a631a33d-ae1d-4742-b88a-1857d64c84ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a59866a-0764-4b86-a055-099fc99a1f83",
   "metadata": {
    "id": "8a59866a-0764-4b86-a055-099fc99a1f83"
   },
   "source": [
    "##### Beam search\n",
    "\n",
    "We cannot do an exhaustuve search, but we can keep the top $n$ most probable sequences up to now.\n",
    "This is what beam search does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf9b6e-7e5c-4e5b-9dd1-597512bed9af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1684223360428,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "dcbf9b6e-7e5c-4e5b-9dd1-597512bed9af",
    "outputId": "8490aeaa-39dd-4f33-b4a8-b686a185284b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, num_beams=8, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "233a3836-857e-4e43-ae25-40fc042ae980",
   "metadata": {
    "id": "233a3836-857e-4e43-ae25-40fc042ae980"
   },
   "source": [
    "#### Sampling\n",
    "\n",
    "Sampling based decoding adds more spice to the output sampling the next token with a certain probability given by the language model.\n",
    "The nice thing is that given the same input the generated content may change (higher diversity in the text of responses), the bad thing is that given the same input the generated content may change (possibly inconsistent behaviour)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3b9ebd2-8734-49fa-bfa9-7c5066a3acfd",
   "metadata": {
    "id": "f3b9ebd2-8734-49fa-bfa9-7c5066a3acfd",
    "tags": []
   },
   "source": [
    "##### Top-k\n",
    "\n",
    "Consider only first $k$ most probable tokens and zero out others probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YhXiOu96iGrS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1684223385533,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "YhXiOu96iGrS",
    "outputId": "d50655f7-14cf-4d09-9b24-2b7e1bec6496",
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, top_k=16, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7a4e683-53d5-429e-b0a4-36f532327e4b",
   "metadata": {
    "id": "f7a4e683-53d5-429e-b0a4-36f532327e4b"
   },
   "source": [
    "##### Top-p (nucleus sampling)\n",
    "\n",
    "Consider only first most probable tokens so that their probability sum up to $p \\in [0, 1] \\subseteq \\mathbb{R}$ and zero out others probabilities \n",
    "Similar to top-$k$ but variable window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0o1qdXnuiYYM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 1135,
     "status": "ok",
     "timestamp": 1684223435749,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "0o1qdXnuiYYM",
    "outputId": "74634fa8-1195-481f-8838-641d1a381a75",
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, top_p=0.95, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0768d0db-61f4-4095-a6f8-169c5ebf54b0",
   "metadata": {
    "id": "0768d0db-61f4-4095-a6f8-169c5ebf54b0"
   },
   "source": [
    "##### Temperature rescoring\n",
    "\n",
    "Divide the logits by a value $\\tau \\in \\mathbb{R}^+_0$:\n",
    "- if $\\tau > 1$ (high temprature) the distribution get softer (reduces probability of most probable tokens and increases that of least probable)\n",
    "- if $\\tau = 1$ the distribution is unchanged\n",
    "- if $\\tau < 1$ (low temperature) the distribution get sharper (reduces probability of most probable tokens and increases that of least probable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d6b87-6852-46c3-91b5-f6cf25bd3ca7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 1210,
     "status": "ok",
     "timestamp": 1684223610679,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "a50d6b87-6852-46c3-91b5-f6cf25bd3ca7",
    "outputId": "474b6c48-d8ab-4992-9ec2-bfe7ba1efc9e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, temperature=0.8, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51f6ebb9-3a97-4c16-9f79-5578fd186fc7",
   "metadata": {
    "id": "51f6ebb9-3a97-4c16-9f79-5578fd186fc7"
   },
   "source": [
    "##### Sample multiple candidates\n",
    "\n",
    "You can also sample muliple candidates an pick one according to some criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NmgGJhnqjQye",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1651,
     "status": "ok",
     "timestamp": 1684223754559,
     "user": {
      "displayName": "Vincenzo Scotti",
      "userId": "11083671475743415769"
     },
     "user_tz": -120
    },
    "id": "NmgGJhnqjQye",
    "outputId": "25daff45-6df1-4331-e3a4-bb24e84a7335",
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, num_return_sequences=8, top_p=0.9, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(output_ids[:, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "j0O6N-ugj6er",
   "metadata": {
    "id": "j0O6N-ugj6er"
   },
   "source": [
    "For example if you combine sampling and beam search the `generate()` method will output automatically the most probable of the samples sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6o87VHHYkIg-",
   "metadata": {
    "id": "6o87VHHYkIg-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, temperature=0.8, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "BwYbVIRflJzv",
   "metadata": {
    "id": "BwYbVIRflJzv"
   },
   "source": [
    "#### Chatting\n",
    "\n",
    "Now pick your favourite appraoch and chat with DialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J7oVpe87lRPN",
   "metadata": {
    "id": "J7oVpe87lRPN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Maximum dialogue length (in turn pairs)\n",
    "max_len = 5\n",
    "# Initialise dialogue history\n",
    "dialogue_history = []\n",
    "\n",
    "for i in range(max_len):\n",
    "    # Read user message\n",
    "    user_message = input(\"User: \")\n",
    "    # Append message to dialogue history\n",
    "    dialogue_history.append(user_message)\n",
    "    # Convert dialogue to string\n",
    "    input_string = tokenizer.eos_token \n",
    "    if len(context) > 0:\n",
    "        input_string = tokenizer.eos_token + tokenizer.eos_token.join(dialogue_history) + input_string\n",
    "    # Encode input\n",
    "    input_encoding = tokenizer(input_string, return_tensors='pt').to(device)\n",
    "    # Generate DialoGPT response\n",
    "    output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, top_p=0.9, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "    chatbot_response = tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)\n",
    "    # Append chatbot response to dialogue history\n",
    "    dialogue_history.append(chatbot_response)\n",
    "    # Print chatbot response\n",
    "    print(f\"DialoGPT: {chatbot_response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "qZKpwC1hlWNh",
   "metadata": {
    "id": "qZKpwC1hlWNh"
   },
   "source": [
    "Notes\n",
    "1. As you go forward the model will start gettin slower. There is a way to cache the hidden outputs to avoid re-computing attention on past \n",
    "2. The context of the model is limited, at some point you should start dropping older utterances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e7fe2bd-3208-401e-a2f2-875dbc66910f",
   "metadata": {
    "id": "9e7fe2bd-3208-401e-a2f2-875dbc66910f"
   },
   "source": [
    "### Fine-tuning\n",
    "\n",
    "Now you are ready, you can finally fine-tune a generative chatbot and chat with it instead of studying for the exams! (I am not responsable for your choices, I am just offering you an alterantive)\n",
    "\n",
    "We wiil fine-tune a vanilla GPT-2 (pick your favourite version).\n",
    "Let's load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70472be8-8ef5-486b-a71d-8b95d977963f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_id = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id , device_map=\"auto\")  # , torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7883fed-35f6-4220-90bb-c1699ea709e5",
   "metadata": {},
   "source": [
    "Let's set the padding token to be the `eos_token` to simplify some passages later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af333157-15fe-44ce-aac8-7c96cb7fc9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c6f452a-378d-4d1f-9059-6de7c163a85e",
   "metadata": {
    "id": "5c6f452a-378d-4d1f-9059-6de7c163a85e"
   },
   "source": [
    "#### Data preparation\n",
    "\n",
    "We are going to use the [Persona-Chat](https://arxiv.org/abs/1801.07243) corpus (It was used in the ConvAI 2 challenge).\n",
    "It's a data set where conversations are grounded in the persona description of the two participants.\n",
    "\n",
    "We are going to use the [ParlAI](https://parl.ai/docs/index.html) package to get the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83703547-032e-404d-9faa-f2310f1fcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install parlai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "285f6aed-edad-4d1b-88b9-83b7aad0e661",
   "metadata": {},
   "source": [
    "Now let's download the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bccf187-0798-4851-8c8b-2ff3530a8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.tasks.convai2.build import build\n",
    "\n",
    "build({'datapath': './data/'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ded26e1b-864f-4b26-a63b-d95615ea7ade",
   "metadata": {},
   "source": [
    "Now the samples are stored in `.txt` files we need to parse.\n",
    "We can build a simple function that given the path to one of the files parse the content into Python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfba5a-5f76-4ddd-becb-45d840c493ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pc(path):\n",
    "    # Open file\n",
    "    with open('./data/ConvAI2/train_self_original.txt') as f:\n",
    "        # Read raw file lines\n",
    "        data = [line.strip() for line in f]\n",
    "    # Data set container\n",
    "    persona_chat = list()\n",
    "    # Now we iterate through lines and build the data set\n",
    "    for line in data:\n",
    "        # Split line data from initial index\n",
    "        line_idx, line_data = line.split(' ', 1)\n",
    "        # Check if new conversation is started\n",
    "        if line_idx == '1':\n",
    "            # Add new empthy dialogue in data set\n",
    "            persona_chat.append(\n",
    "                {'persona_a': list(), 'persona_b': list(), 'utterances': list()}\n",
    "            )\n",
    "        # If the line is from Speaker A persona\n",
    "        if line_data.startswith('your persona: '):\n",
    "            # Append it to Persona A\n",
    "            persona_chat[-1]['persona_a'].append(line_data[len('your persona: '):])\n",
    "        # Else if the line is from Speaker B persona\n",
    "        elif line_data.startswith('partner\\'s persona: '):\n",
    "            # Append it to Persona B\n",
    "            persona_chat[-1]['persona_b'].append(line_data[len('partner\\'s persona: '):])\n",
    "        # Else the line is a regular dialogue line\n",
    "        else:\n",
    "            # Split utterances from distractors and separate A and B\n",
    "            utt_a, utt_b = line_data.split('\\t\\t')[0].split('\\t')\n",
    "            # Append to dialogue utterances\n",
    "            persona_chat[-1]['utterances'].append(\n",
    "                {'speaker': 'A', 'text': utt_a}\n",
    "            )\n",
    "            persona_chat[-1]['utterances'].append(\n",
    "                {'speaker': 'B', 'text': utt_b}\n",
    "            )\n",
    "            \n",
    "    return persona_chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "398b99f5-7756-4a36-a69f-bf5f4048dbae",
   "metadata": {},
   "source": [
    "Let's load trainign and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9727ae76-9ffe-4cce-addb-44caec48cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = parse_pc('./data/ConvAI2/train_both_original.txt')\n",
    "validation_data = parse_pc('./data/ConvAI2/valid_both_original.txt')\n",
    "\n",
    "training_data[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5832ea5a-868d-4067-bc6e-d4c838175301",
   "metadata": {},
   "source": [
    "Now we are going to convert to strings all the samples using the `eos_token` as separator.\n",
    "We will include also personae.\n",
    "\n",
    "Let's define another function to do that on a single dialogue, then we will apply it to the entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534ee88-fda2-4f9a-ba4f-d1cfd11c726f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_to_string(sample, eos_token):\n",
    "    # Join strings of Persona A\n",
    "    persona_a = ' '.join(sample['persona_a'])\n",
    "    # Join strings of Persona B\n",
    "    persona_b = ' '.join(sample['persona_b'])\n",
    "    # Join dialogue strings\n",
    "    dialogue = eos_token.join(f\"{utterance['speaker']}: {utterance['text']}\" for utterance in sample['utterances'])\n",
    "    # Build the dialogue string\n",
    "    dialogue_string = f\"Persona A: {persona_a}{eos_token}Persona B: {persona_b}{eos_token}{dialogue}{eos_token}\"\n",
    "    \n",
    "    return dialogue_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "343d207d-58d2-449b-829b-0395df6f3ba0",
   "metadata": {},
   "source": [
    "Apply the funtion to all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2412d-51d2-4494-86d8-a520dd85c1fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data_str = [sample_to_string(dialogue, tokenizer.eos_token) for dialogue in training_data]\n",
    "validation_data_str = [sample_to_string(dialogue, tokenizer.eos_token) for dialogue in validation_data]\n",
    "\n",
    "training_data_str[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c212aafb-33fa-4400-85e2-382ffbabfe69",
   "metadata": {},
   "source": [
    "And now are samples are ready"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e1fb3ec-83a3-4cf9-9aaf-1791e0a50e54",
   "metadata": {
    "id": "3e1fb3ec-83a3-4cf9-9aaf-1791e0a50e54"
   },
   "source": [
    "#### Training\n",
    "\n",
    "Ok we made it to fine-tuning a language model.\n",
    "We can use HuggingFace trainer to do the training.\n",
    "\n",
    "First we need to build trainer compatible Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315c5ea-cccc-4ecf-acd4-01096118282f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data = Dataset.from_dict({'text': training_data_str})\n",
    "valid_data = Dataset.from_dict({'text': validation_data_str})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13d57afa-f9fc-4219-b339-e7a7e1c00f20",
   "metadata": {},
   "source": [
    "... and trainer compatible DatasetDict object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7803cf6-8b2b-49da-a088-343e4bbf34ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['validation'] = valid_data\n",
    "data['test'] = valid_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d18ff25-2deb-487c-bac4-d13cbfa5fd24",
   "metadata": {},
   "source": [
    "Finally use the tokenizer to convert the input strings into sequences of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0b878-f29d-423c-aa25-2f5401ce9a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba4743-3eda-4d59-a712-bffdbce39b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36e8d62c-e923-40e2-9bb4-2d1d99ba47b9",
   "metadata": {},
   "source": [
    "The last step we are missing is to create a collator that gets together all the sequences in the same batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b65f8-ce68-44d4-9c3f-990aa2a26bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "557a6a6e-f8aa-497c-80a8-a3f4f093cf56",
   "metadata": {},
   "source": [
    "Now we can create an instance of the trainer specifying the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fa0fb-424e-409f-97fa-723aac4a6cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"cooler_trainer_name\", \n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=6.25e-5,\n",
    "    lr_scheduler_type=\"linear\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a3f7a2f-36af-4e09-bccb-b6eec2d406e5",
   "metadata": {},
   "source": [
    "Create the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097398f-6c22-47af-9711-3ddfd95cd123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=tokenized_data['train'], \n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8c2e37-bbca-4afa-9c66-5a1a400bbe6d",
   "metadata": {},
   "source": [
    "And now let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c620f-9751-4f1f-a78f-33b502f5103a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cdfcbb0-90c6-4608-8e4b-1a699ab0845e",
   "metadata": {},
   "source": [
    "Finally let's save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0951341f-1524-48f5-8f5c-068254c1bd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = f\"persona_chat_fine_tuning_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "tokenizer.save_pretrained(checkpoint_path)\n",
    "model.save_pretrained(checkpoint_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5dfc6b7-46a9-4e2f-bf22-62de72cdb4a0",
   "metadata": {
    "id": "a5dfc6b7-46a9-4e2f-bf22-62de72cdb4a0"
   },
   "source": [
    "#### Testing\n",
    "\n",
    "We can compute some automatc metrics to assess the quality of the chatbot.\n",
    "We can use the ParlAI utilities to compute the metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9415b82-6942-4fc6-bfa0-4e8938d063a9",
   "metadata": {},
   "source": [
    "Let's pick a random test dialogue and let's generate response.\n",
    "We will compare the original target response to the generated one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2531a04a-a9ce-40c3-a35e-ae1727ab655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1995)\n",
    "\n",
    "idx = random.choice(range(len(validation_data)))\n",
    "print(idx)\n",
    "dialogue = validation_data[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "389e7d8d-cb62-4eaa-994b-6655836abd65",
   "metadata": {},
   "source": [
    "Now we can pick a turn from the middle of a dialogue to be our target response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdcce83-a332-43a0-a42d-cd1617d79c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_idx = len(dialogue['utterances']) // 2\n",
    "\n",
    "original_response = dialogue['utterances'][response_idx]\n",
    "original_response_string = f\"{original_response['speaker']}: {original_response['text']}\"\n",
    "original_response_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1046041-ee6a-42a0-8d43-c19adc0fd1d3",
   "metadata": {},
   "source": [
    "And we can drop the response and all the following ones to build ourr context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e8a1e-d2fc-4321-bb5d-12471b3f73a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = {\n",
    "    'persona_a': dialogue['persona_a'],\n",
    "    'persona_b': dialogue['persona_b'],\n",
    "    'utterances': dialogue['utterances'][:response_idx]\n",
    "}\n",
    "context_string = sample_to_string(context, tokenizer.eos_token)\n",
    "context_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6118d7d-21a7-4df5-9867-afb76c4dfb69",
   "metadata": {},
   "source": [
    "And let's load back the model from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efaec23-1bd6-46ab-a6c3-7583a609dd47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path , device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7bc98ad-cbbd-4b8c-8182-50045e33ef59",
   "metadata": {},
   "source": [
    "Generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba42ba8-bb7f-4812-814c-0b8dcd266d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode context\n",
    "input_encoding = tokenizer(context_string, return_tensors='pt').to(device)\n",
    "# Generate response\n",
    "output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, top_p=0.9, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "# Decode generated response\n",
    "generated_response = tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ifOnlKvRl3e9",
   "metadata": {
    "id": "ifOnlKvRl3e9"
   },
   "source": [
    "##### Perplexity\n",
    "\n",
    "Perplexity can be computed using the cross entropy on the generated response.\n",
    "First let's process the the context and the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0rjR2Q9-l3zQ",
   "metadata": {
    "id": "0rjR2Q9-l3zQ"
   },
   "outputs": [],
   "source": [
    "# Encode dialogue\n",
    "input_encoding = tokenizer(context_string + original_response_string + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "# Compute model outputs\n",
    "outputs = model(**input_encoding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6a7f630-c5e5-4279-a963-67cb7044b510",
   "metadata": {},
   "source": [
    "We get the target labels (the ids of the response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91787b2b-5889-4d2c-ad3a-45f64baa0133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = tokenizer(original_response_string + tokenizer.eos_token, return_tensors='pt').input_ids.to(device)\n",
    "labels.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e3f6f81-3318-42d5-bd73-562f1e1d4014",
   "metadata": {},
   "source": [
    "And then we retain only the logits from the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91c772-a045-4fe0-ade9-3442387c0bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = outputs.logits[:, -labels.size(1):]\n",
    "logits.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff72d697-7d5d-488f-9955-0dc8ad3bcba7",
   "metadata": {},
   "source": [
    "Compute the average cross-entropy shifting the inputs and the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3f8ab-082e-4ae0-900c-3f55accff807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Shift logits to exclude the last element\n",
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "# shift labels to exclude the first element\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "# Compute loss\n",
    "lm_loss = F.cross_entropy(\n",
    "    shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    ")\n",
    "lm_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a6d30b-7e04-4fc8-99ec-c7bc6e5026e3",
   "metadata": {},
   "source": [
    "Exponentiate to have PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf90c3-2cfd-4e3d-a210-4637e8cd7205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppl = torch.exp(lm_loss)\n",
    "ppl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ef11911-30a6-494d-848d-7c44addb75fe",
   "metadata": {},
   "source": [
    "The process can be simplified but at least now you have seen all the steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "T9by6tNYl548",
   "metadata": {
    "id": "T9by6tNYl548"
   },
   "source": [
    "BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993AnB2Fl6Gv",
   "metadata": {
    "id": "993AnB2Fl6Gv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from parlai.core.metrics import BleuMetric\n",
    "\n",
    "bleu = BleuMetric.compute(generated_response, [original_response_string])\n",
    "print(f\"BLEU: {bleu}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d33d5fa-8503-455e-9e50-af0753a6df0f",
   "metadata": {},
   "source": [
    "##### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be237c-13f4-4f40-bb28-3c719d4d85e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from parlai.core.metrics import F1Metric\n",
    "\n",
    "f1_score = F1Metric.compute(generated_response, [original_response_string])\n",
    "print(f\"F1: {f1_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "hjEBLEHxmFYY",
   "metadata": {
    "id": "hjEBLEHxmFYY"
   },
   "source": [
    "##### Chatting\n",
    "\n",
    "There is no better way to evalaute a generative chatbot than using it to chat.\n",
    "Define a custom persona for you and the chatbot (or sample two from the data set) and write a chatting loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qGArtuODmGBy",
   "metadata": {
    "id": "qGArtuODmGBy"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c027b0f6-1e3a-4db1-b826-2b1d73a3993d",
   "metadata": {
    "id": "c027b0f6-1e3a-4db1-b826-2b1d73a3993d"
   },
   "source": [
    "## ELIZA meets DialoGPT\n",
    "\n",
    "In the 70s they made ELIZA and PARRY meet each other: https://www.theatlantic.com/technology/archive/2014/06/when-parry-met-eliza-a-ridiculous-chatbot-conversation-from-1972/372428/.\n",
    "We could you have ELIZA meet ChatGPT, but since we are humble we will settle with DialoGPT.\n",
    "\n",
    "The is this implementation of ELIZA in Python we can use: https://github.com/wadetb/eliza\n",
    "Let's start by cloning the repository and adding it to our path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b86ea-6590-4772-ad70-3cd1d78a3758",
   "metadata": {
    "id": "c426eff4-335d-4b33-b5ec-fa5ebd43102a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/wadetb/eliza.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b810a-66e6-4bce-aa10-a523d125d360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/arcslab/Documents/vincenzo_scotti_polimi/rp_3_1/notebooks/eliza/eliza.py')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f09c196a-feb5-4540-a96f-1619dcaa2e22",
   "metadata": {
    "id": "f09c196a-feb5-4540-a96f-1619dcaa2e22"
   },
   "source": [
    "Now we should be able to import the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3460802-729f-4fd2-95d0-745801b2e144",
   "metadata": {
    "id": "734f32d3-8fee-4316-b717-eb67bf5e6c15",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from eliza.eliza import Eliza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33559208-537b-436d-b4c3-a103fe951bff",
   "metadata": {
    "id": "33559208-537b-436d-b4c3-a103fe951bff"
   },
   "source": [
    "Now let's create an instance of ELIZA using the available rules (https://github.com/wadetb/eliza/blob/master/doctor.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e1c85-7db1-4e52-9f4f-c955176c0488",
   "metadata": {
    "id": "7aad7262-3e1c-42a1-b16c-8b3b492865c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "eliza = Eliza()\n",
    "eliza.load('./eliza/doctor.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f0622c5-9600-419b-83c2-45cefb0a2901",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now you can chat with ELIZA.\n",
    "Note that ELIZA manages its context interanlly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ea019-de3b-4b2b-b2dd-a976f8eb7145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Maximum dialogue length (in turn pairs)\n",
    "max_len = 5\n",
    "\n",
    "for i in range(max_len):\n",
    "    # Read user message\n",
    "    user_message = input(\"User: \")\n",
    "    # Ask ELIZA for response\n",
    "    response = eliza.respond(user_message)\n",
    "    # Print ELIZA response\n",
    "    print(f\"ELIZA: {response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a453651-0b0e-4b4f-bf3d-49873ddc5d35",
   "metadata": {},
   "source": [
    "But that's not as fun as having DialoGPT chat with ELIZA, right?\n",
    "Let's load back DialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d272120-4469-4353-a970-a6df53c5717f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_id = 'microsoft/DialoGPT-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id , device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff2cea5b-faa4-414d-8868-8263e0e41200",
   "metadata": {},
   "source": [
    "Now we can re-use the same chatting script from before, but instead of asking input to the user, we are going to ask ELIZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77561898-ae6f-459c-bc75-5bc8cae3726d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Maximum dialogue length (in turn pairs)\n",
    "max_len = 5\n",
    "# Initialise dialogue history\n",
    "dialogue_history = [\"Hello\"]\n",
    "# Print first message\n",
    "print(f\"DialoGPT: {dialogue_history[0]}\")\n",
    "\n",
    "\n",
    "for i in range(max_len):\n",
    "    # Generate ELIZA response\n",
    "    eliza_message = eliza.respond(dialogue_history[-1])\n",
    "    # Append message to dialogue history\n",
    "    dialogue_history.append(eliza_message)\n",
    "    # Print ELIZA response\n",
    "    print(f\"ELIZA: {eliza_message}\")\n",
    "    # Convert dialogue to string\n",
    "    input_string = tokenizer.eos_token \n",
    "    if len(context) > 0:\n",
    "        input_string = tokenizer.eos_token + tokenizer.eos_token.join(dialogue_history) + input_string\n",
    "    # Encode input\n",
    "    input_encoding = tokenizer(input_string, return_tensors='pt').to(device)\n",
    "    # Generate DialoGPT response\n",
    "    output_ids = model.generate(input_encoding.input_ids, max_new_tokens=32, do_sample=True, top_p=0.9, top_k=0, pad_token_id=tokenizer.eos_token_id)\n",
    "    dialogpt_message = tokenizer.decode(output_ids[0, input_encoding.input_ids.size(1):], skip_special_tokens=True)\n",
    "    # Append DialoGPT response to dialogue history\n",
    "    dialogue_history.append(dialogpt_message)\n",
    "    # Print DialoGPT response\n",
    "    print(f\"DialoGPT: {dialogpt_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0053dff-63f9-40d7-8be7-bd9ee29366b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0022b9d7c82b478e8c17bc56edd3cab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34730f6f972c47a18809b1bacd550364",
      "max": 642,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4cfd24194cde45d2967b17808dea993e",
      "value": 642
     }
    },
    "0e70451ca4c3478ea8bd32b628f5ea8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c62e94fde69f46afa505eb24ba03be3d",
      "placeholder": "​",
      "style": "IPY_MODEL_6ce125927e8245e990d5606a516b472f",
      "value": "Downloading (…)olve/main/vocab.json: 100%"
     }
    },
    "0f3643c0b3d644b2889e27d51b302fbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_664782bff2714ee4b365de17130e1245",
      "placeholder": "​",
      "style": "IPY_MODEL_6d05e6d754b84e49ab70b21b34a276f3",
      "value": "Downloading (…)olve/main/merges.txt: 100%"
     }
    },
    "15d33819bd21408898dd600aff77323c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18a1b79006ca47d8b1e7e74385ace43b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4bfb2488f3284728be62cf92e3782ead",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1c807338e6e74c3195213ddb03868617",
      "value": 124
     }
    },
    "1a3aea788dfc42c884296fbde19757dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f15b974cbc8847be9047faff6a86a630",
      "placeholder": "​",
      "style": "IPY_MODEL_5ca0ea332a754fedaba0df7fd3f8f0f8",
      "value": " 124/124 [00:00&lt;00:00, 6.66kB/s]"
     }
    },
    "1c807338e6e74c3195213ddb03868617": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1cb3e1aedf8a416380ba288e333ce925": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d70dcadb8f74194b088e4dfdea0ae55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2acba93c4b21404ca367f44c13dc1002": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e21e71dd8a3454fbebf7d3ca25df966",
      "placeholder": "​",
      "style": "IPY_MODEL_a60568f613ca4bcd8da9150a54f60eb3",
      "value": " 456k/456k [00:00&lt;00:00, 22.4MB/s]"
     }
    },
    "32629bccb57048c4a28cee33eefdf8de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "328ae82f282c4cccbc324c30a409d536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34730f6f972c47a18809b1bacd550364": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34f84e904398483092b6e64d3e382a6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3511be86d8934ab4ac64ed3d8e02a65f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37784f58d647426698ba34a9e96bbb34",
      "placeholder": "​",
      "style": "IPY_MODEL_c84d5275fe0845a29916d77adf34630c",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 10.4MB/s]"
     }
    },
    "37784f58d647426698ba34a9e96bbb34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "405205e401bf4972b3ed1ff0e11e5af8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48a8325dc4a443e5a9bc467bf3df2f78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ba8155b35924dfcbdce02828e4ed64c",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7c14cc02a6c144bc9f8b491973b719a7",
      "value": 1042301
     }
    },
    "4bfb2488f3284728be62cf92e3782ead": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cfd24194cde45d2967b17808dea993e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5190a6e308084dafb92ca9e8efb26e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "552aa9a8d19947b9bb8ef71769649a22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "58f3cb124dcf45438a749da2678e6a32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a2300a1029c4a4bb28bea94a09c72de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a23e7c48eb34a65bd5b07b6db00a3da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bd9e91e79814b54a7a6545ab57d4e41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32629bccb57048c4a28cee33eefdf8de",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e53b5a609514709afd747a034f53400",
      "value": 26
     }
    },
    "5ca0ea332a754fedaba0df7fd3f8f0f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6043c4db0f2f45209fb146fc8914ece8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_981cf3b90fa642819254afc204a4e6ae",
       "IPY_MODEL_18a1b79006ca47d8b1e7e74385ace43b",
       "IPY_MODEL_1a3aea788dfc42c884296fbde19757dc"
      ],
      "layout": "IPY_MODEL_5a2300a1029c4a4bb28bea94a09c72de"
     }
    },
    "664782bff2714ee4b365de17130e1245": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ba8155b35924dfcbdce02828e4ed64c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ce125927e8245e990d5606a516b472f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d05e6d754b84e49ab70b21b34a276f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "736c682a759242888297a3c1b7504c65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "757e467c74f14465b743219e68a65326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f3643c0b3d644b2889e27d51b302fbe",
       "IPY_MODEL_a1aa44f1301d4295b63e3cbefcbc86f3",
       "IPY_MODEL_2acba93c4b21404ca367f44c13dc1002"
      ],
      "layout": "IPY_MODEL_e316fa98c07241f38057e42c471fdee7"
     }
    },
    "76105038289540a68a560379a6c361e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a696bac8dee40f1b3d8010636b254ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c14cc02a6c144bc9f8b491973b719a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e53b5a609514709afd747a034f53400": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83177b6659ba46ee8a9b8a88dc90f098": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c6650c299b24c37b0c95b6e60c08125",
       "IPY_MODEL_e220b17e036e41f8807230e321baf524",
       "IPY_MODEL_f244f885a9fa4265b20b88dddbe7f37e"
      ],
      "layout": "IPY_MODEL_15d33819bd21408898dd600aff77323c"
     }
    },
    "843c830297594fe7aa7cacd4fea6bfc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8495ad273edf4ebfadc8bc0d377ae8dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c6650c299b24c37b0c95b6e60c08125": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93207531a2d04714ba2c0a955c61a67d",
      "placeholder": "​",
      "style": "IPY_MODEL_552aa9a8d19947b9bb8ef71769649a22",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "93207531a2d04714ba2c0a955c61a67d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94075dee90e2425689176dbe91cef4ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0e70451ca4c3478ea8bd32b628f5ea8a",
       "IPY_MODEL_48a8325dc4a443e5a9bc467bf3df2f78",
       "IPY_MODEL_3511be86d8934ab4ac64ed3d8e02a65f"
      ],
      "layout": "IPY_MODEL_8495ad273edf4ebfadc8bc0d377ae8dd"
     }
    },
    "981cf3b90fa642819254afc204a4e6ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a696bac8dee40f1b3d8010636b254ed",
      "placeholder": "​",
      "style": "IPY_MODEL_5a23e7c48eb34a65bd5b07b6db00a3da",
      "value": "Downloading (…)neration_config.json: 100%"
     }
    },
    "9e21e71dd8a3454fbebf7d3ca25df966": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1aa44f1301d4295b63e3cbefcbc86f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc91ac92549546f2b528acdc5d7cd86f",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5190a6e308084dafb92ca9e8efb26e48",
      "value": 456318
     }
    },
    "a20c80af13fb4037852605f82f714cd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf40ac805ffd421d9ecd1db6894efee0",
       "IPY_MODEL_0022b9d7c82b478e8c17bc56edd3cab5",
       "IPY_MODEL_eacb7cc951434c72bddcd5754a4ae78a"
      ],
      "layout": "IPY_MODEL_843c830297594fe7aa7cacd4fea6bfc3"
     }
    },
    "a60568f613ca4bcd8da9150a54f60eb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "afec3e01ffaf438a92cb11b358694f42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9cd29f61c8b428f97e32e140c0250ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c62e94fde69f46afa505eb24ba03be3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c84d5275fe0845a29916d77adf34630c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf40ac805ffd421d9ecd1db6894efee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_405205e401bf4972b3ed1ff0e11e5af8",
      "placeholder": "​",
      "style": "IPY_MODEL_afec3e01ffaf438a92cb11b358694f42",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "d618a30185c74432aee3907ec8b1225d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f99f6290ca3640a18a9fbe41980f3411",
      "placeholder": "​",
      "style": "IPY_MODEL_f43e68bb14bc49f4a78491a525461689",
      "value": " 26.0/26.0 [00:00&lt;00:00, 1.55kB/s]"
     }
    },
    "dc91ac92549546f2b528acdc5d7cd86f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de1d5b5b97364df28408441b99209c99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e220b17e036e41f8807230e321baf524": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9cd29f61c8b428f97e32e140c0250ef",
      "max": 1752292117,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_34f84e904398483092b6e64d3e382a6f",
      "value": 1752292117
     }
    },
    "e316fa98c07241f38057e42c471fdee7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea6e7a8957754378bb4e4a87f460207a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f012d92822dc46bebd6881fe48d0312c",
       "IPY_MODEL_5bd9e91e79814b54a7a6545ab57d4e41",
       "IPY_MODEL_d618a30185c74432aee3907ec8b1225d"
      ],
      "layout": "IPY_MODEL_1d70dcadb8f74194b088e4dfdea0ae55"
     }
    },
    "eacb7cc951434c72bddcd5754a4ae78a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76105038289540a68a560379a6c361e5",
      "placeholder": "​",
      "style": "IPY_MODEL_de1d5b5b97364df28408441b99209c99",
      "value": " 642/642 [00:00&lt;00:00, 26.3kB/s]"
     }
    },
    "f012d92822dc46bebd6881fe48d0312c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_736c682a759242888297a3c1b7504c65",
      "placeholder": "​",
      "style": "IPY_MODEL_58f3cb124dcf45438a749da2678e6a32",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "f15b974cbc8847be9047faff6a86a630": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f244f885a9fa4265b20b88dddbe7f37e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cb3e1aedf8a416380ba288e333ce925",
      "placeholder": "​",
      "style": "IPY_MODEL_328ae82f282c4cccbc324c30a409d536",
      "value": " 1.75G/1.75G [00:20&lt;00:00, 180MB/s]"
     }
    },
    "f43e68bb14bc49f4a78491a525461689": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f99f6290ca3640a18a9fbe41980f3411": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
